<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning | Jin Jeon</title>
    <link>https://jinjeon.me/tags/machine-learning/</link>
      <atom:link href="https://jinjeon.me/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>machine-learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 developed by Jin Jeon with HTML/CSS/Markdown and ☕️ </copyright><lastBuildDate>Sat, 01 May 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jinjeon.me/img/icon.png</url>
      <title>machine-learning</title>
      <link>https://jinjeon.me/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Sentiment Analysis, Textual Data Analysis, and Visualization using Natural Language API</title>
      <link>https://jinjeon.me/post/textual-data-analysis/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/textual-data-analysis/</guid>
      <description>

&lt;p&gt;I use survey data collected from Amazon Mechanical Turk and Reddit user groups (all personal data  have been removed) in a study to examine the impact of cultural localization on web-based account creation between American and Korean users. I use the experiment data to display basic statistical tests in Python.&lt;/p&gt;

&lt;h3 id=&#34;research-question&#34;&gt;Research Question:&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Is there a difference in providing personal information between USA and Korean Internet users &lt;br&gt;
within two different use scenarios: online banking and shopping?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I use the following tests:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#1.-Pearson-Correlation-Coefficient&#34;&gt;Pearson Correlation Coefficient&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#2.-T-Test&#34;&gt;T-Test&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#3.-Mann-Whitney-Test&#34;&gt;Mann-Whitney Test&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#4.-One-Way-Analysis-of-Variance-(ANOVA)&#34;&gt;One-Way Analysis of Variance (ANOVA)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;#5.-Two-Way-ANOVA&#34;&gt;Two-Way ANOVA&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import pandas as pd
import numpy as np
import seaborn as sns
import scipy
from matplotlib import pyplot
import matplotlib.pyplot as plt
from statsmodels.formula.api import ols
import statsmodels.formula.api as smf
import statsmodels.api as sm
from statsmodels.stats.anova import AnovaRM
import pdb  # for debugging
import warnings
warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)

# set color
sns.set_color_codes(&#39;pastel&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;setup-querying-data&#34;&gt;Setup &amp;amp; Querying Data&lt;/h2&gt;

&lt;p&gt;It is first critical to understand the dataframe to play around and make analysis. Usually, &lt;strong&gt;&lt;em&gt;long-format&lt;/em&gt;&lt;/strong&gt; data is desired (or at least I&amp;rsquo;m used to it) for using Python and Seaborn for data visualization. Long format is basically when each variable is represented as a column, and each observation or event is a row. Below, we read in, and query the data.&lt;/p&gt;

&lt;h4 id=&#34;useful-commands&#34;&gt;Useful commands:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;df.head()&lt;/code&gt;: by default, shows first five rows of df&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df.columns()&lt;/code&gt;: prints all the columns in df&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df.describe()&lt;/code&gt;: provides summary description of df&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;pd.read_csv(data, usecols=[&#39;col1&#39;, &#39;col2&#39;, ...,])&lt;/code&gt;: can be used to filter columns&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# read in data.csv file as df &amp;amp; see data structure
df = pd.read_csv(&#39;data.csv&#39;)

# query data by scenario and culture
bank = df.query(&amp;quot;scenario == &#39;Bank&#39;&amp;quot;).copy()
shop = df.query(&amp;quot;scenario == &#39;Shop&#39;&amp;quot;).copy()
kor = df.query(&amp;quot;culture == &#39;Korea&#39;&amp;quot;).copy()
usa = df.query(&amp;quot;culture == &#39;USA&#39;&amp;quot;).copy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# an example of the data structure
usa.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;UserGuid&lt;/th&gt;
      &lt;th&gt;culture&lt;/th&gt;
      &lt;th&gt;scenario&lt;/th&gt;
      &lt;th&gt;interface&lt;/th&gt;
      &lt;th&gt;complete&lt;/th&gt;
      &lt;th&gt;first&lt;/th&gt;
      &lt;th&gt;last&lt;/th&gt;
      &lt;th&gt;phone&lt;/th&gt;
      &lt;th&gt;dob&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;address&lt;/th&gt;
      &lt;th&gt;citizenship&lt;/th&gt;
      &lt;th&gt;website&lt;/th&gt;
      &lt;th&gt;password&lt;/th&gt;
      &lt;th&gt;username&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;reason&lt;/th&gt;
      &lt;th&gt;total&lt;/th&gt;
      &lt;th&gt;total_possible&lt;/th&gt;
      &lt;th&gt;percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.518519&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Shop&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0.208333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.518519&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Shop&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0.208333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.037037&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;1-pearson-correlation-coefficient&#34;&gt;1. Pearson Correlation Coefficient&lt;/h2&gt;

&lt;p&gt;When we want to ask &lt;em&gt;&amp;ldquo;how strongly correlated are the two variables?&amp;rdquo;&lt;/em&gt;, we can use &lt;strong&gt;Perason&amp;rsquo;s Correlation&lt;/strong&gt;. It is used to measure statistical relationship or association between two &lt;strong&gt;&lt;em&gt;continuous variables&lt;/em&gt;&lt;/strong&gt; that are linearly related to each other. The coefficient value &lt;em&gt;&amp;ldquo;r&amp;rdquo;&lt;/em&gt; ranges from -1 (negative relation) to 1 (perfectly positive). 0 would mean that there is no relationship at all.&lt;/p&gt;

&lt;h3 id=&#34;properties-of-pearson-correlation&#34;&gt;Properties of Pearson Correlation&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;The units of the values do not affect the Pearson Correlation.

&lt;ul&gt;
&lt;li&gt;i.e. Changing the unit of value from cm to inches do not affect the r value&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The correlation between the two variables is symmetric:

&lt;ul&gt;
&lt;li&gt;i.e. A -&amp;gt; B is equal to B -&amp;gt; A&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;** Use &lt;strong&gt;&lt;em&gt;Spearman&amp;rsquo;s Correlation&lt;/em&gt;&lt;/strong&gt; when the two variables have non-linear relationship (e.g. a curve instead of a straight line).&lt;/p&gt;

&lt;h3 id=&#34;code-implementation&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We use scipy package to calculate the Pearson Correlation. The method will return two values: &lt;strong&gt;&lt;em&gt;r&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; value.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let&#39;s look at the correlation of information provided by different scenarios: online banking vs. shopping
# bank[&#39;percent&#39;] will return an array of percentage values

r, p = scipy.stats.pearsonr(bank[&#39;percent&#39;], shop[&#39;percent&#39;])  
print(&#39;r: &#39; + str(r.round(4)))
print(&#39;p: &#39; + str(p.round(4)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;r: 0.7592
p: 0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the results above, we can see &lt;strong&gt;there is a strong positive relationship between the amount of information provided in banking and shopping.&lt;/strong&gt; i.e. Providing information in banking would affect how a user provides personal information in shopping.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-t-test&#34;&gt;2. T-Test&lt;/h2&gt;

&lt;p&gt;When comparing the means of two groups, we can use a &lt;strong&gt;t-test&lt;/strong&gt;. It takes into account of the means and the spread of the data to determine &lt;strong&gt;&lt;em&gt;whether a difference between the two would occur by chance or not&lt;/em&gt;&lt;/strong&gt; (determined by the p-value being less than 0.05 usually). In a t-test, there should be only two independent variables (categorical/nominal variables) and one dependent continuous variable.&lt;/p&gt;

&lt;h3 id=&#34;properties-of-t-test&#34;&gt;Properties of t-test&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The data is assumed to be &lt;strong&gt;normal&lt;/strong&gt; (If the distribution is skewed, use Mann-Whitney test). &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;T-test yields &lt;strong&gt;&lt;em&gt;t&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; value:&lt;br&gt;
2a. &lt;strong&gt;The higher the t, the more difference there is between the two groups.&lt;/strong&gt; The lower the t, the more similar the two groups are.&lt;br&gt;
2b. T-value of 2 means the groups are twice as different from each other than they are within each other&lt;br&gt;
2c. &lt;strong&gt;The lower the p-value, the better&lt;/strong&gt; (meaning that it is significant and the difference did not occure by chance). P-value of 0.05 means that there is 5 percent happening by chance&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;code-implementation-1&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We use scipy package again to run a t-test. Before we decide which test to run, we can quickly plot and see the distribution like below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.distplot(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1c238f61d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./output_10_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The distribution looks relatively normal. We can run a t-test to see whether there is a difference between the total amount of information provided by the users from each use scenario: i.e. banking vs. shopping&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we run a t-test to see whether there ia a difference in the amount of information provided in each scenario
t, p = scipy.stats.ttest_ind(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent, df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent)
print(&#39;t: &#39; + str(t.round(4)))
print(&#39;p: &#39; + str(p.round(6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t: 4.8203
p: 2e-06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result above shows that there is a significant difference in the amount of information provided between two use scenarios with t-value being high, and p-value being very small. However, we don&amp;rsquo;t actually know which scenario yields more information than the other. The t-test only tells there is a significant difference.&lt;/p&gt;

&lt;p&gt;To find out, we can create a little fancy distribution plot with some box plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;banking = df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent
shopping = df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent

# let&#39;s plot box-dist plot combined
f, (ax_box1, ax_box2, ax_dist) = plt.subplots(3, sharex=True,
                                              gridspec_kw= {&amp;quot;height_ratios&amp;quot;: (0.3, 0.3, 1)})

# add boxplots at the top
sns.boxplot(banking, ax=ax_box1, color=&#39;g&#39;)
sns.boxplot(shopping, ax=ax_box2, color=&#39;m&#39;)
ax_box1.axvline(np.mean(banking), color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_box2.axvline(np.mean(shopping), color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.subplots_adjust(top=0.87)
plt.suptitle(&#39;Amount of information provided by use scenario&#39;, fontsize = 17)

# add distplots below
sns.distplot(banking, ax=ax_dist, label=&#39;Banking&#39;, kde=True, rug=True, color=&#39;g&#39;, norm_hist=True, bins=2)
sns.distplot(shopping, ax=ax_dist, label=&#39;Shopping&#39;, kde=True, rug=True, color=&#39;m&#39;, norm_hist=True, bins=2)

ax_dist.axvline(np.mean(banking), color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_dist.axvline(np.mean(shopping), color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.legend()
plt.xlabel(&#39;Percentage of information&#39;, fontsize=16)
ax_box1.set(xlabel=&#39;&#39;)
ax_box2.set(xlabel=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[Text(0.5, 0, &#39;&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./output_14_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the graph above, we see that the mean of the banking is greater than the mean of shopping. This shows us that regardless of cultural background, users are more likely to provide personal information in the banking scenario.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-mann-whitney-test&#34;&gt;3. Mann-Whitney Test&lt;/h2&gt;

&lt;p&gt;The Mann-Whitney Test allows you to determine if the observed difference is statistically significant without making the assumption that the values are normally distributed. You should have two independent variables and one continuous dependent variable.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-2&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We can run the test on the same banking vs. shopping scenario.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t, p = scipy.stats.mannwhitneyu(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent, df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent)
print(&#39;t: &#39; + str(t.round(4)))
print(&#39;p: &#39; + str(p.round(6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t: 14795.5
p: 4.1e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;4-one-way-analysis-of-variance-anova&#34;&gt;4. One-Way Analysis of Variance (ANOVA)&lt;/h2&gt;

&lt;p&gt;ANOVA is similar to a t-test, but it is used when there are three or more independent variables (categorical). It assumes normal distribution (use Kruskal-Wallis if abnormal?). One-way ANOVA compares the means between the variables to test whether the difference is statistically significant. However, it does not tell you which specific groups were statistically different from one another. Thus, a post-hoc analysis is required.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-3&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;The result below suggests that there is a statistical difference in the means of the three variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we can create a third variable, and compare the var1, var2, and var3 with one-way ANOVA
var3 = df[df[&#39;culture&#39;] == &#39;USA&#39;].percent
scipy.stats.f_oneway(banking, shopping, var3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;F_onewayResult(statistic=11.171874914065159, pvalue=1.7072783704546878e-05)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;5-two-way-anova&#34;&gt;5. Two-Way ANOVA&lt;/h2&gt;

&lt;p&gt;A two-way ANOVA can be used when you want to know how two independent variables have an interaction effect on a dependent variable. CAVEAT: a two-way ANOVA does not tell which variable is dominant.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-4&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;Below in the code, we see &lt;strong&gt;&lt;em&gt;if there is an interaction effect between culture and scenario use cases on the total amount of information provided.&lt;/em&gt;&lt;/strong&gt; For example, would Americans be more willing to provide personal information than Koreans? If so, does the use case (either banking vs. shopping) affect at all?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we give in a string value of each variable, and the interaction variable &#39;culture:scenario&#39;

model = ols(&#39;percent ~ culture + scenario + culture:scenario&#39;, data=df).fit()
sm.stats.anova_lm(model, typ=2)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sum_sq&lt;/th&gt;
      &lt;th&gt;df&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
      &lt;th&gt;PR(&amp;gt;F)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;culture&lt;/th&gt;
      &lt;td&gt;0.000344&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.007439&lt;/td&gt;
      &lt;td&gt;0.931312&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;scenario&lt;/th&gt;
      &lt;td&gt;1.070130&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;23.159298&lt;/td&gt;
      &lt;td&gt;0.000002&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;culture:scenario&lt;/th&gt;
      &lt;td&gt;0.032834&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.710576&lt;/td&gt;
      &lt;td&gt;0.399772&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Residual&lt;/th&gt;
      &lt;td&gt;17.928461&lt;/td&gt;
      &lt;td&gt;388.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;From the table above, only scenario has a sole effect on the total amount of information provided (depicted as &lt;code&gt;percent&lt;/code&gt; in the dataframe). We see culture, and the interaction of culture and scenario do not have an effect on the amount of information that users provided.&lt;/p&gt;

&lt;p&gt;The finding matches with the previous t-test and graph results, where users provided more information in the banking than they would in shopping.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Universal Sentence Encoder and GloVe on Narrative Semantic Representation</title>
      <link>https://jinjeon.me/post/vectorspace/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/vectorspace/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;See full repo at &lt;a href=&#34;https://github.com/jeon11/use-glove-narrative.git&#34; target=&#34;_blank&#34;&gt;https://github.com/jeon11/use-glove-narrative.git&lt;/a&gt;&lt;/strong&gt;
&lt;br&gt;
&lt;strong&gt;Note:&lt;/strong&gt; The results are shown in the &lt;a href=&#34;https://jinjeon.me/#posters&#34;&gt;poster&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Google&amp;rsquo;s Universal Sentence Encoder (USE)&lt;/strong&gt; provides 512-dimension vectors for each input that are pre-trained on large corpus, and can be plugged into a variety of different task models, such as sentiment analysis, classification, and etc. It is speed-efficient without losing task accuracy, and also provides embeddings not just for word level, but also for phrases, sentences, and even paragraphs. However, the more the words are given as input, the more likely each word meaning gets diluted.&lt;/p&gt;

&lt;p&gt;This notebook is based on the Semantic Similarity with TF-Hub Universal Encoder tutorial, but uses a separate input from one of the projects. We will also use &lt;strong&gt;GloVe&lt;/strong&gt; vectors to compare how the vectors and cosine similarity differ between the two models.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, the notebook goes over setting up locally and use one sample data to create embeddings saved out as a separate csv file using Pandas.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then assuming you have cloned the repository, we call in custom functions to quickly extract vectors of given word, phrase, sentences in USE and GloVe.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;table-of-contents-short-cuts&#34;&gt;Table of Contents/Short-cuts:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#About-USE-Models-and-Deep-Average-Network&#34;&gt;About USE Models and Deep Average Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Installation-&amp;amp;-Setup&#34;&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Path-Setup&#34;&gt;Path Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Raw-Data-Format&#34;&gt;Raw Data Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Get-USE-Embeddings&#34;&gt;Get USE Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Cosine-Similarity&#34;&gt;Cosine Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Cosine-Similarity-Example&#34;&gt;Cosine Similarity Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Plotting-Similarity-Matrix&#34;&gt;Plotting Similarity Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;about-use-models-and-deep-average-network&#34;&gt;About USE Models and Deep Average Network&lt;/h3&gt;

&lt;p&gt;There are two types of models in &lt;strong&gt;USE&lt;/strong&gt;: &lt;strong&gt;Transformer&lt;/strong&gt; and &lt;strong&gt;Deep Averaging Network (DAN)&lt;/strong&gt;. We will use DAN which is a lighter version for efficiency and speed in exchange for reduced accuracy (still accurate enough).&lt;/p&gt;

&lt;p&gt;DAN first averages the input word embeddings to create a sentence embedding. It uses PTB tokenizer, which divides a sentence into a sequence of tokens based on set of rules on  how to process punctuation, articles, etc, in order to create 512 dimension embeddings. This averaged 512 vector is passed to one or more feedforward layers. Then it is multi-task-trained on unsupervised data drawn from various internet sources,  Wikipedia, Stanford Natural Language Inference corpus, web news, and forums.
- Training  goals:
    - Uses skip-thought-like model that predicts the surrounding sentences of a given text (see below)
    - Conversational response suggestion
    - Classification task on supervised data&lt;/p&gt;

&lt;p&gt;The intuition behind deep feedforward neural network is that each layer learns a more abstract representation of the input than the previous one. So its depth allows to capture subtle variations of the input with more depths. Also, each layer only involves a single matrix multiplication, allowing minimal computing time.&lt;/p&gt;

&lt;p&gt;See full USE paper: &lt;a href=&#34;https://arxiv.org/pdf/1803.11175.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1803.11175.pdf&lt;/a&gt;
See full DAN paper: &lt;a href=&#34;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&#34; target=&#34;_blank&#34;&gt;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;installation-setup&#34;&gt;Installation &amp;amp; Setup&lt;/h3&gt;

&lt;p&gt;I used Anaconda to create a TensorFlow-specific environment to customize the package versions. After installing Anaconda&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Creating a new environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create -n py3 python=3.6.8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Activate the created environment by &lt;code&gt;conda activate py3&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Using pip, install packages for pandas, numpy, seaborn, tensorflow, tensorflow_hub. ie. &lt;code&gt;pip install pckge-name&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, let&amp;rsquo;s make sure to set the packages to exact version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install --upgrade tensorflow=1.15.0
pip install --upgrade tensorflow-hub=0.7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the steps are done, we should be able to run the codes locally.&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from absl import logging
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from glob import glob
import re
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;
due to some depecrated methods and changes made with the tf version upgrade from tf1.X to tf2.0, here we use a specific set of Python and tf versions. You can check via &lt;code&gt;pip freeze&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tested on python == 3.6.8 | tensorflow == 1.15.0 | tensorflow_hub == 0.7.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Or you can check the version in Python via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
print(sys.version_info)  # sys.version_info(major=3, minor=6, micro=8, releaselevel=&#39;final&#39;)
print(tf.__version__)    # &#39;1.15.0&#39;
print(hub.__version__)   # &#39;0.7.0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# script variables

# for lite/DAN version:
module_url = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder/2&amp;quot;

# for heavy/Transformer version:
# module_url = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder-large/3&amp;quot;

baseDir = &#39;use-glove-narrative&#39;  # repository/base folder name
embedding_size = 512  # base 512-dimension embedding
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;path-setup&#34;&gt;Path Setup&lt;/h3&gt;

&lt;p&gt;Assuming that you git cloned the project (which is for demo purposes) to your local directory, we set the path so the code knows where to look for certain data files using the &lt;code&gt;baseDir&lt;/code&gt; specified above. We will mainly just work within the cloned folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pwd = os.getcwd()
# recursively find absolute path
while os.path.basename(pwd) != baseDir:
    os.chdir(&#39;..&#39;)
    pwd = os.getcwd()
baseDir = pwd
dataDir = baseDir + &#39;/data&#39;

# recursively find all csv files. We will work with one file here
all_csvs = [y for x in os.walk(dataDir) for y in glob(os.path.join(x[0], &#39;*.csv&#39;))]
all_csvs.sort()
all_csvs = all_csvs[0]  # we will just use one sample data
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;raw-data-format&#34;&gt;Raw Data Format&lt;/h3&gt;

&lt;p&gt;To briefly show the data, the data is comprised of numerous idea units, or phrases of words with unique meanings. Here, we are only interested in the &amp;lsquo;text&amp;rsquo; column and &amp;lsquo;index&amp;rsquo; column. We will call in the text of the entire story to create embeddings for each idea unit. Below the example print out, we will loop over each story to create embeddings. Since we will use one story this time, it shouldn&amp;rsquo;t take that long.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# an example print of data format
datafile = pd.read_csv(all_csvs)
datafile.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;paragraph&lt;/th&gt;
      &lt;th&gt;index&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;scoring&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;One fine day an old Maine man was fishing&lt;/td&gt;
      &lt;td&gt;mentions at least 3 of the following: “old”, “...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;on his favorite lake&lt;/td&gt;
      &lt;td&gt;mentions “favorite lake” or “favorite river”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;and catching very little.&lt;/td&gt;
      &lt;td&gt;mentions that he/the fisherman was not having ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Finally, he gave up&lt;/td&gt;
      &lt;td&gt;mentions that he gave up/stopped fishing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;and walked back along the shore to his fishing...&lt;/td&gt;
      &lt;td&gt;mentions that he walked home/to his fishing sh...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;get-use-embeddings&#34;&gt;Get USE Embeddings&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let&#39;s read in the data file
textfile = pd.read_csv(all_csvs)
# get the title of the narrative story, cutting out the .csv extension
title = os.path.basename(all_csvs)[:-4]


# create df to save out at the end
vector_df_columns = [&#39;paragraph&#39;, &#39;index&#39;, &#39;text&#39;, &#39;size&#39;]
# create column for each dimension (out of 512)
for i in range(1, embedding_size + 1):
    vector_df_columns.append(&#39;dim&#39; + str(i))
vector_df = pd.DataFrame(columns=vector_df_columns)


# import the Universal Sentence Encoder&#39;s TF Hub module
embed = hub.Module(module_url)  # hub.load(module_url) for tf==2.0.0

# we call in the text column from data file
messages = []
for t in range(0, len(textfile)):
    messages.append(textfile.iloc[t][&#39;text&#39;])

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reduce logging output.
logging.set_verbosity(logging.ERROR)

with tf.compat.v1.Session() as session:
    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])
    message_embeddings = session.run(embed(messages))

# make sure all units are there/sanity check
assert len(message_embeddings) == len(textfile) == len(messages)

# loop over each vector value to corresponding text
for e in range(0, len(message_embeddings)):
    vector_df.at[e, &#39;paragraph&#39;] = textfile.iloc[e][&#39;paragraph&#39;]
    vector_df.at[e, &#39;index&#39;] = textfile.iloc[e][&#39;index&#39;]
    vector_df.at[e, &#39;text&#39;] = messages[e]
    vector_df.at[e, &#39;size&#39;] = len(message_embeddings[e])
    for dim in range(0, len(message_embeddings[e])):
        vector_df.at[e, &#39;dim&#39;+str(dim+1)] = message_embeddings[e][dim]

# display sample format
vector_df.head()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;paragraph&lt;/th&gt;
      &lt;th&gt;index&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
      &lt;th&gt;dim1&lt;/th&gt;
      &lt;th&gt;dim2&lt;/th&gt;
      &lt;th&gt;dim3&lt;/th&gt;
      &lt;th&gt;dim4&lt;/th&gt;
      &lt;th&gt;dim5&lt;/th&gt;
      &lt;th&gt;dim6&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;dim503&lt;/th&gt;
      &lt;th&gt;dim504&lt;/th&gt;
      &lt;th&gt;dim505&lt;/th&gt;
      &lt;th&gt;dim506&lt;/th&gt;
      &lt;th&gt;dim507&lt;/th&gt;
      &lt;th&gt;dim508&lt;/th&gt;
      &lt;th&gt;dim509&lt;/th&gt;
      &lt;th&gt;dim510&lt;/th&gt;
      &lt;th&gt;dim511&lt;/th&gt;
      &lt;th&gt;dim512&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;One fine day an old Maine man was fishing&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0169429&lt;/td&gt;
      &lt;td&gt;-0.0030699&lt;/td&gt;
      &lt;td&gt;-0.0156278&lt;/td&gt;
      &lt;td&gt;-0.00649163&lt;/td&gt;
      &lt;td&gt;0.0213989&lt;/td&gt;
      &lt;td&gt;-0.0541645&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0404136&lt;/td&gt;
      &lt;td&gt;-0.0577177&lt;/td&gt;
      &lt;td&gt;0.0108959&lt;/td&gt;
      &lt;td&gt;-0.0337963&lt;/td&gt;
      &lt;td&gt;0.0817816&lt;/td&gt;
      &lt;td&gt;-0.074783&lt;/td&gt;
      &lt;td&gt;0.0231454&lt;/td&gt;
      &lt;td&gt;0.0719041&lt;/td&gt;
      &lt;td&gt;-0.047105&lt;/td&gt;
      &lt;td&gt;0.0127639&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;on his favorite lake&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;-0.0172151&lt;/td&gt;
      &lt;td&gt;0.0418602&lt;/td&gt;
      &lt;td&gt;0.0105562&lt;/td&gt;
      &lt;td&gt;0.0290091&lt;/td&gt;
      &lt;td&gt;0.0351211&lt;/td&gt;
      &lt;td&gt;-0.0121579&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0319399&lt;/td&gt;
      &lt;td&gt;-0.0201722&lt;/td&gt;
      &lt;td&gt;-0.00480706&lt;/td&gt;
      &lt;td&gt;-0.0490393&lt;/td&gt;
      &lt;td&gt;0.0562807&lt;/td&gt;
      &lt;td&gt;-0.0840528&lt;/td&gt;
      &lt;td&gt;0.0359073&lt;/td&gt;
      &lt;td&gt;0.0519214&lt;/td&gt;
      &lt;td&gt;0.0635523&lt;/td&gt;
      &lt;td&gt;-0.0615548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;and catching very little.&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0515628&lt;/td&gt;
      &lt;td&gt;0.00556853&lt;/td&gt;
      &lt;td&gt;-0.0606079&lt;/td&gt;
      &lt;td&gt;-0.0281095&lt;/td&gt;
      &lt;td&gt;-0.0631535&lt;/td&gt;
      &lt;td&gt;-0.0586548&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.0266129&lt;/td&gt;
      &lt;td&gt;0.0111167&lt;/td&gt;
      &lt;td&gt;-0.0238963&lt;/td&gt;
      &lt;td&gt;0.00741908&lt;/td&gt;
      &lt;td&gt;-0.0685881&lt;/td&gt;
      &lt;td&gt;-0.0858848&lt;/td&gt;
      &lt;td&gt;0.066858&lt;/td&gt;
      &lt;td&gt;-0.0616563&lt;/td&gt;
      &lt;td&gt;-0.0844253&lt;/td&gt;
      &lt;td&gt;-0.026685&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Finally, he gave up&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0818241&lt;/td&gt;
      &lt;td&gt;0.00549721&lt;/td&gt;
      &lt;td&gt;-0.0245033&lt;/td&gt;
      &lt;td&gt;0.0286504&lt;/td&gt;
      &lt;td&gt;-0.0284165&lt;/td&gt;
      &lt;td&gt;-0.0575481&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0474779&lt;/td&gt;
      &lt;td&gt;-0.00603216&lt;/td&gt;
      &lt;td&gt;-0.0116888&lt;/td&gt;
      &lt;td&gt;-0.06419&lt;/td&gt;
      &lt;td&gt;0.0268704&lt;/td&gt;
      &lt;td&gt;-0.0640136&lt;/td&gt;
      &lt;td&gt;0.103409&lt;/td&gt;
      &lt;td&gt;-0.0235997&lt;/td&gt;
      &lt;td&gt;-0.0781731&lt;/td&gt;
      &lt;td&gt;-0.0365196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;and walked back along the shore to his fishing...&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.00286058&lt;/td&gt;
      &lt;td&gt;0.0576001&lt;/td&gt;
      &lt;td&gt;0.0103945&lt;/td&gt;
      &lt;td&gt;-0.00301533&lt;/td&gt;
      &lt;td&gt;0.0199591&lt;/td&gt;
      &lt;td&gt;-0.0644398&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.0145959&lt;/td&gt;
      &lt;td&gt;0.0137776&lt;/td&gt;
      &lt;td&gt;0.0165417&lt;/td&gt;
      &lt;td&gt;-0.0406641&lt;/td&gt;
      &lt;td&gt;-0.0204453&lt;/td&gt;
      &lt;td&gt;-0.0713526&lt;/td&gt;
      &lt;td&gt;0.0121754&lt;/td&gt;
      &lt;td&gt;0.00591647&lt;/td&gt;
      &lt;td&gt;0.0262764&lt;/td&gt;
      &lt;td&gt;-0.0329477&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 516 columns&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.shape(vector_df))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(0, 516)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sample data shows each idea unit/text converted to 512 dimension vectors. &lt;code&gt;np.shape(vector_df)&lt;/code&gt; will return a 41 total idea units/phrases to 516 columns (512 dimensions + custom columns (paragraph info, index, text, and size)). We then use these vectors to explore semantic similarity between text and phrases.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run the code below to save out as csv file
vector_df.reindex(columns=vector_df_columns)
vector_df.to_csv(title + &#39;_vectors.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h2&gt;

&lt;p&gt;As a brief description, &lt;strong&gt;cosine similarity&lt;/strong&gt; is basically the measure of cosine angle between the two vectors. Since we have USE and GloVe vectors that represent words into multidimensional vectors, we can apply these vector values to calculate how similar the two words are.&lt;/p&gt;

&lt;p&gt;It can be easily calculated in Python with its useful packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cos_sim = numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1) * numpy.linalg.norm(vector2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming we established some basic understanding, let&amp;rsquo;s call in the functions I made so that we can easily get USE and GloVe vectors at multiple word level.&lt;/p&gt;

&lt;p&gt;I will highlight some of the functions below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from get_glove_use import *
help(glove_vec)
help(use_vec)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Help on function glove_vec in module get_glove_use:

glove_vec(item1, item2)
    get vectors for given two words and calculate cosine similarity

    Parameters
    ----------
    item1 : str
        string in glove word pool vector to compare
    item2 : str
        string in glove word pool vector to compare

    Returns
    -------
    item1_vector : array
        item1 GloVe vector
    item2_vector : array
        item2 GloVe vector
    cos_sim : float
        cosine similarity of item1 and item2 vectors

Help on function use_vec in module get_glove_use:

use_vec(item1, item2)
    get USE vectors and cosine similairty of the two items

    Parameters
    ----------
    item1 : str, list
        any word to compare, put in string for more than one word
    item2 : str, list
        any word to compare, put in string for more than one word

    Returns
    -------
    item1_vector : array
        item1 USE vector
    item2_vector : array
        item2 USE vector
    cos_sim : float
        cosine similarity of item1 and item2 vectors
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cosine-similarity-example&#34;&gt;Cosine Similarity Example&lt;/h3&gt;

&lt;p&gt;Using the two functions above, and another function compare_word_vec (which basically uses the two functions), we can easily obtain cosine similarity of two words.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# using the two functions above, we can get
# GloVe and USE vectors and cosine similarity of two input words
os.chdir(gloveDir)
_, _, glove_sim = glove_vec(&#39;fish&#39;,&#39;bear&#39;)
_, _, use_sim = use_vec(&#39;fish&#39;,&#39;bear&#39;)
print(&#39;use cos: &#39; + str(use_sim))
print(&#39;glove cos: &#39; + str(glove_sim))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


0.11964830574261577
0.5305143
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the two functions glove_vex and use_vec are use in compare_word_vec
compare_word_vec(&#39;man&#39;,&#39;fish&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


use cos: 0.49838725
glove cos: 0.18601566881803455
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; From the example above, USE and GloVe similarly identy &lt;em&gt;fish&lt;/em&gt; to be somewhat equally similar to &lt;em&gt;bear&lt;/em&gt; and &lt;em&gt;man&lt;/em&gt; (but just in different scale/degree).&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s try comparing at multiple words or phrase level. We will use new functions and give in new inputs as strings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentence1 = [&#39;old&#39;, &#39;man&#39;, &#39;caught&#39;, &#39;fish&#39;]
sentence2 = [&#39;bear&#39;, &#39;hunted&#39;, &#39;trout&#39;]
sentence3 = [&#39;bear&#39;,&#39;eat&#39;,&#39;six&#39;,&#39;fish&#39;]

print(&#39;old man caught fish &amp;amp; bear hunted trout:&#39;)
phrase_vec(sentence1, sentence2)

print(&#39;old man caught fish &amp;amp; bear eat six fish:&#39;)
phrase_vec(sentence1, sentence3)

print(&#39;bear hunted trout &amp;amp; bear eat six fish:&#39;)
phrase_vec(sentence2, sentence3)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;old man caught fish &amp;amp; bear hunted trout:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.36609688461789297
USE sim: 0.50494814
old man caught fish &amp;amp; bear eat six fish:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.6818474640845398
USE sim: 0.5896743
bear hunted trout &amp;amp; bear eat six fish:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.6082457470353315
USE sim: 0.72352856
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; From the example above, we can see that USE and GloVe capture somewhat differently. We can see that &lt;em&gt;bear hunted trout&lt;/em&gt; and &lt;em&gt;bear eat six fish&lt;/em&gt; are the most similar to each other, whereas &lt;em&gt;old man caught fish&lt;/em&gt; is also similar to the context of bear eating six fish.&lt;/p&gt;

&lt;p&gt;More detailed analysis is required, but the example above shows great possibilities to exploring semantics.&lt;/p&gt;

&lt;h3 id=&#34;plotting-similarity-matrix&#34;&gt;Plotting Similarity Matrix&lt;/h3&gt;

&lt;p&gt;Now that we can compare similarity of words and sentences, we can plot a simple pairwise matrix, which basically compares how similar each word is to another in the given list. Fortunately, we already have a plot for doing it (using Seaborn).&lt;/p&gt;

&lt;p&gt;I will only use few words as demonstration, since it&amp;rsquo;s been slowing up my computer so much!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_sim_matrix([&#39;man&#39;, &#39;bear&#39;, &#39;fish&#39;, &#39;trout&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./use_glove_cosine_similarity_25_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;ending-note&#34;&gt;Ending Note&lt;/h2&gt;

&lt;p&gt;In the example above, we only used simple noun words. The stronger blue color, the more similar the two words are. Thus, the diagonal strip is deep blue (similarity of same two words is 1). You can see &lt;em&gt;fish&lt;/em&gt; and &lt;em&gt;trout&lt;/em&gt; are more similar to each other, than is &lt;em&gt;man&lt;/em&gt; to &lt;em&gt;trout&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Keep in mind that you can feed in more words and sentences to create and visualize a larger matrix.&lt;/p&gt;

&lt;p&gt;We looked at setting up USE locally, and creating embeddings from USE. The cloned project also has sample version of GloVe vectors. We use the vectors from the two models to extract vectors and compare similarity of two texts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

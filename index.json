[{"authors":["jin"],"categories":null,"content":"I graduated from UC Berkeley with BA in cognitive science with concentration in computer science and cognitive psychology.\nI currently work as a lab manager/research analyst at Vanderbilt Computational Memory Lab directed by PI Sean Polyn, facilitating ongoing studies to explore cognitive and neural mechanism of human memory.\nClick here for CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://jinjeon.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I graduated from UC Berkeley with BA in cognitive science with concentration in computer science and cognitive psychology.\nI currently work as a lab manager/research analyst at Vanderbilt Computational Memory Lab directed by PI Sean Polyn, facilitating ongoing studies to explore cognitive and neural mechanism of human memory.\nClick here for CV.","tags":null,"title":"Jin Jeon","type":"authors"},{"authors":null,"categories":null,"content":"In prep for fMRI neuroimaging processing and building classifier\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"a68fa4468003b45c0900a3c396f2b82d","permalink":"http://jinjeon.me/post/fmri/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/post/fmri/","section":"post","summary":"Click to see my brain in .gif for now ðŸ§ ","tags":null,"title":"fMRI Neuroimaging \u0026 Classifier Coming Soon...","type":"post"},{"authors":null,"categories":null,"content":"In prep for setting up Google\u0026rsquo;s Universal Sentence Encoder, and comparing the two models for measuring cosine similarity of semantics in narrative stories.\nThe results are shown in the poster\n","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"7a4f6295ae0d4e8a6165d80d69954fad","permalink":"http://jinjeon.me/post/vectorspace/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/post/vectorspace/","section":"post","summary":"Coming soon...","tags":null,"title":"Universal Sentence Encoder and GloVe on Narrative Semantic Representation","type":"post"},{"authors":null,"categories":null,"content":" Note: This post is a ported version of Jupyter Notebook from my mne-eeg project: https://github.com/jeon11/mne-egi/blob/master/walkthrough_advanced.ipynb/walkthrough_basics.ipynb Advanced Processing In the previous walkthrough notebook, we got to manually inspect raw instance and do some cleaning based on annotations and creating evoked responses from time-locked events.\nIn this section, we run independent component analysis (ICA) on the epochs we had from the last notebook. We look in ICs to identify potentially bad components with eye related artifcats. Then, we implement autoreject (http://autoreject.github.io) which automatically attempts to find bad channels and interpolate those based on nearby channels. At the end, we plot the ERPs by channels that we are interested in looking and make comparison.\nNote that the plots below will be using print statements for demonstration purposes.\nimport mne import pandas as pd import numpy as np from matplotlib import pyplot as plt import Tkinter from autoreject import AutoReject from autoreject import get_rejection_threshold from mne.preprocessing import ICA  Loading epochs We imported all the necessary dependencies. Now we load the saved epochs from last notebook.\nepochs_tlstS = mne.read_epochs('/data/epochs_tlsts-epo.fif', preload=True) print(epochs_tlstS)  \u0026lt;EpochsFIF | 388 events (all good), -0.25 - 0.8 sec, baseline [-0.25, 0], ~72.8 MB, data loaded, with metadata, u'lstS': 388\u0026gt;  Running Independent Component Analysis (ICA) ICA is a signal processing method to decompose signals into independent sources from a mixed signal. A representative example is the cocktail party effect, which is a phenomenon in which you are able to concentrate on the voice of the speaker you are conversing with regardless of the various background noise in a party. Using ICA helps seperate the different sources of mixed sound, under the assumption that the sound components are linear. This method works for EEG signal preprocessing because we assume that each electrode is independent from the others. To think of it easily, I consider ICA as decomposing the data into multiple layers, and by excluding bad ICs, we filter the data.\n# the function calculates optimal reject threshold for ICA reject = get_rejection_threshold(epochs_tlstS) print(reject)  Estimating rejection dictionary for eeg Estimating rejection dictionary for eog {'eeg': 0.0007759871430524497, 'eog': 5.903189072009943e-05}  Low-frequency slow drifts Because ICA is sensitive to low-frequency slow drifts, it is recommended that 1Hz highpass filter is applied. Since this was already done to our raw instance in the previous notebook, it can be skipped. You can double check as below, or apply the highpass filter if you haven\u0026rsquo;t already.\n# double check highpass filter print(epochs_tlstS.info['highpass']) # epochs_tlstS.info['highpass'] = 1  1.0  Fit ICA Now we will run ICA on our epoch data. For simplicity and time sake, we will limit the number of components to 20 with fastICA method, which is the generally used one. The number of ICs can be created up to as many electrodes (in this case 128 - bad channels). In ica1.fit, we use the recommended reject threshold from Autoreject.\nica = ICA(n_components=20, max_pca_components=None, n_pca_components=None, noise_cov=None, random_state=None, method='fastica', fit_params=None, max_iter=200, verbose=None) print('fitting ica...') ica.fit(epochs_tlstS, reject=reject)  fitting ica... /Users/Jin/Library/Python/2.7/lib/python/site-packages/scipy/linalg/basic.py:1321: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver. x, resids, rank, s = lstsq(a, b, cond=cond, check_finite=False) \u0026lt;ICA | epochs decomposition, fit (fastica): 81868 samples, 20 components, channels used: \u0026quot;eeg\u0026quot;\u0026gt;  Artifact detection using ICA correlation plot_sources can show the signals of each ICs. We can manually inspect for ICs with noise, or identify bad ICs that correlates with oscillations from eye-related channels. We use the builtin find_bads_eog from ICA class.\nplot_scores will show the correlation values for each component, and mark the ones that are potentially bad with red. Note that because we only specified 20 components, the decomposition is rather compressed.\neog_inds, scores = ica.find_bads_eog(epochs_tlstS) print('suggested eog component: ' + str(eog_inds)) print(ica.plot_scores(scores, exclude=eog_inds, labels='eog'))  suggested eog component: [3]  Figure(460.8x194.4)  The find_bads_eog suggested that component \u0026lsquo;3\u0026rsquo; is bad IC related to eye-related artifact. We can plot that specific component to inspect manually.\nprint(ica.plot_properties(epochs_tlstS, picks=eog_inds, psd_args={'fmax': 35.}, image_args={'sigma': 1.})) ica.exclude += eog_inds print(ica.exclude)  /Users/Jin/Library/Python/2.7/lib/python/site-packages/mne/transforms.py:689: RuntimeWarning: invalid value encountered in divide out[:, 2] = np.arccos(cart[:, 2] / out[:, 0])  [\u0026lt;Figure size 504x432 with 5 Axes\u0026gt;] [3]  Usually, eye blinks are characterized as having significantly polar activities between the frontal and the posterior regions with high activity in the frontal region (ie. eyes). Also, activities shown in the frontal region, especially near the eye area, would not be helpful in our analysis. Eye movements are characterized as having significantly split activities between left and right. Component above does seem containing eye blinks, we mark that component bad by ica.exclude and we can see that component has been added.\nWe can also manually inspect for other components using plot_components besides the ones that the builtin method suggested. You can see that the component speficied above being grayed out as a bad IC. The plot prioritizes showing ICs with large activations and polarity, which means that most of the bad ICs could be found in the early ICs.\nprint(ica.plot_components(inst=epochs_tlstS))  [\u0026lt;Figure size 540x504 with 20 Axes\u0026gt;]  When running the code on ipython as suggested in the previous notebook, the plot is actually interactive. By clicking on the component, it shows the component properties. Clicking on the name of the component will gray out the name and be marked as bad IC. Here, it seems components 2, 14, and 18 have high activation in the eye regions, which could be identified as components with eye blinks. Also, componnent 5 has activation in the frontal region, and has polar activities between left and right, which could potentially be eye movements. Because the plot above is not interactive, we will specify which ICs to exclude as a line of code.\nSince we\u0026rsquo;ve identified the bad ICs, we can apply it to our epochs_tlstS, and proceed to autoreject.\nica.exclude += [0, 2, 5, 14, 18] print(ica.exclude) ica.apply(epochs_tlstS) print('number of ICs dropped: ' + str(len(ica.exclude)))  [3, 0, 2, 5, 14, 18] number of ICs dropped: 6  Autoreject Now that we have bad ICs identified, we try implementing autoreject for cleaning. Note that the step below may take some time as it tries find bad channels and fix them.\nar = AutoReject() epochs_clean = ar.fit_transform(epochs_tlstS)  Running autoreject on ch_type=eeg [........................................] 100.00% Creating augmented epochs \\ Computing thresholds ... [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 2.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 4.0s remaining: 0.0s [Parallel(n_jobs=1)]: Done 3 out of 3 | elapsed: 5.5s remaining: 0.0s [Parallel(n_jobs=1)]: Done 4 out of 4 | elapsed: 7.3s remaining: 0.0s [Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 10.5s remaining: 0.0s [Parallel(n_jobs=1)]: Done 6 out of 6 | elapsed: 13.2s remaining: 0.0s [Parallel(n_jobs=1)]: Done 7 out of 7 | elapsed: 16.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 8 out of 8 | elapsed: 19.5s remaining: 0.0s [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 21.7s remaining: 0.0s [Parallel(n_jobs=1)]: Done 10 out of 10 | elapsed: 24.6s remaining: 0.0s [Parallel(n_jobs=1)]: Done 11 out of 11 | elapsed: 27.5s remaining: 0.0s [Parallel(n_jobs=1)]: Done 12 out of 12 | elapsed: 30.3s remaining: 0.0s [Parallel(n_jobs=1)]: Done 13 out of 13 | elapsed: 33.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 14 out of 14 | elapsed: 35.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 15 out of 15 | elapsed: 36.8s remaining: 0.0s [Parallel(n_jobs=1)]: Done 16 out of 16 | elapsed: 38.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 17 out of 17 | elapsed: 40.2s remaining: 0.0s [Parallel(n_jobs=1)]: Done 18 out of 18 | elapsed: 42.2s remaining: 0.0s [Parallel(n_jobs=1)]: Done 19 out of 19 | elapsed: 44.8s remaining: 0.0s [Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 47.0s remaining: 0.0s [Parallel(n_jobs=1)]: Done 21 out of 21 | elapsed: 48.7s remaining: 0.0s [Parallel(n_jobs=1)]: Done 22 out of 22 | elapsed: 50.1s remaining: 0.0s [Parallel(n_jobs=1)]: Done 23 out of 23 | elapsed: 51.6s remaining: 0.0s [Parallel(n_jobs=1)]: Done 24 out of 24 | elapsed: 53.1s remaining: 0.0s [Parallel(n_jobs=1)]: Done 25 out of 25 | elapsed: 54.8s remaining: 0.0s [Parallel(n_jobs=1)]: Done 26 out of 26 | elapsed: 56.4s remaining: 0.0s [Parallel(n_jobs=1)]: Done 27 out of 27 | elapsed: 57.9s remaining: 0.0s [Parallel(n_jobs=1)]: Done 28 out of 28 | elapsed: 59.8s remaining: 0.0s [Parallel(n_jobs=1)]: Done 29 out of 29 | elapsed: 1.0min remaining: 0.0s [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 1.1min remaining: 0.0s [Parallel(n_jobs=1)]: Done 31 out of 31 | elapsed: 1.1min remaining: 0.0s [Parallel(n_jobs=1)]: Done 32 out of 32 | elapsed: 1.1min remaining: 0.0s [Parallel(n_jobs=1)]: Done 33 out of 33 | elapsed: 1.1min remaining: 0.0s [Parallel(n_jobs=1)]: Done 34 out of 34 | elapsed: 1.2min remaining: 0.0s [Parallel(n_jobs=1)]: Done 35 out of 35 | elapsed: 1.2min remaining: 0.0s [Parallel(n_jobs=1)]: Done 36 out of 36 | elapsed: 1.2min remaining: 0.0s [Parallel(n_jobs=1)]: Done 37 out of 37 | elapsed: 1.2min remaining: 0.0s [Parallel(n_jobs=1)]: Done 38 out of 38 | elapsed: 1.3min remaining: 0.0s [Parallel(n_jobs=1)]: Done 39 out of 39 | elapsed: 1.3min remaining: 0.0s [Parallel(n_jobs=1)]: Done 40 out of 40 | elapsed: 1.3min remaining: 0.0s [Parallel(n_jobs=1)]: Done 41 out of 41 | elapsed: 1.4min remaining: 0.0s [Parallel(n_jobs=1)]: Done 42 out of 42 | elapsed: 1.4min remaining: 0.0s [Parallel(n_jobs=1)]: Done 43 out of 43 | elapsed: 1.4min remaining: 0.0s [Parallel(n_jobs=1)]: Done 44 out of 44 | elapsed: 1.4min remaining: 0.0s [Parallel(n_jobs=1)]: Done 45 out of 45 | elapsed: 1.5min remaining: 0.0s [Parallel(n_jobs=1)]: Done 46 out of 46 | elapsed: 1.5min remaining: 0.0s [Parallel(n_jobs=1)]: Done 47 out of 47 | elapsed: 1.5min remaining: 0.0s [Parallel(n_jobs=1)]: Done 48 out of 48 | elapsed: 1.6min remaining: 0.0s [Parallel(n_jobs=1)]: Done 49 out of 49 | elapsed: 1.6min remaining: 0.0s [Parallel(n_jobs=1)]: Done 50 out of 50 | elapsed: 1.6min remaining: 0.0s [Parallel(n_jobs=1)]: Done 115 out of 115 | elapsed: 3.8min finished [........................................] 100.00% n_interp \\ chs | Estimated consensus=0.30 and n_interpolate=4 [........................................] 100.00% Repairing epochs |  The above created a new epochs called epochs_clean. We can compare how the epochs are cleaned by comparing the two plots. For demonstration, we only plot the epochs_clean. The plot shows individual epochs with green line being 0 (the onset of the word in the experiment). In the interactive plot mode, you can scroll vertically to see different channels and horizontally to search through epochs.\nprint(epochs_clean.plot())  Figure(869.6x536.8)  Creating evoked response from epochs_clean Now that we have a new, ideally cleaner epochs, we create evoked response for each condition. Currently, epochs_clean contains all four conditions with approximately 100 epochs for each (less than 400 now because epochs been rejected). Note that the y-axis microvolt scale has been refined compared to our previous notebook.\n# now let's create a new evoked responses (ie. the autoreject evoked) arevoked_tlst_c1 = epochs_clean[\u0026quot;label=='lstS' and cond=='1'\u0026quot;].average() arevoked_tlst_c2 = epochs_clean[\u0026quot;label=='lstS' and cond=='2'\u0026quot;].average() arevoked_tlst_c3 = epochs_clean[\u0026quot;label=='lstS' and cond=='3'\u0026quot;].average() arevoked_tlst_c4 = epochs_clean[\u0026quot;label=='lstS' and cond=='4'\u0026quot;].average() # let's see a sample evoked response print(arevoked_tlst_c1.plot_joint(times='peaks'))  Figure(576x302.4)  Plotting ERP comparison Now that we have evoked response for each condition, we can look into specific channels of interest to see how the signals differ by conditions. For the selection list, we will only specify channel E92 as it will create 4 graphs for each channel.\n# we specify which channels to look at selection = ['E92'] # ['EB','E11','E24','E124','E36','E104','E52','E62','E92'] picks_select = mne.pick_types(epochs_clean.info, meg=False, eeg=True, eog=True, stim=False, exclude='bads', selection=selection) # create dictionary for each condition evoked_dict = {'highcosval': arevoked_tlst_c1, 'lowcosval': arevoked_tlst_c2, 'highcosinval': arevoked_tlst_c3, 'lowcosinval': arevoked_tlst_c4} picks_select = mne.pick_types(arevoked_tlst_c1.info, meg=False, eeg=True, eog=True, stim=False, exclude='bads', selection=selection) # this will plot each selected channel with comparison of two conditions title = '%s_vs_%s_E%s.png' for i in range(0, len(picks_select)): fig1 = mne.viz.plot_compare_evokeds({'highcos/val':evoked_dict['highcosval'], 'lowcos/val':evoked_dict['lowcosval']}, picks=picks_select[i]) fig2 = mne.viz.plot_compare_evokeds({'highcos/inval':evoked_dict['highcosinval'], 'lowcos/inval':evoked_dict['lowcosinval']}, picks=picks_select[i]) fig3 = mne.viz.plot_compare_evokeds({'highcos/val':evoked_dict['highcosval'], 'highcos/inval':evoked_dict['highcosinval']},picks=picks_select[i]) fig4 = mne.viz.plot_compare_evokeds({'lowcos/val':evoked_dict['lowcosval'], 'lowcos/inval':evoked_dict['lowcosinval']}, picks=picks_select[i]) # save figs # fig1.savefig(title % (evoked_dict.keys()[0], evoked_dict.keys()[1], i)) # fig2.savefig(title % (evoked_dict.keys()[2], evoked_dict.keys()[3], i)) # fig3.savefig(title % (evoked_dict.keys()[0], evoked_dict.keys()[2], i)) # fig4.savefig(title % (evoked_dict.keys()[1], evoked_dict.keys()[3], i))  # this will plot just the evoked responses per conditions with all channels fig5 = arevoked_tlst_c1.plot(titles='cond1: high cos/val') fig6 = arevoked_tlst_c2.plot(titles='cond2: low cos/val') fig7 = arevoked_tlst_c3.plot(titles='cond3: high cos/inval') fig8 = arevoked_tlst_c4.plot(titles='cond4: low cos/inval') # save figs # fig5.savefig('c1all.png') # fig6.savefig('c2all.png') # fig7.savefig('c3all.png') # fig8.savefig('c4all.png')  Sources and useful links EEGLab ICA guide: https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA\nMNE ICA class: https://martinos.org/mne/stable/generated/mne.preprocessing.ICA.html\nautoreject: http://autoreject.github.io/auto_examples/plot_auto_repair.html#sphx-glr-auto-examples-plot-auto-repair-py\nClemens Brunner\u0026rsquo;s great guide on ICA: https://cbrnr.github.io/2018/01/29/removing-eog-ica/\nClemens Brunner\u0026rsquo;s great guide on EOG detection using linear regression: https://cbrnr.github.io/2017/10/20/removing-eog-regression/\nMNE stats: https://martinos.org/mne/stable/auto_tutorials/plot_stats_cluster_erp.html#sphx-glr-auto-tutorials-plot-stats-cluster-erp-py\n","date":1525219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525219200,"objectID":"a8fa2f40e29fb9bac38d1afe68ac5f18","permalink":"http://jinjeon.me/post/eeg-advanced/","publishdate":"2018-05-02T00:00:00Z","relpermalink":"/post/eeg-advanced/","section":"post","summary":"Continuation from Part 1, the script runs independent component analysis and automatic epoch rejection, and plots improved ERP","tags":null,"title":"EEG/Signal Processing--Advanced Part 2","type":"post"},{"authors":null,"categories":null,"content":" Note: This post is a ported version of Jupyter Notebook from my mne-eeg project: https://github.com/jeon11/mne-egi/blob/master/walkthrough_basics.ipynb Overview This script runs through sample experiment data from manually reading in raw file to preprocessing through applying filters, eye blink detection using peak finding techniques. Then we create epochs and plot evoked responses.\nIn the advanced walkthrough: walkthrough_advanced.ipynb, we implement independent component analysis (ICA) and autoreject, which is an automated tool for fixing data, to see how the epochs are improved and compare the evoked responses by conditions.\nThe script requires at least two files: - the raw data (download from Google Drive ~500MB) - exported event text log from NetStation software\nRunning the script in command line When running the Python script from command line, MNE recommends using ipython via:\nipython â€”-pylab osx -i mne-egi-walkthrough.py  For Windows, instead of osx, you would be specifying qt.\nImporting Let\u0026rsquo;s begin by importing all the necessary modules. Make sure you have all the required dependencies setup.\nimport mne import pandas as pd import numpy as np import matplotlib from matplotlib import pyplot as plt from mne.preprocessing import eog from mne.preprocessing import create_eog_epochs from mne.preprocessing.peak_finder import peak_finder import Tkinter import extract_nslog_event  Setting basic variables Before we begin any preprocessing, we create variables here to specify what we want to look for. The whole script basically requires two main files. 1. raw_fname: The raw instance of eeg data file in .raw format 2. ns_eventlog: Netstation\u0026rsquo;s event exports in text\nThe selection variable is later used to specify which channels to plot and compare. Note, the first item in the selection list, EB channel is a virtual channel created from bipolar referene.\n# specify sample subject data directory raw_fname = '/Users/Jin/Documents/MATLAB/research/mne-egi/data/sfv_eeg_011ts.raw' ns_eventlog = '/Users/Jin/Documents/MATLAB/research/mne-egi/data/sfv_eeg_011ts_nsevent' # specify sub-sample of channels to look in detail selection = ['EB','E11','E24','E124','E36','E104','E52','E62','E92']  Reading in raw file Raw eeg data can be read in with a simple line below. You can specify montage kind in strings. See https://martinos.org/mne/dev/generated/mne.channels.read_montage.html for available montages. We set preload=True because some of the preprocessing functions require raw file to be preloaded.\nOnce the raw file is loaded, typing raw and raw.info will show details about the raw instance.\nprint('reading raw file...') raw = mne.io.read_raw_egi(raw_fname, montage='GSN-HydroCel-128', preload=True) print('Done!')  reading raw file... \u0026lt;ipython-input-3-8bc42ae4bead\u0026gt;:2: RuntimeWarning: The following EEG sensors did not have a position specified in the selected montage: ['E129']. Their position has been left untouched. raw = mne.io.read_raw_egi(raw_fname, montage='GSN-HydroCel-128', preload=True) Done!  print(raw) # see the first ten list of channel names (note by default, prefix 'E' is appended) print(raw.info['ch_names'][0:10]) # see highpass \u0026amp; lowpass filter print('highpass filter: ' + str(raw.info['highpass'])) print('lowpass filter: ' + str(raw.info['lowpass']))  \u0026lt;RawEGI | sfv_eeg_011ts.raw, n_channels x n_times : 136 x 989490 (4947.4 sec), ~1.00 GB, data loaded\u0026gt; ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10'] highpass filter: 0.0 lowpass filter: 100.0  Applying bandpass filter Our first preprocessing step will be applying the bandpass filter of 1Hz and 30Hz. The numbers can be played around with, but this filter range will potentially remove general noise from environment and slow drifting signals. Other suggestions for highpass is 0.1; for 40 Hz lowpass.\nAfter bandpass filter is applied, type raw.info to check how raw.filter made changes.\n# apply bandpass filter to raw file (highpass, lowpass) raw.filter(1,30) # see highpass \u0026amp; lowpass filter print('highpass filter: ' + str(raw.info['highpass'])) print('lowpass filter: ' + str(raw.info['lowpass']))  highpass filter: 1.0 lowpass filter: 30.0  Creating meta dataframe We will deviate a little from processing raw file, and construct a dataframe that can be later used for effectively creating epochs or querying information we just need. This part uses the custom built module (also experiment specific as each experiment will have different paradigms and event tags). The extract_nslog_event constructs necessary pandas dataframe from ns_eventlog text file which we specified earlier in #Setting-basic-variables.\ncreate_df returns five dataframes, in which nsdata is a list from simply csv-read file that is used to create task-specific pandas dataframes. For example, df_lst is the initial dataframe created that includes all practice, trials, and sentences tasks. The rest of dfs contain task specific data.\n# create pandas data frames for different tasks nsdata, df_lst, df_plst, df_tlst, df_slst = extract_nslog_event.create_df(ns_eventlog)  creating data frame from ns event log... dataframes created for subject 011 trials found: 800 sentences found: 200  From now on, for simplicity sake, we will only examine the actual trials task part in this walkthrough. We can focus on looking at the data structure of trials task. Since the dataframe is already created specifically for trials, what we really want now is the onset (sample numbers) of when the event occured and the condition of the stimuli that was presented.\n# show data frame structure of 2rd index print(df_tlst.iloc[2])  code tlst label lstS onset 99867 cond 4 indx 1 Name: 2, dtype: object  The label for trials task was either a last word start (lstS) or last word end (lstE). Since we are interested in the onset of the word, we will extract just the onsets using the custom code.\n# create onset-only data frame (event tag specifications) df_tlstS = extract_nslog_event.create_df_onset(df_tlst) # show total events of interest len(df_tlstS)  400  Finding impedance check periods and annotating Now that we have dataframes setup, we continue to clean up the raw data. Throughout the acquisition, we ran impedance checks to make sure that all electrodes were in good contact with the scalp and that good signal is being read in. During the impedance check, the waveforms peak in extreme amount and we want to make note of these periods, telling the mne functions to avoid and ignore such periods.\n# find impedance onsets imp_onset, imp_offset, imp_dur = extract_nslog_event.find_impedances(nsdata) # annotate on raw with 'bad' tags (params `reject_by_annotation` will search for 'bad' tags later) annot_imp = mne.Annotations(imp_onset, imp_dur, [\u0026quot;bad imp\u0026quot;] * len(imp_onset), orig_time=raw.info['meas_date']) raw.set_annotations(annot_imp)  finding impedance periods... found 4 impedance periods! \u0026lt;RawEGI | sfv_eeg_011ts.raw, n_channels x n_times : 136 x 989490 (4947.4 sec), ~1.00 GB, data loaded\u0026gt;  Marking bad channels We also want to make note of bad channels. We can manually inspect for bad channels by seeing the actual raw data. raw.plot will show the actual raw file with annotations from above marked as red segments. You can inspect for good/bad channels and manually click on bad channels to mark them bad. Once you manually inspected the channels, type raw.info['bads'] to see how it is updated.\nNote that the plot below is a static figure for example sake. Running the code in ipython will allow us to horizontally and vertically scroll through data. Clicking on a channel will mark that channel red and be considered red. You can see that we\u0026rsquo;ve ran four impedance checks throughout the session (1 task switch period, every 100th trials out fo 400).\n# block=True is useful because it will wait to whatever change you make in the raw file at the plot stage print(raw.plot(bad_color='red', block=True))  Figure(782.64x483.12)  If you already had a list of bad channels noted during the acquisition period, you can skip the above manual inspection and simply specify the bad channels with a line of code:\nraw.info['bads'] = ['E127', 'E107', 'E49', 'E48', 'E115', 'E113', 'E122', 'E121', 'E123', 'E108', 'E63', 'E1'] print(raw.info['bads'])  ['E127', 'E107', 'E49', 'E48', 'E115', 'E113', 'E122', 'E121', 'E123', 'E108', 'E63', 'E1']  Detecting and rejecting eye blinks We\u0026rsquo;ve so far applied generic/broad preprocessing steps, such as bandpass filters, marking chunks of bad segments, and marking bad channels. Now we will look at finding eye blinks in the raw and add more annotations to mark those samples bad.\nStep 1: Setting bipolar reference Because the cap we use do not have EOG-specific channels, we use the channels that are nearest to the eyes and consider those as our virtual eye channels. Thus, such method has the risk of the eye channels actually not having just the eye-related oscillations. This is done by setting the bipolar reference, which is basically the subtraction of two opposing channels (ie. the top and bottom of each eye for eye blinks; the left and right of the eyes for eye movements).\nHere, we use just the right side of the eye only to detect eye blinks. From the subtraction of channel E8 and E126, a virtual channel EB is created.\n# let's begin eye artifact detections print('Starting EOG artifact detection') raw = mne.set_bipolar_reference(raw, ['E8'],['E126'],['EB']) # specify this as the eye channel raw.set_channel_types({'EB': 'eog'}) # double check the changes # print(raw.info['chs'])  Starting EOG artifact detection  Step 2: Detecting eye blinks Now that we have a virtual eye channel to inspect, we can try to identify any eye blinks. Because the virtual eye channel that is created from subtraction of the channels, the waveform of EB channel will be generally flat. You can inspect this by raw.plot.\nEye blinks are generally characterized as two eye channels having sudden opposing peaks. So the methodology is to find a sudden peak within the flat EB line. We have the options of:\n finding eye blinks via mne built in function finding eye blinks via scipy peak finding method  Both results in similar eye blink detections because the methodology of finding local peaks. We will only use the mne built in function and comment out the custom built function that uses scipy. reject_by_annotation will ignore the bad segments marked as bad earlier. The threshold of 0.0001 can be played around with but it is a reasonable threshold set after having manually inspect the data. The events_eog will be an array with [sample number, 0, eventlabel in number]\nevents_eog = eog.find_eog_events(raw, reject_by_annotation=True, thresh=0.0001, verbose=None) # type `help(scipy_annotate_eyeblinks)` for detail # raw = scipy_annotate_eyeblinks(raw, 'EB', 100) print('number of eye blinks detected: ' + str(len(events_eog)))  number of eye blinks detected: 1720  events_eog above will give where the eye blinks occured in samples. We will convert the sample number to seconds so we can annotate on the raw file.\n# get just the sample numbers from the eog events eog_sampleN = [i[0] for i in events_eog] # convert to seconds for annotation-friendly purposes for i in range(0, len(eog_sampleN)): eog_sampleN[i] = eog_sampleN[i] / float(200) # set annotation annot_eog = mne.Annotations(eog_sampleN, [0.1] * len(eog_sampleN), [\u0026quot;bad eye\u0026quot;] * len(eog_sampleN), orig_time = raw.info['meas_date'])  # add this eye blink annotation to the previous annotation by simply adding new_annot = annot_imp + annot_eog raw.set_annotations(new_annot) print('new annotation set!')  new annotation set!  Now that new annotation is set, let\u0026rsquo;s see the changes made to the raw. Again we will just have a figure printed out here. You can see the bad channels marked red (like E1), and bunch of red bars that potentially mark spikes/eye blinks. Because the Times x-axis is so zoomed out, we see all parts being red, but as we see the plot above, that is actually not true. We see that \u0026lsquo;bad eye\u0026rsquo; is annotated for any potential peaks in the \u0026lsquo;EB\u0026rsquo; channel that is newly created from bipolar reference.\n# you can check that more red segments are marked on the raw file print(raw.plot(bad_color='red'))  Figure(782.64x483.12)  Setting rereference Now that bad channels are marked and we know which bad segments to avoid, we will set eeg reference (We want to avoid doing reference before the bad data are marked and rejected).\nprint('setting eeg reference...') raw.set_eeg_reference('average', projection=True)  setting eeg reference... \u0026lt;RawEGI | sfv_eeg_011ts.raw, n_channels x n_times : 135 x 989490 (4947.4 sec), ~1019.5 MB, data loaded\u0026gt;  Creating epochs Now that we have done some primary artifact detections, we can create a first look on how our epochs look. Epochs are time-locked events of interest. Here, we look at the few hundred milliseconds before and after the onset of the last word of a sentence presentation. Before creating the epochs, we will run some custom codes to update the event arrays accordingly so the event labels are properly labeled ie. 1 for onsets, 2 for offsets.\n# first find events related to tlst stim channel in the cleaned raw events_tlst = mne.find_events(raw, stim_channel='tlst') # events_tlst is a array structure ie. (1, 0, 1) and so far, the all the event tags are 1 which is not true # We will update the event tags with 1s and 2s with custom built function # update event ids in mne events array and double check sampling onset timing as sanity check events_tlstS = extract_nslog_event.assign_event_id(df_tlst, events_tlst)  updating mne event array and double checking sampling onset time...  # epoching initially with metadata applied event_id_tlst = dict(lstS=1) tmin = -0.25 # start of each epoch tmax = 0.8 # end of each epoch # set baseline to 0 baseline = (tmin, 0) # picks specify which channels we are interested picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=True, stim=False, exclude='bads') # `metadata` field is used to put in our comprehensive pandas dataframe # it is useful for later creating evoked responses by conditions epochs_tlstS = mne.Epochs(raw, events_tlstS, event_id_tlst, tmin, tmax, proj=False, picks=picks, baseline=baseline, preload=True, reject=None, reject_by_annotation=True, metadata=df_tlstS) print('epochs_tlstS:') print(epochs_tlstS)  epochs_tlstS: \u0026lt;Epochs | 388 events (all good), -0.25 - 0.8 sec, baseline [-0.25, 0], ~72.8 MB, data loaded, with metadata, 'lstS': 388\u0026gt;  We created epochs named epochs_tlstS which is mne\u0026rsquo;s epochs instance. Note that the epochs are 388 instead of original 400. It is likely that the some epochs are dropped from annotations. Let\u0026rsquo;s see if it\u0026rsquo;s true.\n# show drop percentage from mne.Epochs drop_count = 0 for j in range(0, len(epochs_tlstS.drop_log)): if 'bad eye' in epochs_tlstS.drop_log[j]: drop_count += 1 print(str(drop_count) + ' epochs dropped by eog annotation') print('perecentage dropped: ' + str(epochs_tlstS.drop_log_stats()))  12 epochs dropped by eog annotation perecentage dropped: 3.0  Creating evoked response (ERP) Everything looks good. We can create an evoked response by condition. Currently, the epochs_tlst contains all four conditions of the task. By creating an evoked response by condition, we can examine the data for each condition.\n# create evoked respone using pandas query based on metadata created from previous epochs evoked_tlst_c1 = epochs_tlstS[\u0026quot;label=='lstS' and cond=='1'\u0026quot;].average() evoked_tlst_c2 = epochs_tlstS[\u0026quot;label=='lstS' and cond=='2'\u0026quot;].average() evoked_tlst_c3 = epochs_tlstS[\u0026quot;label=='lstS' and cond=='3'\u0026quot;].average() evoked_tlst_c4 = epochs_tlstS[\u0026quot;label=='lstS' and cond=='4'\u0026quot;].average()  Evoked responses are created by condition. Let\u0026rsquo;s just inspect the first one. The figure below will show the waveforms of all channels (except the ones marked bad and bipolar referenced channels) with total N epochs in that condition. Originally, N=100 for each condition.\nWe can see something happening at 100ms to 300ms range after the onset of the word, which is time point 0s.\nprint(evoked_tlst_c1.plot())  Figure(460.8x216)  Figure above is in black and could be hard to inspect. A more interesting plot could be using plot_joint method. You can see that most of the channels in the frontal region are showing flat, insignificant patterns. On the other hand, the right occipital (marked in red, purplish colors) is revealing potentially interesting results.\nprint(evoked_tlst_c1.plot_joint(times='peaks'))  /Users/Jin/Library/Python/2.7/lib/python/site-packages/mne/transforms.py:689: RuntimeWarning: invalid value encountered in divide out[:, 2] = np.arccos(cart[:, 2] / out[:, 0])  Figure(576x302.4)  To avoid going through the same process everytime you load in a subject, we can save the progress by saving the resulted epochs (ie. epochs_tlstS or raw instance). In the other notebook, we will continue with more advanced artifact detection using the saved epochs.\nepochs_tlstS.save('epochs_tlsts-epo.fif', split_size='2GB', fmt='single', verbose=True)  Sources and useful links MNE querying metadata: https://martinos.org/mne/stable/auto_examples/preprocessing/plot_metadata_query.html\nMNE annotations: https://martinos.org/mne/stable/generated/mne.Annotations.html\n","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"c1741a55e6e61866ee701e693039a694","permalink":"http://jinjeon.me/post/eeg-basics/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/post/eeg-basics/","section":"post","summary":"This script runs through sample experiment data from manually reading in raw file to preprocessing through applying filters, eye blink detection using peak finding techniques. Then we create epochs and plot evoked responses.","tags":null,"title":"EEG/Signal Processing--Basics Part 1","type":"post"},{"authors":null,"categories":null,"content":"Shadow casting possess different physical affordances that follow the laws of physics. Different shadow shapes and patterns can be created by manipulating the shadowâ€™s light source direction and intensity, the physical objectâ€™s distance and angle, and the texture of the surface in which the shadow is casted. These physical properties can be used by the operator to create creative patterns with the projected shadows. Some of these physical affordances include playing with the shadowâ€™s movement, superposing different objectâ€™s shadows and scaling the shadowâ€™s size by moving the physical object closer or further from the light source. As aesthetically pleasing with its unique black and white contrast, shadows are a fascinating form of media that can create different textures and details based on the skill of the user.\nShadow Play aims to enhance such a playful shadow experience to more advanced creative form of art by allowing users digitally manipulate their shadows. By adding new digital affordances Shadow Play, users can print and add layers of multiple shadows onto the screen, and invigorate them with animated effects and motions. With such various mix of augmented shadow effects, users can utilize their body as a tool for creating their own unique shadow artwork and animations.\nSee full paper Summary of interaction:\nInput/Output: Shadow Play requires ample activity space for interaction. The system recognizes the user within the activity space, and then displays the digitally augmented shadow onto the projected screen. Optimizing the interaction was one of our key concerns. With camera as an input device for users to communicate with Shadow Play, we wanted to depict the metaphor of users taking pictures and filming the creation. By selecting different features, such as snapshot, mirror, effects, and loop, users can create unique patterns and artwork. Interaction:\n1. Setting the environment: With the shoot feature, users can add layers of current shadows to the previous ones. Users can create static shadows, in which we refer to as background/environment of the artwork. 2. Adding in motion and effects: After the environment has been set, users can enliven the shadow by adding in animated motions and effects. The loop feature, mainly inspired by Instagram, records the motion of shadows and then is automatically looped. By adding in the fireball effect, Shadow Play recognizes the hand motion of reaching out, which then shoots out a fireball from the tip of the hand. 3. Advanced: Mirror and Patterns: The mirror feature allows users to create symmetrical images and animation. The mirror feature is especially useful for creating patterns and symmetrical image, such as butterfly. With the mix of these features, users can create shadow artwork in playful environment. Shadow Play resembles the thought process and planning of an actual painting. Just as an artist would plan ahead where certain objects would be placed within the space before making the strokes, Shadow Play users have to deliberately organize the angle and distance of their body in order to perfect the layers of shadow into a single object shape. Such deliberation allows for more playful and interactive activity. By having the augmented shadows displayed on the screen, there is a loop of feedback and manipulation as users constantly refer to the captured shadow to make adjustments accordingly.\u0026rdquo;\n","date":1512864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512864000,"objectID":"bbb4c6095e8323a9107c60b88b000c0f","permalink":"http://jinjeon.me/project/shadowplay/","publishdate":"2017-12-10T00:00:00Z","relpermalink":"/project/shadowplay/","section":"project","summary":"Shadow Play aims to enhance playful shadow experience to more advanced creative form of art by allowing users digitally manipulate their shadows. By adding new digital affordances Shadow Play, users can print and add layers of multiple shadows onto the screen, and invigorate them with animated effects and motions. With such various mix of augmented shadow effects, users can ...","tags":["ui","hci"],"title":"Tangible UI: Augmented Shadow Play","type":"project"},{"authors":null,"categories":null,"content":"Object classification is essential to human learning as it helps us cope with various stimulus around the world. Regardless of multiple features within a single object, object classification seems to occur seamlessly within our cognitive process. In this experiment, we test how we prioritize each feature within an object and how these features are weighted when we categorize a certain object. Test subjects were given novel shapes that each featured either size, color, or orientation, and had to determine whether the shape belongs to a category of a given prototypical shape. The preliminary result showed that color was the single most determining feature when categorizing an object, showing 72.6% of incorporation in all trials, while orientation was the least with 60.7%, but the differences were not statistically significant. We further went on to use logistic regression to analyze the result, which showed thresholds for identifying a novel object to be in a certain category. However, these thresholds for each feature was not significantly different. The experiment suggests that categorization is more of an elaborate and holistic process that combines different features when categorizing a novel object.\n","date":1494374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494374400,"objectID":"b5ecd274515c9f24ec97a0896b93d7d9","permalink":"http://jinjeon.me/project/featureprioritization/","publishdate":"2017-05-10T00:00:00Z","relpermalink":"/project/featureprioritization/","section":"project","summary":"Object classification is essential to human learning as it helps us cope with various stimulus around the world. Regardless of multiple features within a single object, object classification seems to occur seamlessly within our cognitive process. In this experiment, we test how we prioritize each feature within an object and how these features are weighted when we categorize a certain","tags":["research","cognition","classification"],"title":"Feature Prioritization in Classification of Novel Objects","type":"project"},{"authors":null,"categories":null,"content":"By analyzing Yelpâ€™s dataset, specifically star ratings and text reviews, we created a classifier that predicts whether reviews are positive (star ratings of four or five) or negative (star ratings of one or two). We excluded star ratings of three because we werenâ€™t sure whether they were positive or negative. While Yelpâ€™s star ratings are helpful for concise overview of local businesses, they are also crucial metrics for businesses as the ratings reflect their reputations. However, we realized that star ratings are often misleading as they are subject to user bias and preference. Thus, we wanted to predict ratings solely based on textual features of the reviews and exclude any potential human errors and biases. Performing logistic regression with the combined five features, we were able to correctly predict the reviews with an overall accuracy of 79%.\n","date":1481328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481328000,"objectID":"29805d4af0ee8b04ed5961745fb9fc71","permalink":"http://jinjeon.me/project/yelp/","publishdate":"2016-12-10T00:00:00Z","relpermalink":"/project/yelp/","section":"project","summary":"By analyzing Yelpâ€™s dataset, specifically star ratings and text reviews, we created a classifier that predicts whether reviews are positive (star ratings of four or five) or negative (star ratings of one or two). We excluded star ratings of three because we werenâ€™t sure whether they were positive or negative. While Yelpâ€™s star ratings are helpful for concise overview of local businesses","tags":["research","cognition","classification"],"title":"Yelp Dataset Challenge","type":"project"},{"authors":null,"categories":null,"content":" In Fall 2015, I took a semester off to build my own mobile app. I wanted to develop a niche-specific social media that is just for families. Starting the project with mere passion and lacking practical knowledge, I self-taught how to conduct market research, design and build prototypes. I hired a developer to make tangible results, and won first place in Rehoboth Business Idea Competition with $5k support. I also received free office space from the state government.\n Developed storyboard and high fidelity prototype through iterations of market research, surveys, and user testing   From ideation and product management to recruiting, engaged in every aspect of early stage startup as founder \n\nAbout Homy Problem Statement:\nWe are spending the least time with the most important people in our lives, family, while spending more and more time with strangers, coworkers and friends. The cause of the problem has been generally identified as increased work and commute time, and hectic individual routines.\nDesign Solution:\nHomy attempts to solve this growing social problem by recreating online home and allowing families to seamlessly connect. With unique post-it style fridge page, Homy wants to encourage families to communicate more often. The mission is to provide emotional communication service to enrich familyâ€™s real, offline relationship. Check more at:\nDefining Market Opportunity.pdf\n","date":1447113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1447113600,"objectID":"243227f7887b94f8d75a2ce74e404f6f","permalink":"http://jinjeon.me/project/homy/","publishdate":"2015-11-10T00:00:00Z","relpermalink":"/project/homy/","section":"project","summary":"Homy provides a private space just for families as an emotional communication tool just for families","tags":["startup","ui","prototyping"],"title":"Homy, Private Social Network for Families","type":"project"}]
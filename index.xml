<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jin Jeon</title>
    <link>https://jinjeon.me/</link>
      <atom:link href="https://jinjeon.me/index.xml" rel="self" type="application/rss+xml" />
    <description>Jin Jeon</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© 2021 developed by Jin Jeon with HTML/CSS/Markdown and ‚ù§Ô∏è </copyright><lastBuildDate>Fri, 13 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jinjeon.me/img/icon.png</url>
      <title>Jin Jeon</title>
      <link>https://jinjeon.me/</link>
    </image>
    
    <item>
      <title>SAP Generative research in data comparison habits (NDA)</title>
      <link>https://jinjeon.me/project/data-viz-copy/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/data-viz-copy/</guid>
      <description>

&lt;style&gt;
.introduction {
  column-count: 2;
}
&lt;/style&gt;

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;
&lt;div class=&#34;introduction&#34;&gt;
&lt;b&gt;&lt;h style=&#34;font-family:georgia&#34;&gt;My Role:&lt;/h&gt;&lt;/b&gt;
&lt;br&gt;&lt;small&gt;UX researcher owning the comparison pattern research project &lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;h style=&#34;font-family:georgia&#34;&gt;Methods:&lt;/h&gt;&lt;/b&gt;
&lt;br&gt;&lt;small&gt;&lt;strong&gt;Contextual inquiries&lt;/strong&gt;, &lt;strong&gt;journey maps&lt;/strong&gt;, collaborative workshop facilitation, generative research, preliminary research &lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;h style=&#34;font-family:georgia&#34;&gt;Timeline: &lt;/h&gt;&lt;/b&gt;
&lt;br&gt;&lt;small&gt;June 2021 - Aug 2021 (~3 months)&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;h style=&#34;font-family:georgia&#34;&gt;Stakeholders:&lt;/h&gt;&lt;/b&gt;
&lt;br&gt;&lt;small&gt; UX manager, UX designers and researchers, dev team, PM and product team &lt;/small&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-overview-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Overview &lt;/h&gt;&lt;/h2&gt;

&lt;p&gt;SAP provides various products and software for enterprise resource planning. &lt;mark&gt;&lt;strong&gt;In order to deliver consistent and coherent user experience across SAP products, this project is an initiative to integrate user research into the design process so that design decisions are informed and inspired by user data&lt;/strong&gt;&lt;/mark&gt;. As a starting point, the research focuses on comparison patterns in sourcing product domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer:&lt;/strong&gt; Due to NDA, the page primarily highlights the research process, decision making, reasoning and generalized outcomes.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-objectives-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Objectives &lt;/h&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Identify user needs&lt;/strong&gt; and &lt;strong&gt;pain points&lt;/strong&gt; when comparing data in a procurement process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualize&lt;/strong&gt; users&amp;rsquo; touchpoints of typical use cases through &lt;strong&gt;journey mapping&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Serve research findings to better &lt;strong&gt;inform design decisions&lt;/strong&gt; in building comparison pattern.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-impact-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Impact &lt;/h&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Validated&lt;/strong&gt; major research findings that matched with previous research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provided high-level recommendations&lt;/strong&gt; to address major pain points to stakeholders involving UX framework team, sourcing product team, procurement organization, PM, and dev team.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set the foundation&lt;/strong&gt; for the next stage of the design process which involves ideating and concept testing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-research-question-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Research Question &lt;/h&gt;&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;p style=&#34;font-size: 16pt&#34;&gt;&lt;mark&gt;&lt;em&gt;&amp;ldquo;What are the different use cases that involve comparing data in sourcing?&amp;rdquo;&lt;br&gt;&lt;br&gt;
&lt;mark&lt;em&gt;&amp;ldquo;How does typical workflow look like, and what are the needs and pain points at each touchpoint?&amp;rdquo;&lt;/em&gt;&lt;/mark&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-high-level-research-process-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; High Level Research Process &lt;/h&gt;&lt;/h2&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;research-process.png&#34; data-caption=&#34;The project began with preliminary research, examining what has been done in the past. It served as a guide for designing primary user research. Through a collaborative data synthesis workshop, I was able to engage different stakeholders onboard with the key research findings and deliver recommendations.&#34;&gt;
&lt;img src=&#34;research-process.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The project began with preliminary research, examining what has been done in the past. It served as a guide for designing primary user research. Through a collaborative data synthesis workshop, I was able to engage different stakeholders onboard with the key research findings and deliver recommendations.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-1-preliminary-research-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; 1. Preliminary Research &lt;/h&gt;&lt;/h2&gt;

&lt;h3 id=&#34;goals&#34;&gt;Goals:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;To first better understand the product space in procurement and sourcing&lt;/li&gt;
&lt;li&gt;To understand what has been done in the past to avoid overlapping work&lt;/li&gt;
&lt;li&gt;Identify knowledge gaps&lt;/li&gt;
&lt;li&gt;Use the preliminary research findings as a guide for designing preliminary research&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;at-a-glance-intentionally-not-readable&#34;&gt;At a Glance (Intentionally not readable)&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;preliminary-research.png&#34; data-caption=&#34;Using FigJam, I visually laid out findings from past research.  The white empty spaces indicated potential knowledge gaps.&#34;&gt;
&lt;img src=&#34;preliminary-research.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Using FigJam, I visually laid out findings from past research. &lt;br&gt; &lt;strong&gt;The white empty spaces indicated potential knowledge gaps.&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;outcomes&#34;&gt;Outcomes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Examined 5+ past research work and findings, and &lt;strong&gt;organized into an affinity map to identify knowledge gaps.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Studies were from 2018-2019, involving evaluative research, testing design iterations and concepts. &lt;strong&gt;Because past work often dealt with specific design concepts, it was difficult to generalize findings across different studies.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lot of data was missing or not as well organized.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;insights&#34;&gt;Insights:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preliminary research revealed the knowledge gaps and how past work has been mainly focused on testing design iterations with lack of understanding of the users.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In order to inform future design decisions, the primary user research was designed to focus on foundational research to understand users&amp;rsquo; needs, behaviors, pain points, and typical use cases in their sourcing workflow.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-2-primary-user-research-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; 2. Primary User Research &lt;/h&gt;&lt;/h2&gt;

&lt;h3 id=&#34;goals-1&#34;&gt;Goals:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Identify major use cases and touchpoints in sourcing and data comparison&lt;/li&gt;
&lt;li&gt;Understand users&amp;rsquo; behaviors and pain points&lt;/li&gt;
&lt;li&gt;Develop user journey map of typical use cases&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;study-design&#34;&gt;Study Design:&lt;/h3&gt;

&lt;p&gt;The purpose of the study was &lt;strong&gt;generative&lt;/strong&gt; to understand users&amp;rsquo; behaviors, needs, and pain points.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;60 minutes of &lt;strong&gt;qualitative&lt;/strong&gt; remotely-moderated &lt;strong&gt;contextual inquiries&lt;/strong&gt; and &lt;strong&gt;interviews&lt;/strong&gt; via Respondent.io&lt;/li&gt;
&lt;li&gt;Design of the study was 2 folds:

&lt;ol&gt;
&lt;li&gt;High level start to end process of sourcing&lt;/li&gt;
&lt;li&gt;Detailed step by step of data comparison process from analyzing bids to making decisions&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;study-demographics-at-a-glance&#34;&gt;Study Demographics at a Glance:&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;primary-demographics.png&#34; data-caption=&#34;Participant criteria were strictly controlled to ensure that I spoke with the participants that best matches with the current users. &#34;&gt;
&lt;img src=&#34;primary-demographics.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Participant criteria were strictly controlled to ensure that I spoke with the participants that best matches with the current users. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-3-data-analysis-collaborative-workshop-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; 3. Data Analysis &amp;amp; Collaborative Workshop &lt;/h&gt;&lt;/h2&gt;

&lt;h3 id=&#34;3-1-data-analysis&#34;&gt;3.1 Data Analysis:&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;data-analysis1.png&#34; data-caption=&#34; For the initial data analysis, notes from Excel was converted to Mural whiteboard, and was organized into high level themes. Each participant was color-coded. &#34;&gt;
&lt;img src=&#34;data-analysis1.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt; For the initial data analysis, notes from Excel was converted to Mural whiteboard, and was organized into high level themes. Each participant was color-coded. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;3-2-thematic-analysis-and-journey-mapping&#34;&gt;3.2 Thematic Analysis and Journey Mapping:&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;data-analysis2.png&#34; &gt;
&lt;img src=&#34;data-analysis2.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Each sticky was broken down by corresponding themes.&lt;/li&gt;
&lt;li&gt;By visually laying out the participants&amp;rsquo; workflow from start to end in sourcing, it helped understand the journey of each participant.&lt;/li&gt;
&lt;li&gt;I was able to identify emerging patterns, characterizing behaviors, needs, and major paint points at each touchpoint in the journey.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-3-collaborative-synthesis-workshop&#34;&gt;3.3 Collaborative Synthesis Workshop:&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;workshop.png&#34; &gt;
&lt;img src=&#34;workshop.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Over two days, my supervising mentor and I facilitated a data synthesis workshop to onboard stakeholders with research findings. In order to make the data more digestible, I made sure the data was organized and presented concisely.&lt;/li&gt;
&lt;li&gt;Through affinity mapping, we had two groups of stakeholders move stickies by topics and discuss emerging patterns.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-research-findings-deliverables-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Research Findings &amp;amp; Deliverables &lt;/h&gt;&lt;/h2&gt;

&lt;h3 id=&#34;outcomes-deliverables&#34;&gt;Outcomes &amp;amp; Deliverables:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Identified three major use cases in sourcing&lt;/li&gt;
&lt;li&gt;Developed persona, user story, and journey map for each use case highlighting actions taken, pain points, quotes, and emotions at each touch point.&lt;/li&gt;
&lt;li&gt;Delivered general findings related to sourcing, and data comparison habits

&lt;ul&gt;
&lt;li&gt;Including &lt;strong&gt;high level objectives&lt;/strong&gt; in sourcing and data comparison&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Major pain points&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recommendations for each pain point&lt;/strong&gt; to improve the experience&lt;/li&gt;
&lt;li&gt;Showcasing examples of &lt;strong&gt;data manipulation workflow step by step&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;example-of-journey-map-1-redacted&#34;&gt;Example of Journey Map #1 (Redacted):&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;journey-map.png&#34; data-caption=&#34;Example of journey map of use case #1. Total 3 use cases were presented to the stakeholders. They highlighted the corresponding user story, key descriptions of the persona, behaviors, needs and pain points, quotes, and emotion scale.&#34;&gt;
&lt;img src=&#34;journey-map.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Example of journey map of use case #1.&lt;/b&gt; Total 3 use cases were presented to the stakeholders. They highlighted the corresponding user story, key descriptions of the persona, behaviors, needs and pain points, quotes, and emotion scale.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-limitations-challenges-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Limitations &amp;amp; Challenges &lt;/h&gt;&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;None of the participants were able to share their interactions in the e-procurement tools due to company compliance reasons. As the moderator of the interview sessions, it was my responsibility to maneuver and extract the information needed to address the research questions that have been set out.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Because of time constraints, the scope of the project was focused on the &amp;ldquo;buyer&amp;rdquo; side of the procurement process. To have a better holistic understanding of the entire process, &amp;ldquo;supplier&amp;rdquo; side of research needs to be done.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;h-style-font-family-georgia-lessons-reflections-h&#34;&gt;&lt;h style=&#34;font-family:georgia&#34;&gt; Lessons &amp;amp; Reflections &lt;/h&gt;&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;I learned to engage with stakeholders as early as the project planning stage.&lt;/strong&gt;&lt;br&gt; By weekly updating the stakeholders and asking for their feedback, I was able to plan and develop a research plan that addresses their needs. Moreover, facilitating a collaborative data synthesis workshop session helped better synchronize the stakeholders as a team to share key findings and results.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Knowledge of product domain is essential especially in the B2B space.&lt;/strong&gt;&lt;br&gt; My initial challenge was to get my head around the B2B space and procurement processes within a short period of time. Despite studying the terms, watching tutorial videos, and reading, it took time to really soak the information in.
My way around the challenge was to not only study hard, but also to acknowledge that I am new to the field and asking follow-up clarifying questions during the user interviews.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;It was fascinating to speak to users from all over the world from different industries.&lt;/strong&gt;&lt;br&gt; One major reason that I love user research is being able to speak with users and approaching a problem from users&amp;rsquo; perspectives. During the interview sessions, I was speaking to a consultant in Belgium one day and to a procurement executive in India the next day.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Last but not least, I had fun with amazing mentorship, rich resources, and owning the project.&lt;/strong&gt; &lt;br&gt;The lessons I learned were not limited specifically to user research, but also taught me how to navigate myself through the corporate organization and personal networking.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;back-to-top&#34;&gt;&lt;a href=&#34;#&#34;&gt;Back to top ^&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>WA COVID Exposure Notification Usability Study</title>
      <link>https://jinjeon.me/project/wa-notify/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/wa-notify/</guid>
      <description>

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;page-is-currently-under-development&#34;&gt;Page is currently under development&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;
&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantitative Data Analysis with Statistical Testing</title>
      <link>https://jinjeon.me/post/quant-ux/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/quant-ux/</guid>
      <description>

&lt;p&gt;I use survey data collected from Amazon Mechanical Turk and Reddit user groups (all personal data  have been removed) in a study to examine the impact of cultural localization on web-based account creation between American and Korean users. I use the experiment data to display basic statistical tests in Python.&lt;/p&gt;

&lt;h3 id=&#34;research-question&#34;&gt;Research Question:&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Is there a difference in providing personal information between USA and Korean Internet users &lt;br&gt;
within two different use scenarios: online banking and shopping?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I use the following tests:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#1.-Pearson-Correlation-Coefficient&#34;&gt;Pearson Correlation Coefficient&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#2.-T-Test&#34;&gt;T-Test&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#3.-Mann-Whitney-Test&#34;&gt;Mann-Whitney Test&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#4.-One-Way-Analysis-of-Variance-(ANOVA)&#34;&gt;One-Way Analysis of Variance (ANOVA)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;#5.-Two-Way-ANOVA&#34;&gt;Two-Way ANOVA&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import pandas as pd
import numpy as np
import seaborn as sns
import scipy
from matplotlib import pyplot
import matplotlib.pyplot as plt
from statsmodels.formula.api import ols
import statsmodels.formula.api as smf
import statsmodels.api as sm
from statsmodels.stats.anova import AnovaRM
import pdb  # for debugging
import warnings
warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)

# set color
sns.set_color_codes(&#39;pastel&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;setup-querying-data&#34;&gt;Setup &amp;amp; Querying Data&lt;/h2&gt;

&lt;p&gt;It is first critical to understand the dataframe to play around and make analysis. Usually, &lt;strong&gt;&lt;em&gt;long-format&lt;/em&gt;&lt;/strong&gt; data is desired (or at least I&amp;rsquo;m used to it) for using Python and Seaborn for data visualization. Long format is basically when each variable is represented as a column, and each observation or event is a row. Below, we read in, and query the data.&lt;/p&gt;

&lt;h4 id=&#34;useful-commands&#34;&gt;Useful commands:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;df.head()&lt;/code&gt;: by default, shows first five rows of df&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df.columns()&lt;/code&gt;: prints all the columns in df&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df.describe()&lt;/code&gt;: provides summary description of df&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;pd.read_csv(data, usecols=[&#39;col1&#39;, &#39;col2&#39;, ...,])&lt;/code&gt;: can be used to filter columns&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# read in data.csv file as df &amp;amp; see data structure
df = pd.read_csv(&#39;data.csv&#39;)

# query data by scenario and culture
bank = df.query(&amp;quot;scenario == &#39;Bank&#39;&amp;quot;).copy()
shop = df.query(&amp;quot;scenario == &#39;Shop&#39;&amp;quot;).copy()
kor = df.query(&amp;quot;culture == &#39;Korea&#39;&amp;quot;).copy()
usa = df.query(&amp;quot;culture == &#39;USA&#39;&amp;quot;).copy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# an example of the data structure
usa.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;UserGuid&lt;/th&gt;
      &lt;th&gt;culture&lt;/th&gt;
      &lt;th&gt;scenario&lt;/th&gt;
      &lt;th&gt;interface&lt;/th&gt;
      &lt;th&gt;complete&lt;/th&gt;
      &lt;th&gt;first&lt;/th&gt;
      &lt;th&gt;last&lt;/th&gt;
      &lt;th&gt;phone&lt;/th&gt;
      &lt;th&gt;dob&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;address&lt;/th&gt;
      &lt;th&gt;citizenship&lt;/th&gt;
      &lt;th&gt;website&lt;/th&gt;
      &lt;th&gt;password&lt;/th&gt;
      &lt;th&gt;username&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;reason&lt;/th&gt;
      &lt;th&gt;total&lt;/th&gt;
      &lt;th&gt;total_possible&lt;/th&gt;
      &lt;th&gt;percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.518519&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Shop&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0.208333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.518519&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Shop&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0.208333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;USA&lt;/td&gt;
      &lt;td&gt;Bank&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.037037&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows √ó 24 columns&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;1-pearson-correlation-coefficient&#34;&gt;1. Pearson Correlation Coefficient&lt;/h2&gt;

&lt;p&gt;When we want to ask &lt;em&gt;&amp;ldquo;how strongly correlated are the two variables?&amp;rdquo;&lt;/em&gt;, we can use &lt;strong&gt;Perason&amp;rsquo;s Correlation&lt;/strong&gt;. It is used to measure statistical relationship or association between two &lt;strong&gt;&lt;em&gt;continuous variables&lt;/em&gt;&lt;/strong&gt; that are linearly related to each other. The coefficient value &lt;em&gt;&amp;ldquo;r&amp;rdquo;&lt;/em&gt; ranges from -1 (negative relation) to 1 (perfectly positive). 0 would mean that there is no relationship at all.&lt;/p&gt;

&lt;h3 id=&#34;properties-of-pearson-correlation&#34;&gt;Properties of Pearson Correlation&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;The units of the values do not affect the Pearson Correlation.

&lt;ul&gt;
&lt;li&gt;i.e. Changing the unit of value from cm to inches do not affect the r value&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The correlation between the two variables is symmetric:

&lt;ul&gt;
&lt;li&gt;i.e. A -&amp;gt; B is equal to B -&amp;gt; A&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;** Use &lt;strong&gt;&lt;em&gt;Spearman&amp;rsquo;s Correlation&lt;/em&gt;&lt;/strong&gt; when the two variables have non-linear relationship (e.g. a curve instead of a straight line).&lt;/p&gt;

&lt;h3 id=&#34;code-implementation&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We use scipy package to calculate the Pearson Correlation. The method will return two values: &lt;strong&gt;&lt;em&gt;r&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; value.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let&#39;s look at the correlation of information provided by different scenarios: online banking vs. shopping
# bank[&#39;percent&#39;] will return an array of percentage values

r, p = scipy.stats.pearsonr(bank[&#39;percent&#39;], shop[&#39;percent&#39;])  
print(&#39;r: &#39; + str(r.round(4)))
print(&#39;p: &#39; + str(p.round(4)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;r: 0.7592
p: 0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the results above, we can see &lt;strong&gt;there is a strong positive relationship between the amount of information provided in banking and shopping.&lt;/strong&gt; i.e. Providing information in banking would affect how a user provides personal information in shopping.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-t-test&#34;&gt;2. T-Test&lt;/h2&gt;

&lt;p&gt;When comparing the means of two groups, we can use a &lt;strong&gt;t-test&lt;/strong&gt;. It takes into account of the means and the spread of the data to determine &lt;strong&gt;&lt;em&gt;whether a difference between the two would occur by chance or not&lt;/em&gt;&lt;/strong&gt; (determined by the p-value being less than 0.05 usually). In a t-test, there should be only two independent variables (categorical/nominal variables) and one dependent continuous variable.&lt;/p&gt;

&lt;h3 id=&#34;properties-of-t-test&#34;&gt;Properties of t-test&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The data is assumed to be &lt;strong&gt;normal&lt;/strong&gt; (If the distribution is skewed, use Mann-Whitney test). &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;T-test yields &lt;strong&gt;&lt;em&gt;t&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; value:&lt;br&gt;
2a. &lt;strong&gt;The higher the t, the more difference there is between the two groups.&lt;/strong&gt; The lower the t, the more similar the two groups are.&lt;br&gt;
2b. T-value of 2 means the groups are twice as different from each other than they are within each other&lt;br&gt;
2c. &lt;strong&gt;The lower the p-value, the better&lt;/strong&gt; (meaning that it is significant and the difference did not occure by chance). P-value of 0.05 means that there is 5 percent happening by chance&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;code-implementation-1&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We use scipy package again to run a t-test. Before we decide which test to run, we can quickly plot and see the distribution like below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.distplot(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x1c238f61d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./output_10_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The distribution looks relatively normal. We can run a t-test to see whether there is a difference between the total amount of information provided by the users from each use scenario: i.e. banking vs. shopping&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we run a t-test to see whether there ia a difference in the amount of information provided in each scenario
t, p = scipy.stats.ttest_ind(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent, df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent)
print(&#39;t: &#39; + str(t.round(4)))
print(&#39;p: &#39; + str(p.round(6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t: 4.8203
p: 2e-06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result above shows that there is a significant difference in the amount of information provided between two use scenarios with t-value being high, and p-value being very small. However, we don&amp;rsquo;t actually know which scenario yields more information than the other. The t-test only tells there is a significant difference.&lt;/p&gt;

&lt;p&gt;To find out, we can create a little fancy distribution plot with some box plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;banking = df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent
shopping = df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent

# let&#39;s plot box-dist plot combined
f, (ax_box1, ax_box2, ax_dist) = plt.subplots(3, sharex=True,
                                              gridspec_kw= {&amp;quot;height_ratios&amp;quot;: (0.3, 0.3, 1)})

# add boxplots at the top
sns.boxplot(banking, ax=ax_box1, color=&#39;g&#39;)
sns.boxplot(shopping, ax=ax_box2, color=&#39;m&#39;)
ax_box1.axvline(np.mean(banking), color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_box2.axvline(np.mean(shopping), color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.subplots_adjust(top=0.87)
plt.suptitle(&#39;Amount of information provided by use scenario&#39;, fontsize = 17)

# add distplots below
sns.distplot(banking, ax=ax_dist, label=&#39;Banking&#39;, kde=True, rug=True, color=&#39;g&#39;, norm_hist=True, bins=2)
sns.distplot(shopping, ax=ax_dist, label=&#39;Shopping&#39;, kde=True, rug=True, color=&#39;m&#39;, norm_hist=True, bins=2)

ax_dist.axvline(np.mean(banking), color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_dist.axvline(np.mean(shopping), color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.legend()
plt.xlabel(&#39;Percentage of information&#39;, fontsize=16)
ax_box1.set(xlabel=&#39;&#39;)
ax_box2.set(xlabel=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[Text(0.5, 0, &#39;&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./output_14_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the graph above, we see that the mean of the banking is greater than the mean of shopping. This shows us that regardless of cultural background, users are more likely to provide personal information in the banking scenario.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-mann-whitney-test&#34;&gt;3. Mann-Whitney Test&lt;/h2&gt;

&lt;p&gt;The Mann-Whitney Test allows you to determine if the observed difference is statistically significant without making the assumption that the values are normally distributed. You should have two independent variables and one continuous dependent variable.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-2&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;We can run the test on the same banking vs. shopping scenario.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t, p = scipy.stats.mannwhitneyu(df[df[&#39;scenario&#39;] == &#39;Bank&#39;].percent, df[df[&#39;scenario&#39;] == &#39;Shop&#39;].percent)
print(&#39;t: &#39; + str(t.round(4)))
print(&#39;p: &#39; + str(p.round(6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t: 14795.5
p: 4.1e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;4-one-way-analysis-of-variance-anova&#34;&gt;4. One-Way Analysis of Variance (ANOVA)&lt;/h2&gt;

&lt;p&gt;ANOVA is similar to a t-test, but it is used when there are three or more independent variables (categorical). It assumes normal distribution (use Kruskal-Wallis if abnormal?). One-way ANOVA compares the means between the variables to test whether the difference is statistically significant. However, it does not tell you which specific groups were statistically different from one another. Thus, a post-hoc analysis is required.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-3&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;The result below suggests that there is a statistical difference in the means of the three variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we can create a third variable, and compare the var1, var2, and var3 with one-way ANOVA
var3 = df[df[&#39;culture&#39;] == &#39;USA&#39;].percent
scipy.stats.f_oneway(banking, shopping, var3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;F_onewayResult(statistic=11.171874914065159, pvalue=1.7072783704546878e-05)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;5-two-way-anova&#34;&gt;5. Two-Way ANOVA&lt;/h2&gt;

&lt;p&gt;A two-way ANOVA can be used when you want to know how two independent variables have an interaction effect on a dependent variable. CAVEAT: a two-way ANOVA does not tell which variable is dominant.&lt;/p&gt;

&lt;h3 id=&#34;code-implementation-4&#34;&gt;Code Implementation&lt;/h3&gt;

&lt;p&gt;Below in the code, we see &lt;strong&gt;&lt;em&gt;if there is an interaction effect between culture and scenario use cases on the total amount of information provided.&lt;/em&gt;&lt;/strong&gt; For example, would Americans be more willing to provide personal information than Koreans? If so, does the use case (either banking vs. shopping) affect at all?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we give in a string value of each variable, and the interaction variable &#39;culture:scenario&#39;

model = ols(&#39;percent ~ culture + scenario + culture:scenario&#39;, data=df).fit()
sm.stats.anova_lm(model, typ=2)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sum_sq&lt;/th&gt;
      &lt;th&gt;df&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
      &lt;th&gt;PR(&amp;gt;F)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;culture&lt;/th&gt;
      &lt;td&gt;0.000344&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.007439&lt;/td&gt;
      &lt;td&gt;0.931312&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;scenario&lt;/th&gt;
      &lt;td&gt;1.070130&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;23.159298&lt;/td&gt;
      &lt;td&gt;0.000002&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;culture:scenario&lt;/th&gt;
      &lt;td&gt;0.032834&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.710576&lt;/td&gt;
      &lt;td&gt;0.399772&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Residual&lt;/th&gt;
      &lt;td&gt;17.928461&lt;/td&gt;
      &lt;td&gt;388.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;From the table above, only scenario has a sole effect on the total amount of information provided (depicted as &lt;code&gt;percent&lt;/code&gt; in the dataframe). We see culture, and the interaction of culture and scenario do not have an effect on the amount of information that users provided.&lt;/p&gt;

&lt;p&gt;The finding matches with the previous t-test and graph results, where users provided more information in the banking than they would in shopping.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coffitok</title>
      <link>https://jinjeon.me/project/coffitok/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/coffitok/</guid>
      <description>&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alaska Airlines (Company sponsored summer project/NDA)</title>
      <link>https://jinjeon.me/project/alaska-airlines/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/alaska-airlines/</guid>
      <description>

&lt;style&gt;
.introduction {
  column-count: 2;
}
&lt;/style&gt;

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;
&lt;div class=&#34;introduction&#34;&gt;
&lt;b&gt;My Role:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;UX researcher in a 4-person team of researchers and designers&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Methods:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;generative research, competitive analysis, remote qualitative interview, journey map, affinity diagram&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Timeline: &lt;/b&gt;
&lt;br&gt;&lt;small&gt;June 2020 - Oct 2020 (~4 months)&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Stakeholders:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;Research manager, product designers, &lt;br&gt;brand manager, eCommerce team&lt;/small&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;To help Alaska Airlines flexibly adapt to the COVID-19 pandemic and improve its upsell strategies, the team conducted user research to understand users&amp;rsquo; behaviors, values, and motivations at different stages of their flight travel experiences. We studied Alaska Airlines and five other major airlines operating in the US to conduct competitive analysis and pair insights from user experience benchmarking.&lt;/p&gt;

&lt;h2 id=&#34;impact&#34;&gt;Impact&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Presented four major recommendations to research managers, product and eCommerce teams.&lt;/li&gt;
&lt;li&gt;Research findings matched with the ongoing internal research, suggesting its validity.&lt;/li&gt;
&lt;li&gt;Provided recommendations on how the company in the travel industry can adapt to the pandemic.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;research-question&#34;&gt;Research Question&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;p style=&#34;font-size: 16pt&#34;&gt;&lt;mark&gt;&lt;em&gt;&amp;ldquo;What are the motivations for upgrading throughout the booking and overall flight experience with Alaska and its competitors? Additionally, has the current pandemic affected this experience?&amp;rdquo;&lt;/em&gt;&lt;/mark&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;project-timeline&#34;&gt;Project Timeline&lt;/h2&gt;

&lt;!-- ![Project Timeline](./timeline.svg) --&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;timeline.svg&#34; data-caption=&#34;Our project spanned from late June to October. We initiated our study with reviewing company&amp;rsquo;s past research, examining the different competitors and case studies. We then proceeded with running qualitative interviews on UserTesting.com to get to the hearty meat of this research&#34;&gt;
&lt;img src=&#34;timeline.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Our project spanned from late June to October. We initiated our study with reviewing company&amp;rsquo;s past research, examining the different competitors and case studies. We then proceeded with running qualitative interviews on UserTesting.com to get to the hearty meat of this research
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h2 id=&#34;user-research&#34;&gt;User Research&lt;/h2&gt;

&lt;p&gt;Please note that below are &lt;strong&gt;&lt;em&gt;summarized insights&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;challenges&lt;/em&gt;&lt;/strong&gt; our team had during the user research process.&lt;/p&gt;

&lt;h3 id=&#34;preliminary-competitive-analysis&#34;&gt;Preliminary &amp;amp; Competitive Analysis&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;br&gt;
To first better understand the competitive landscape of the airline industries, we examined the websites of the six airlines to visualize the user flow of booking a flight.&lt;/p&gt;

&lt;p&gt;üí° &lt;strong&gt;Key/fun Fact:&lt;/strong&gt;&lt;br&gt;
Despite the industry standard of generalizing the classification of seat types into economy, business, and first class, we noticed that each airline had different ways of classifying, naming, and promoting the different fare classes. We searched for areas that could be confusing and be improved for the users.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;fareclass.svg&#34; data-caption=&#34;Each airline had its own ways of classifying and naming the seats. Even though the perks that came along with each fare type were quite consistent throughout, low-cost airline, such as Southwest, had a uniquely different seating plan.&#34;&gt;
&lt;img src=&#34;fareclass.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Each airline had its own ways of classifying and naming the seats. Even though the perks that came along with each fare type were quite consistent throughout, low-cost airline, such as Southwest, had a uniquely different seating plan.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h4 id=&#34;insights&#34;&gt;Insights:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Alaska Airlines&lt;/strong&gt;&amp;rsquo; fare types were straightforward and transparent. The seat names are in consistent ascending order: &lt;br&gt;i.e. saver ‚Üí economy ‚Üí premium ‚Üí first class.&lt;/li&gt;
&lt;li&gt;Fare types and their associated perks can become very unclear:&lt;br&gt;
e.g. &lt;strong&gt;JetBlue&lt;/strong&gt;&amp;rsquo;s first class name is &amp;ldquo;Mint&amp;rdquo; which has no association with Blue.&lt;br&gt;
e.g. &lt;strong&gt;SouthWest&lt;/strong&gt;&amp;rsquo;s distinction between basic economy and economy is vague.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;United&lt;/strong&gt; had the most visually busy interface and complex fare types, leading to a poor booking experience.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;qualitative-interview&#34;&gt;Qualitative Interview&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;br&gt;
To understand users&amp;rsquo;:
&lt;ol&gt;
  &lt;li&gt;General flight behavior&lt;/li&gt;
  &lt;li&gt;Top values&lt;br&gt;&lt;/li&gt;
  &lt;li&gt;Pain points&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Methods:&lt;/strong&gt;&lt;br&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Remote Qualitative Interview: &lt;/b&gt;&lt;br&gt;
We conducted a total of 30 interviews (5 participants from 6 different airlines) remotely over UserTesting.com and Zoom. We probed for their general flying behaviors and preferences, and narrowed down to their most recent flight experience (considering both pre-COVID &amp;amp; post-COVID).&lt;/li&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;demographics.svg&#34; data-caption=&#34;We accounted for various factors, including age, gender, income range, and travel behaviors, during our interview to capture the holistic view of participants&amp;rsquo; travel experiences.&#34;&gt;
&lt;img src=&#34;demographics.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;We accounted for various factors, including age, gender, income range, and travel behaviors, during our interview to capture the holistic view of participants&amp;rsquo; travel experiences.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;&lt;li&gt;&lt;b&gt;Interactive Journey Maps:&lt;/b&gt;&lt;br&gt;
We incorporated an &lt;strong&gt;interactive journey map&lt;/strong&gt; session in each interview to have participants walkthrough their most recent experience and capture their moment-to-moment emotions and actions.&lt;/li&gt;&lt;/p&gt;

&lt;p&gt;To capture their moment-to-moment emotions and actions, we divided the entire flight experience into 5 phases:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;booking the flight ticket&lt;/strong&gt; (e.g. web vs. mobile vs. phone)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;before 24 hours of travel&lt;/strong&gt; (includes preparation and traveling to the airport)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;being at the airport&lt;/strong&gt; (checking-in, luggage, security checks, and so on)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;in-flight&lt;/strong&gt; (seat space, perks, food &amp;amp; beverage, etc)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;arrival&lt;/strong&gt; (post travel experience)&lt;/li&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;journeymapping.svg&#34; data-caption=&#34;A simplified abstract example of the interactive journey map: Participants can vertically toggle each action item (as shown in the red arrows) based on their overall experience of that particular event (Good to Poor).&#34;&gt;
&lt;img src=&#34;journeymapping.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;A simplified abstract example of the interactive journey map: Participants can vertically toggle each action item (as shown in the red arrows) based on their overall experience of that particular event (Good to Poor).&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;/ol&gt;&lt;/p&gt;

&lt;h2 id=&#34;thematic-analysis&#34;&gt;Thematic Analysis&lt;/h2&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;themes.svg&#34; data-caption=&#34;We explored multi-facets of the travel experience to search for emerging themes from the qualitative data we collected from interviews and journey maps.&#34;&gt;
&lt;img src=&#34;themes.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;We explored multi-facets of the travel experience to search for emerging themes from the qualitative data we collected from interviews and journey maps.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;affinitymap.svg&#34; data-caption=&#34;Overview of our affinity map. We inductively searched for overarching themes by&amp;hellip;Participant journey map ‚Üí airline journey map ‚Üí all airline journey map. From each participant&amp;rsquo;s journey map, we combined them to characterize the overall experience of each airline. We then combined all airlines to identify any patterns.&#34;&gt;
&lt;img src=&#34;affinitymap.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Overview of our affinity map. We inductively searched for overarching themes by&amp;hellip;&lt;br&gt;Participant journey map ‚Üí airline journey map ‚Üí all airline journey map. &lt;br&gt;From each participant&amp;rsquo;s journey map, we combined them to characterize the overall experience of each airline. We then combined all airlines to identify any patterns.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h2 id=&#34;key-insights-results&#34;&gt;Key Insights &amp;amp; Results&lt;/h2&gt;

&lt;p&gt;Please note that below are &lt;strong&gt;&lt;em&gt;filtered results&lt;/em&gt;&lt;/strong&gt; due to NDA.&lt;/p&gt;

&lt;h3 id=&#34;personas-highlights&#34;&gt;Personas/Highlights&lt;/h3&gt;

&lt;p&gt;Three prototypical personas were developed. The information provides basic demographic information, a hypothetical trip destination, quote from the interview, means used to book and upgrade the seat. It also identifies top values, pain points, and motivations for upgrading a seat. Only 1 of the 3 is shown here.
 &lt;!-- (**we intentionally used the term &#34;highlight&#34; instead of &#34;persona&#34; because they were modeled after participants that stood out with clear preference of values and motivations**) --&gt;
&lt;!-- 












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;persona1.svg&#34; data-caption=&#34;This first type of personas is characterized by how upgrading a seat is mandatory due to his large body size. With his tall height and broad shoulders, upgrading a seat is a necessity.&#34;&gt;
&lt;img src=&#34;persona1.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;This first type of personas is characterized by how upgrading a seat is mandatory due to his large body size. With his tall height and broad shoulders, upgrading a seat is a necessity.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
 --&gt;&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;persona2.svg&#34; data-caption=&#34;The second person is characterized by how traveling in the pandemic era is worst not just because of the safety concerns, but also due to the lack of services. Another unique perspective is the consideration for how family members can buy tickets for other family members.&#34;&gt;
&lt;img src=&#34;persona2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;The second person is characterized by how traveling in the pandemic era is worst not just because of the safety concerns, but also due to the lack of services. Another unique perspective is the consideration for how family members can buy tickets for other family members.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;!-- 












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;persona3.svg&#34; data-caption=&#34;This person is characterized as a bargain hunter, constantly looking for deals and offers. The person is unique in that he is tech savvy and even conscious about which aircraft type he would be flying in.&#34;&gt;
&lt;img src=&#34;persona3.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;This person is characterized as a &lt;i&gt;bargain hunter&lt;/i&gt;, constantly looking for deals and offers. The person is unique in that he is tech savvy and even conscious about which aircraft type he would be flying in.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
 --&gt;

&lt;h3 id=&#34;characterizing-the-travel-experience&#34;&gt;Characterizing the Travel Experience&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;journeymap.svg&#34; data-caption=&#34;An overall experience of Airline X: We visualized a typical journey map, capturing the different motivations for upgrading a seat at each stage of travel. We identified values, emotions, and quotes.&#34;&gt;
&lt;img src=&#34;journeymap.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;An overall experience of Airline X:&lt;/b&gt; &lt;br&gt;We visualized a typical journey map, capturing the different motivations for upgrading a seat at each stage of travel. We identified values, emotions, and quotes.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;We then devised step-by-step recommendations for each travel stage (NDA).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;recommendations&#34;&gt;Recommendations&lt;/h2&gt;

&lt;p&gt;Please note this part is NDA sensitive. Based on key insights and results we synthesized from the data, we came up with four major recommendations.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;recommendations.svg&#34; data-caption=&#34;The four high level recommendations&#34;&gt;
&lt;img src=&#34;recommendations.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;The four high level recommendations&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;üí° &lt;strong&gt;Key Fact:&lt;/strong&gt;&lt;br&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;TRANSPARENCY MATTERS&lt;/strong&gt;: it is a critical factor in the overall experience especially due to COVID. Users desire clear communication on what services and ancillaries to expect when flying during this pandemic era.&lt;/li&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Quantitative metrics, such as &lt;strong&gt;time to complete a booking&lt;/strong&gt;, are &lt;strong&gt;less important&lt;/strong&gt; than &lt;strong&gt;transparency&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Transparency translates to expectations. When expectations are not met, the overall experience worsens.&lt;/li&gt;
&lt;li&gt;Clear communication on what each seat upgrade entails can entice users. &lt;br&gt;e.g. &lt;strong&gt;&amp;ldquo;upgrading a seat will get you a 7-inch wider leg room&amp;rdquo;&lt;/strong&gt;.
&lt;!-- &lt;li&gt;**Users don&#39;t mind receiving few more notifications** for clarifying and communicating the travel information (especially if contains deals). &lt;/li&gt; --&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;design-recommendations&#34;&gt;Design Recommendations&lt;/h3&gt;

&lt;p&gt;Based on the four recommendations, we additionally modified some of the website elements to convey more transparency.&lt;/p&gt;

&lt;h4 id=&#34;1-show-what-services-are-suspended&#34;&gt;1. Show what services are suspended&lt;/h4&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;design_rec1.svg&#34; data-caption=&#34;Click to see in large view/ We added a devoted section that clearly communicates what services are currently suspended especially due to COVID (e.g. in-flight food &amp;amp; beverages). When a user is expecting quality in-flight service but does not receive it, the experience cannot be reverted&#34;&gt;
&lt;img src=&#34;design_rec1.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;/ We added a devoted section that clearly communicates what services are currently suspended especially due to COVID (e.g. in-flight food &amp;amp; beverages). When a user is expecting quality in-flight service but does not receive it, the experience cannot be reverted
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h4 id=&#34;2-clearly-communicate-the-perks-of-upgrading&#34;&gt;2. Clearly communicate the perks of upgrading&lt;/h4&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;design_rec2.svg&#34; data-caption=&#34;Click to see in large view/ Communicating what an upgraded seat entails provide transparency and facilitates the upgrading decision.&#34;&gt;
&lt;img src=&#34;design_rec2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;/ Communicating what an upgraded seat entails provide transparency and facilitates the upgrading decision.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h2 id=&#34;limitations-lessons-learned&#34;&gt;Limitations &amp;amp; Lessons Learned&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The participants were not racially diverse as recruitment of participants were automatically done via UserTesting.com.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Majority of participants&amp;rsquo; recent travels occurred 6 months ago, in which they may have distorted memory of their travel experiences.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In a qualitative research, it&amp;rsquo;s more about finding about the big buckets &amp;amp; themes unlike a quant research. &lt;br&gt;
When presenting the different demographic groups, it can be as easy as describing &amp;ldquo;we examined a wide income group&amp;rdquo; instead of graphing out all the different income groups. If all the demographic information was graphically presented, it can convey the different stakeholders, esp. data scientists, that the results and insights will be quant focused when the research is actually qualitative.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;ending.png&#34; data-caption=&#34;With that, I end with a fun quote from one of our participants üòÑ&#34;&gt;
&lt;img src=&#34;ending.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;With that, I end with a fun quote from one of our participants&lt;/b&gt; üòÑ
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;back-to-top&#34;&gt;&lt;a href=&#34;#&#34;&gt;Back to top ^&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Navigating Cancer (NDA)</title>
      <link>https://jinjeon.me/project/navigating-cancer/</link>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/navigating-cancer/</guid>
      <description>

&lt;style&gt;
.introduction {
  column-count: 2;
}
&lt;/style&gt;

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;
&lt;div class=&#34;introduction&#34;&gt;
&lt;b&gt;My Role:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;UX researcher in a 4-person team of researchers and designers&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Methods:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;survey, interview, affinity diagram, prototyping, heuristic evaluation, usability testing&lt;/small&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Timeline: &lt;/b&gt;
&lt;br&gt;&lt;small&gt;Jul 2020 - Sep 2020 (~3 months)&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Stakeholders:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;product manager, senior designer, clinicians, patients&lt;/small&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;To improve patients&amp;rsquo; experiences with symptom management and engagement with the health tracker, the team conducted &lt;strong&gt;user research to identify needs&lt;/strong&gt;, &lt;strong&gt;concept-tested prototypes&lt;/strong&gt;, and &lt;strong&gt;delivered a high-fidelity prototype&lt;/strong&gt; to the leaders and multiple stakeholders.
&lt;br&gt;&lt;strong&gt;+ With COVID-19&lt;/strong&gt;, the team was working remotely as an international, cross-functional team with each of us in different time zones. I was also flying around being in the US and my hometown Korea, making the project more dynamic!&lt;/p&gt;

&lt;h2 id=&#34;impact&#34;&gt;Impact&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Presented research findings and 4 major feature recommendations to leaders and multiple stakeholders.&lt;/li&gt;
&lt;li&gt;Received strong positive feedbacks from the stakeholders, and recommendations aligned with company&amp;rsquo;s future milestones.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;research-question&#34;&gt;Research Question&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;p style=&#34;font-size: 16pt&#34;&gt;&lt;mark&gt;&lt;em&gt;&amp;ldquo;How might we increase patient engagement with health tracker?&amp;rdquo;&lt;/em&gt;&lt;/mark&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;the-design-process&#34;&gt;The Design Process&lt;/h2&gt;

&lt;h3 id=&#34;unfolding-the-research-through-design-framework&#34;&gt;Unfolding the Research through Design Framework&lt;/h3&gt;

&lt;p&gt;Part of our team&amp;rsquo;s mission was to &lt;strong&gt;evangelize the impact of research&lt;/strong&gt; as the company lacked a dedicated research team. We incorporated the &lt;a href=&#34;https://www.designcouncil.org.uk/news-opinion/what-framework-innovation-design-councils-evolved-double-diamond&#34; target=&#34;_blank&#34;&gt;Double Diamond&lt;/a&gt; design framework to help us guide our research focus by better framing the problem and solving the users&amp;rsquo; underlying needs.&lt;/p&gt;

&lt;p&gt;&lt;mark&gt;This case study will unfold along with the different stages of the Double Diamond.&lt;/mark&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-ideal-double-diamond&#34;&gt;The Ideal Double Diamond&lt;/h3&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;doublediamond.png&#34; data-caption=&#34;The ideal double diamond. Design frameworks help organize the design thinking process by helping researchers better frame the problem and focus on identifying and solving the users‚Äô underlying needs.The model, divided into 4 phases (discover, define, develop, and deliver), maps the divergent and convergent stages of the design process.&#34;&gt;
&lt;img src=&#34;doublediamond.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;The ideal double diamond. Design frameworks help organize the design thinking process by helping researchers better frame the problem and focus on identifying and solving the users‚Äô underlying needs.&lt;br&gt;The model, divided into 4 phases (discover, define, develop, and deliver), maps the divergent and convergent stages of the design process.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h4 id=&#34;but-the-reality&#34;&gt;but the Reality&amp;hellip;&lt;/h4&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;reality.png&#34; data-caption=&#34;The deformed double diamond. While the ideal model is a perfectly-shaped two diamonds, the reality of our research journey was a deformed diamond process. We made multiple pivots along the research, which ended up being a valuable process as we gained greater insights to the problem space.&#34;&gt;
&lt;img src=&#34;reality.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;The deformed double diamond. While the ideal model is a perfectly-shaped two diamonds, the reality of our research journey was a deformed diamond process. We made multiple pivots along the research, which ended up being a valuable process as we gained greater insights to the problem space.&lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h2 id=&#34;user-research&#34;&gt;User Research&lt;/h2&gt;

&lt;p&gt;Please note that below are &lt;strong&gt;&lt;em&gt;summarized insights&lt;/em&gt;&lt;/strong&gt; our team had during the user research process.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;user_research.png&#34; data-caption=&#34;Click to see in large view  Overall high-level research process. We began our research with a general survey. From the insights from the general survey and existing survey results from the company, we conducted interviews with cancer patients and survivors.&#34;&gt;
&lt;img src=&#34;user_research.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt; &lt;br&gt; &lt;strong&gt;Overall high-level research process.&lt;/strong&gt; We began our research with a general survey. From the insights from the general survey and existing survey results from the company, we conducted interviews with cancer patients and survivors.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h3 id=&#34;mark-discover-mark&#34;&gt;&lt;mark&gt;Discover&lt;/mark&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We initiated our research with a general survey sent out online (n=72) to understand the general space of health care and health trackers. It helped us &lt;strong&gt;identify people&amp;rsquo;s health goals and tracking habits.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;From the general survey results, we devised a remote moderated &lt;strong&gt;interview plan to better understand what it feels like to undergo cancer treatments and how they manage their symptoms.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;From the interview with cancer patients and survivors, &lt;strong&gt;affinity mapping helped us categorize their responses&lt;/strong&gt; into various sections, such as their emotions throughout the oncology journey, how they track and manage their symptoms, interaction with the care team and caregivers, and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h4&gt;

&lt;p&gt;Interview with the patients and survivors shed light to clearly understand not just how they manage their symptoms, but &lt;strong&gt;empathize with their adjusted lifestyles and daily challenges.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Through quantitative analysis of the self-reported metrics, I identified that &amp;ldquo;older people&amp;rsquo;s self perception of health level is higher than that of younger people&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Cancer oncology treatments were perceived as &lt;strong&gt;long &amp;ldquo;journeys&amp;rdquo;&lt;/strong&gt; even sometimes with no end.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Symptom management varies by each individual, severity, and cancer types.&lt;/strong&gt; As a researcher and designer, another design challenge was how we can deliver a solution that addresses all cancer types and patients.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;discover.svg&#34; data-caption=&#34;Click to see large view. &#34;&gt;
&lt;img src=&#34;discover.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Click to see large view. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h3 id=&#34;mark-define-mark&#34;&gt;&lt;mark&gt;Define&lt;/mark&gt;&lt;/h3&gt;

&lt;p&gt;Below are only partial insights synthesized.&lt;br&gt;&lt;/p&gt;

&lt;p&gt;üí° &lt;strong&gt;Key Insight:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Identifying the user needs and listing out the stories in a prioritized order immensely helped the team properly guide to the next steps.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We synthesized potential features that would address the user needs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;define.svg&#34; data-caption=&#34;User Stories. These are partial high-level user stories synthesized from the research. &#34;&gt;
&lt;img src=&#34;define.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;User Stories. These are partial high-level user stories synthesized from the research. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h3 id=&#34;mark-develop-mark&#34;&gt;&lt;mark&gt;Develop &lt;/mark&gt;&lt;/h3&gt;

&lt;p&gt;üí° &lt;strong&gt;Key Insights:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;With my background in data visualization, &lt;strong&gt;I was particularly fascinated in developing visual dashboards and data visualization to help patients track their symptoms.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;One key lesson I learned was: &lt;mark&gt;&lt;strong&gt;Designing should always consider the target audience first.&lt;/strong&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Excited to develop data visualization screens, &lt;strong&gt;I initially thought that the more diverse, interactive, and comprehensive graphs are, the better.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Nevertheless, during the expert evaluation and concept-testing, I realized that the screens were simply too busy and users (particularly the old population that are less tech savvy) find the &lt;strong&gt;visuals to be too complicated and less informative.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;develop.svg&#34; data-caption=&#34;Click to see in large view.  Developing prototypes. We began our development with quick sketches using Google&amp;rsquo;s Crazy 8&amp;rsquo;s methodology. We used Figma to develop into a high-fidelity prototypes, which were later concept-tested in the deliver phase &#34;&gt;
&lt;img src=&#34;develop.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Click to see in large view. &lt;br&gt; Developing prototypes. We began our development with quick sketches using Google&amp;rsquo;s Crazy 8&amp;rsquo;s methodology. We used Figma to develop into a high-fidelity prototypes, which were later concept-tested in the deliver phase &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h4 id=&#34;concept-testing&#34;&gt;Concept-testing&lt;/h4&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;concept_test.svg&#34; data-caption=&#34;We conducted concept-testing with 8 users, and organized by each participant. We then categorized by themes and features. &#34;&gt;
&lt;img src=&#34;concept_test.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;We conducted concept-testing with 8 users, and organized by each participant. We then categorized by themes and features. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;concept_test2.svg&#34; data-caption=&#34;Concept-testing helped us narrow down our scope, and focus on key 3 areas, and specifically 4 feature design recommendations. &#34;&gt;
&lt;img src=&#34;concept_test2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Concept-testing helped us narrow down our scope, and focus on key 3 areas, and specifically 4 feature design recommendations. &lt;/b&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;h3 id=&#34;mark-deliver-mark-design-recommendations&#34;&gt;&lt;mark&gt;Deliver&lt;/mark&gt; (Design Recommendations)&lt;/h3&gt;

&lt;p&gt;** &lt;strong&gt;Disclaimer: Below displays only 2 of the total 4 final design recommendations&lt;/strong&gt; **&lt;/p&gt;

&lt;p&gt;In order to ensure that our design solutions stem to delivering the actual user needs, we explicitly stated the user needs that are being met with each design recommendation.&lt;/p&gt;

&lt;h4 id=&#34;1-dashboard-and-data-visualization&#34;&gt;&lt;strong&gt;1. Dashboard and Data Visualization&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Personalized dashboard provides a way to effectively track users&amp;rsquo; past symptoms and even compare and predict how they would feel in the next few days.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-1.svg&#34; &gt;
&lt;img src=&#34;feature-1.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-1.1.svg&#34; &gt;
&lt;img src=&#34;feature-1.1.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-1.2.svg&#34; &gt;
&lt;img src=&#34;feature-1.2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;h4 id=&#34;2-journal&#34;&gt;&lt;strong&gt;2. Journal&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Journal feature lets users record their symptoms through various means. &lt;strong&gt;Considering accessibility and finger &amp;ldquo;tickling&amp;rdquo; &amp;amp; &amp;ldquo;numbness&amp;rdquo; being a common symptom of cancer, we include voice memo, photo upload, and emotion scale&lt;/strong&gt; to faciliate their input.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-2.svg&#34; &gt;
&lt;img src=&#34;feature-2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-2.1.svg&#34; &gt;
&lt;img src=&#34;feature-2.1.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;feature-2.2.svg&#34; &gt;
&lt;img src=&#34;feature-2.2.svg&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;To learn more about this project, please reach out to me!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;reflections&#34;&gt;Reflections&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Interviewing minorities and patients could be overwhelming at first. I thought I needed to know all the jargons related to cancer and oncology treatments. While it would be great to know all these details, acknowledging that you are &amp;ldquo;learning&amp;rdquo; in this space and trying to hear more from interviewees&amp;rsquo; experience not only helps lighten the atmosphere but also gives patients more confidence.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;User centered design approach is critical: For designing data visualizations, more comprehensive data, interaction, and fancy features do not translate to happy experience.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;There are fine lines of products requiring FDA approval, and fortunately, this project did not require it.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Coding &amp; Data Visualizations</title>
      <link>https://jinjeon.me/project/data-viz/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/data-viz/</guid>
      <description>

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;interactive-art-image-reconstruction-using-perlin-noise-p5-js-javascript&#34;&gt;Interactive Art: Image Reconstruction using Perlin Noise (p5.js Javascript)&lt;/h2&gt;

&lt;p&gt;&lt;/div&gt;&lt;iframe src=&#34;https://editor.p5js.org/jeon11/embed/w2Ugnl4dR&#34; width=&#34;760&#34; height=&#34;600&#34;&gt;Image Reconstruction using Perlin Noise (Javascript- p5.js) &lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;The image above uses Perlin Noise to reconstruct old classic artworks, which is useful for generating patterns. Starting from a blank screen, it creates thousands of random noises that start recovering the image. They do so by reading the color of the image coordinates. The noises diffuse by generating random polygons spreading around, filling in the gap.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Refresh the page to see different image creations (total 4 images).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Drag the mouse to draw circles on top of the noises.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;page-is-currently-under-development&#34;&gt;Page is currently under development&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;
&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conference Presentations</title>
      <link>https://jinjeon.me/project/conference/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project/conference/</guid>
      <description>












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://jinjeon.me/pdf/cns20.pdf&#34; data-caption=&#34;Click to see in large viewCutler RA, Jeon J, Polyn SM. Characterizing the interaction of temporal and semantic information in categorized memory search. Poster presented at: Cognitive Neuroscience Society; 2020 May 2-5; Virtual&#34;&gt;
&lt;img src=&#34;https://jinjeon.me/pdf/cns20.pdf&#34; alt=&#34;&#34; width=&#34;&amp;#39;max&amp;#39;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Click to see in large view&lt;/b&gt;&lt;br&gt;Cutler RA, Jeon J, Polyn SM. Characterizing the interaction of temporal and semantic information in categorized memory search. Poster presented at: Cognitive Neuroscience Society; 2020 May 2-5; Virtual
  &lt;/figcaption&gt;


&lt;/figure&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://jinjeon.me/pdf/Psychonomics19.pdf&#34; data-caption=&#34;Click to see in large viewCutler RA, Jeon J, Brown-Schmidt S, Polyn SM. Semantic and temporal structure in memory for narratives: A benefit for semantically congruent ideas. Poster presented at: Psychonomic Society; 2019 Nov 14-17; Montreal, QC&#34;&gt;
&lt;img src=&#34;https://jinjeon.me/pdf/Psychonomics19.pdf&#34; alt=&#34;&#34; width=&#34;&amp;#39;max&amp;#39;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;b&gt;Click to see in large view&lt;/b&gt;&lt;br&gt;Cutler RA, Jeon J, Brown-Schmidt S, Polyn SM. Semantic and temporal structure in memory for narratives: A benefit for semantically congruent ideas. Poster presented at: Psychonomic Society; 2019 Nov 14-17; Montreal, QC
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Theory: Mutual Information</title>
      <link>https://jinjeon.me/post/mutual-info/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/mutual-info/</guid>
      <description>

&lt;h2 id=&#34;entropy-in-neuroimaging&#34;&gt;Entropy in Neuroimaging&lt;/h2&gt;

&lt;p&gt;Entropy has three interpretations (three are identical, but in different expressions &amp;amp; relations):&lt;/p&gt;

&lt;p&gt;&lt;b&gt;1. Amount of information in an event (N of possible outcomes, or grey value in images)&lt;/b&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The larger the number of possible outcomes, the larger the information gain

&lt;ul&gt;
&lt;li&gt;Ex. Information gain from a sentence would exponentially increase with length of sentence&lt;br&gt;&lt;/li&gt;
&lt;li&gt;If outcome is 1, information gain is 0 (i.e. log1 = 0)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;b&gt;2. Uncertainty of outcome in an event&lt;/b&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amount of information gain with probability is inversely related to the probability that the event will take place. The information per event is weighted by the probability of the event&lt;/li&gt;&lt;/li&gt;
&lt;li&gt;The rarer an event, the more significance the event has&lt;/li&gt;
&lt;li&gt;When all events are likely to occur, uncertainty or entropy is maximum (ie. more possible outcomes)&lt;/li&gt;
&lt;li&gt;Most common entropy form is the Shannon&amp;rsquo;s entropy:
\begin{equation&lt;em&gt;}
H = \sum_{i} p_i log(p_i)
\end{equation&lt;/em&gt;}&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Entropy Example&lt;/strong&gt; &lt;br&gt;
    &amp;gt;  In a fair coin toss, entropy is maximum. Vice versa, the more unfair the coint toss is, the more definitive the outcome is (which means lower entropy)&lt;br&gt;&lt;br&gt;
    &amp;gt;  Fair coin toss: P(head) = 0.5, P(tail) = 0.5&lt;br&gt;
    &amp;gt;  Entropy = -0.5log0.5 - 0.5log0.5 = 0.150 + 0.150 = 0.300
    &amp;gt;&lt;br /&gt;
    &amp;gt;  Unfair coin toss: P(head) = 0.8, P(tail) = 0.2&lt;br&gt;
    &amp;gt;  Entropy = -0.8log0.8 - 0.2log0.2 = 0.077 + 0.140 = 0.217&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;b&gt;3. Dispersion of probability distribution&lt;/b&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shannon&amp;rsquo;s entropy can be used as a measure of dispersion of a probability distribution. It can be computed on images by looking at their dispersion of grey values&lt;/li&gt;
&lt;li&gt;Image with single intensity will have low entropy value as it contains little information. Conversely, if image with varying intesity will have higher entropy value with more information&lt;/li&gt;
&lt;li&gt;Ex. image with single sharp peak (ie. grey value condensed in small area) will have low entropy value&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h2&gt;

&lt;p&gt;The goal of registration is to maximize mutual information or the overlaps. There are three ways of interpreting MI, in which they are identical but in different forms of expression and relation of variables.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;I(X, Y) = H(Y) - H(Y | X)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is the most closest form of mutual information. ie. MI of X and Y is subtracting entropy of H(Y) from the conditional entropy H(Y|X) (or p(Y) given p(X): chance of grey value in B given that corresponding image in A has grey value).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In expression of uncertainty, MI is the amount by which uncertainty about Y changes when the amount of X containing Y is given
&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;I(X, Y) = H(X) + H(Y) - H(X, Y)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is the most closest form to joint entropy. &lt;em&gt;H(X | Y)&lt;/em&gt; tells us that mutual information is greater when the joint entropy is lower. Small entropy or less dispersion would mean that information overlaps more.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;I(X, Y) = Sum of [p(x,y) log (p(x,y) / p(x)p(y))&lt;/em&gt;&lt;/strong&gt;]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This formula is analogous to Kullback-Leibler distance, which measures the distance between two distributions. It measures the dependence of the two images by calculating the distance between the joint distribution of the image&amp;rsquo;s grey values p(x,y) and the joint distribution in case of independence of the two images p(x)p(y)&lt;/li&gt;
&lt;li&gt;We will use this formula to measure MI later in the code&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;code-implementation&#34;&gt;Code Implementation&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s try using Python to measure mutual information of given images. We will be mainly comparing in two ways: comparing the identical images, and two different images.&lt;/p&gt;

&lt;h4 id=&#34;1-let-s-begin-with-a-setup-and-direct-the-image-files&#34;&gt;1. Let&amp;rsquo;s begin with a setup, and direct the image files&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import division
import numpy as np
import matplotlib.pyplot as plt
import os
import nibabel as nib

# set gray colormap and nearest neighbor interpolation by default
plt.rcParams[&#39;image.cmap&#39;] = &#39;gray&#39;
plt.rcParams[&#39;image.interpolation&#39;] = &#39;nearest&#39;

# set the images
os.chdir(&#39;/Users/Jin/Documents/MATLAB&#39;)

img1 = &#39;4_23_Drake.nii&#39;
img2 = &#39;4_24_Denzel_Washington.nii&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-let-s-slice-the-image-and-set-side-by-side-to-display&#34;&gt;2. Let&amp;rsquo;s slice the image and set side by side to display&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_img_slice(img, size=50):
    &amp;quot;&amp;quot;&amp;quot;
    load the image using nibabel and slice the image by given size

    Parameters
    ----------
    img: nii image data read via nibabel


    Returns
    -------
    numpy memmap: ndarray of image slice
    &amp;quot;&amp;quot;&amp;quot;
    img_data = nib.load(img)
    img_data = img_data.get_data()
    img_slice = img_data[:, :, size]  # 50 is arbitrary here
    # convert any nans to 0s
    img_nans = np.isnan(img_slice)
    img_slice[img_nans] = 0
    return img_slice

img1_slice = get_img_slice(img1)
img2_slice = get_img_slice(img2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# display images left and right
def plot_raw(img1, img2):
    plt.imshow(np.hstack((img1, img2)))
    plt.show()

plot_raw(img1_slice, img1_slice)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_6_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-let-s-plot-a-1d-histogram-for-each-of-the-two-iamges&#34;&gt;3. Let&amp;rsquo;s plot a 1d histogram for each of the two iamges&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_hist1d(img1, img2, bins=20):
    &amp;quot;&amp;quot;&amp;quot;
    one dimensional histogram of the slices

    Parameters
    ----------
    img1: nii image data read via nibabel

    img2: nii image data read via nibabel

    bins: optional (default=20)
        bin size of the histogram

    Returns
    -------
    histogram
        comparing two images side by side
    &amp;quot;&amp;quot;&amp;quot;
    fig, axes = plt.subplots(1, 2)
    axes[0].hist(img1.ravel(), bins)
    axes[0].set_title(&#39;Img1 histogram&#39;)
    axes[1].hist(img2.ravel(), bins)
    axes[1].set_title(&#39;Img2 histogram&#39;)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_hist1d(img1_slice, img2_slice)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_9_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-let-s-plot-the-two-images-against-each-other-on-a-scatter-plot-and-calculate-correlation-coefficient&#34;&gt;4. Let&amp;rsquo;s plot the two images against each other on a scatter plot, and calculate correlation coefficient&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_scatter2d(img1, img2):
    &amp;quot;&amp;quot;&amp;quot;
    plot the two image&#39;s histogram against each other

    Parameters
    ----------
    img1: nii image data read via nibabel

    img2: nii image data read via nibabel

    Returns
    -------
    2d plotting of the two images and correlation coeeficient
    &amp;quot;&amp;quot;&amp;quot;
    corr = np.corrcoef(img1.ravel(), img2.ravel())[0, 1]
    plt.plot(img1.ravel(), img2.ravel(), &#39;.&#39;)

    plt.xlabel(&#39;Img1 signal&#39;)
    plt.ylabel(&#39;Img2 signal&#39;)
    plt.title(&#39;Img1 vs Img2 signal cc=&#39; + str(corr))
    plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this one is comparing two identical images so it should equal 1
plot_scatter2d(img1_slice, img1_slice)

# image 1 vs image 2
plot_scatter2d(img1_slice, img2_slice)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_12_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_12_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;feature-space&#34;&gt;Feature Space&lt;/h3&gt;

&lt;p&gt;Using MI as a registration measure, we plot a &lt;strong&gt;&lt;em&gt;feature space&lt;/em&gt;&lt;/strong&gt; (or &lt;strong&gt;&lt;em&gt;joint histogram&lt;/em&gt;&lt;/strong&gt;), a two-dimensional plot showing the combinations of grey values in each of the two images for all corresponding points. For example, for each corresponding point (x, y), in which x and y are coordinates of first and second images respectively,&lt;/p&gt;

&lt;p&gt;As the alignment of the two images change, the feature space changes. The more correctly registered the two images are, the more anatomical structures will overlap, showing clusters for the grey values. When the images are misaligned, the intensity of the clusters for certain structures will decrease, and a new pair of (x, y) will be matched as the image gets incorrectly aligned with other nearby structures of the other image. This is be shown as the dispersion of the clustering.&lt;/p&gt;

&lt;h4 id=&#34;5-let-s-plot-a-joint-histogram-now&#34;&gt;5. Let&amp;rsquo;s plot a joint histogram now&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_joint_histogram(img1, img2, bins=20, log=True):
    &amp;quot;&amp;quot;&amp;quot;
    plot feature space. Given two images, the feature space is constructed by counting the number of times a combination of grey values occur

    Parameters
    ----------
    img1: nii image data read via nibabel

    img2: nii image data read via nibabel

    bins: optional (default=20)
        bin size of the histogram
    log: boolean (default=True)
        keeping it true will show a better contrasted image

    Returns
    -------
    joint histogram
        feature space of the two images in graph

    &amp;quot;&amp;quot;&amp;quot;
    hist_2d, x_edges, y_edges = np.histogram2d(img1.ravel(), img2.ravel(), bins)
    # transpose to put the T1 bins on the horizontal axis and use &#39;lower&#39; to put 0, 0 at the bottom of the plot
    if not log:
        plt.imshow(hist_2d.T, origin=&#39;lower&#39;)
        plt.xlabel(&#39;Img1 signal bin&#39;)
        plt.ylabel(&#39;Img2 signal bin&#39;)

    # log the values to reduce the bins with large values
    hist_2d_log = np.zeros(hist_2d.shape)
    non_zeros = hist_2d != 0
    hist_2d_log[non_zeros] = np.log(hist_2d[non_zeros])
    plt.imshow(hist_2d_log.T, origin=&#39;lower&#39;)
    plt.xlabel(&#39;Img1 signal bin&#39;)
    plt.ylabel(&#39;Img2 signal bin&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this should print a linear graph as it&#39;s comparing it to itself
print(plot_joint_histogram(img1_slice, img1_slice))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_15_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# compare images 1 and 2
print(plot_joint_histogram(img1_slice, img2_slice))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./mutual_information_neuroimaging_16_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;6-let-s-calculate-the-mutual-information-of-the-two-images-now&#34;&gt;6. Let&amp;rsquo;s calculate the mutual information of the two images now.&lt;/h4&gt;

&lt;p&gt;We use the third formula stated above to measure the overlaps of the two images. The goal of registration is to maximize mutual information or the overlaps.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def mutual_information(img1, img2, bins=20):
    &amp;quot;&amp;quot;&amp;quot;
    measure the mutual information of the given two images

    Parameters
    ----------
    img1: nii image data read via nibabel

    img2: nii image data read via nibabel

    bins: optional (default=20)
        bin size of the histogram

    Returns
    -------
    calculated mutual information: float

    &amp;quot;&amp;quot;&amp;quot;
    hist_2d, x_edges, y_edges = np.histogram2d(img1.ravel(), img2.ravel(), bins)

    # convert bins counts to probability values
    pxy = hist_2d / float(np.sum(hist_2d))
    px = np.sum(pxy, axis=1)  # marginal x over y
    py = np.sum(pxy, axis=0)  # marginal y over x
    px_py = px[:, None] * py[None, :]  # broadcast to multiply marginals

    # now we can do the calculation using the pxy, px_py 2D arrays
    nonzeros = pxy &amp;gt; 0  # filer out the zero values
    return np.sum(pxy[nonzeros] * np.log(pxy[nonzeros] / px_py[nonzeros]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the MI value of the first should be greater than the second as the first is comparing the image to itself
print(mutual_information(img1_slice, img1_slice))
print(mutual_information(img1_slice, img2_slice))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.1967155090861803
0.20578049748917815
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculating-the-grand-average-of-mutual-information&#34;&gt;Calculating the Grand Average of Mutual Information&lt;/h2&gt;

&lt;p&gt;The codes above are detailed step-by-step processes. We can condense the codes above into one useful function that will allow us to examine the overall average of mutual information of each every scan image to the register image.&lt;/p&gt;

&lt;p&gt;For example, given a register type (&amp;lsquo;rtf&amp;rsquo; for register to first image, or &amp;lsquo;rtm&amp;rsquo; for register to mean), it will calculate each scan to the register image, calculate the MI, and return the grand average MI value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The code is currently based on ACCRE system so the paths directing to the images must be changed for flexible use. The path should direct to the folder which holds all the scan images, and since it requires gigabytes of data, I had to implement the code on ACCRE-use only.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_avg_mi(subjID, type=&#39;rtf&#39;, saveText=False, verbose=False, nScans=187):
    &amp;quot;&amp;quot;&amp;quot;
    calculates the correlation coefficient and mutual information of the registered image to the rest of image files, and returns the grand average and trial averages

    Parameters
    ----------
    subjID : str
        string of subject data (ie. &#39;cdcatmr011&#39;)
    type : str {&#39;rtf&#39;, &#39;rtm&#39;}
        registration type that would either register to first scan image or the mean image (default: &#39;rtf&#39;)
    saveText : boolean
        if True, will save the table report to a separate text file (default: False)
    verbose : boolean
        if True, will print out which scan file is being worked on (default: False)
    nScans : int
        integer of how many scans to expect per each run (default: 187)

    Returns
    -------
    cc_all: list
        list of all correlation coefficient values
    mi_all: list
        list of all mutual information values

    &amp;quot;&amp;quot;&amp;quot;
    saveText = False
    subj = str(subjID)
    baseDir = &#39;/scratch/polynlab/fmri/cdcatmr/&#39;
    funcDir = baseDir + subj + &#39;/images/func&#39;
    tag = &#39;func&#39;
    if type == &#39;rtf&#39;:  # register to first
        sourceImg = &#39;/func1/func1_00001.nii&#39;  # [rmeanfunc1_00001.nii, meanfunc1_00001.nii]
        # tag = &#39;func&#39;
    elif type == &#39;rtm&#39;:  # register to mean
        sourceImg = &#39;/func1/meanfunc1_00001.nii&#39;
        # tag = &#39;rfunc&#39;
    resDir = &#39;/home/jeonj1/proj/mi&#39;

    os.chdir(funcDir)
    funcs = os.listdir(funcDir)
    funcs.sort()  # funcs = [&#39;func1&#39;, &#39;func2&#39;, ... &#39;func7&#39;, &#39;func8&#39;]

    meanImg = mi.get_img_slice(funcDir + sourceImg, verbose=verbose)
    mi_listVar = []
    cc_listVar = []
    # loop by each functional run
    for i in range(0, len(funcs)):
        curr_func = funcs[i]
        # let&#39;s first create list variables for each functional run
        temp = &#39;&#39;
        temp = &#39;mi_&#39; + curr_func + &#39; = []&#39;
        exec(temp)
        mi_listVar.append(&#39;mi_&#39; + curr_func)
        temp = &#39;&#39;
        temp = &#39;cc_&#39; + curr_func + &#39; = []&#39;
        exec(temp)
        cc_listVar.append(&#39;cc_&#39; + curr_func)

        # now let&#39;s read in each functional run folder
        cfuncDir = funcDir + &#39;/&#39; + curr_func
        os.chdir(cfuncDir)
        nii_files = os.listdir(cfuncDir)
        nii_files = [x for x in nii_files if x.startswith(tag) and x.endswith(&#39;nii&#39;)]
        nii_files.sort()

        # sanity check: count scan files
        assert len(nii_files) == nScans, &amp;quot;total scan files found do not match &amp;quot; + str(nScans) + &amp;quot; for func run &amp;quot; + str(i+1)

        # loop by each scan within run
        for j in range(0, nScans):
            if verbose:
                print(&#39;starting &#39; + curr_func + &#39; | scan &#39; + str(j+1))
            curr_nii = mi.get_img_slice(nii_files[j], verbose=verbose)
            corr = mi.get_core(meanImg, curr_nii)
            mutual_info = mi.mutual_information(meanImg, curr_nii)

            # append each list
            temp_cc = &#39;cc_&#39; + curr_func + &#39;.append(corr)&#39;
            exec(temp_cc)
            temp_mi = &#39;mi_&#39; + curr_func + &#39;.append(mutual_info)&#39;
            exec(temp_mi)

    cc_sums = []
    mi_sums = []
    for r in range(0, len(funcs)):
        cc_sums.append(sum(eval(cc_listVar[r])))
        mi_sums.append(sum(eval(mi_listVar[r])))

    # get all entries in a single list
    cc_all = []
    mi_all = []
    for r in range(0, len(funcs)):
        cc_all = cc_all + eval(cc_listVar[r])
        mi_all = mi_all + eval(mi_listVar[r])


    reports = []
    reports.append([&#39;avg cc&#39;, sum(cc_sums)/(nScans*len(funcs))])
    reports.append([&#39;avg mi&#39;, sum(mi_sums)/(nScans*len(funcs))])
    reports.append([&#39;cc by runs&#39;, &#39;-----&#39;])
    for l in cc_listVar:
        reports.append([l, sum(eval(l))/nScans])
    reports.append([&#39;mi by runs&#39;, &#39;-----&#39;])
    for l in mi_listVar:
        reports.append([l, sum(eval(l))/nScans])

    print(tabulate(reports))

    if saveText:
        with open(resDir + &#39;/&#39; + subj + &#39;_mi_&#39; + os.path.basename(sourceImg) + &#39;.txt&#39;, &#39;w&#39;) as f:
            for item in reports:
                f.write(&amp;quot;%s\n&amp;quot; % item)
        print(&#39;saved the table report to &#39; + resDir)

    return cc_all, mi_all

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;t-test-and-plotting&#34;&gt;T-test and Plotting&lt;/h2&gt;

&lt;p&gt;We can use the function &lt;code&gt;get_avg_mi&lt;/code&gt; above to calculate the grand average. Then we will run a t-test on the two averages and plot the differences for visual reference. The script below is currently somewhat hard coded for simplicity sake.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nScans = 187
nRuns = 8

# we use the get_avg_mi function stated above. Specify the subject and register type.
cc1, mi1 = get_avg_mi(&#39;cdcatmr066&#39;, &#39;rtf&#39;, verbose=True)  # register to first scan
cc2, mi2 = get_avg_mi(&#39;cdcatmr066&#39;, &#39;rtm&#39;, verbose=True)  # register to mean

assert len(cc1) == len(cc2) == nScans * nRuns
assert len(mi1) == len(mi2) == nScans * nRuns

cc_t, cc_p = stats.ttest_ind(cc1, cc2)
mi_t, mi_p = stats.ttest_ind(mi1, mi2)


import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# switch matplotlib backend so that it knows it won&#39;t print anything.
plt.switch_backend(&#39;agg&#39;)

# sns.set(color_codes=True)
sns.set_color_codes(&#39;pastel&#39;)
# sns.set_palette(&amp;quot;tab10&amp;quot;)

plt.figure(0)
sns.distplot(cc1, label=&#39;rtf&#39;);
sns.distplot(cc2, label=&#39;rtm&#39;);
plt.legend()
plt.title(&#39;correlation coefficient n=&#39; + str(len(cc1)))
plt.savefig(&#39;/home/jeonj1/proj/mi/cc_distplot.png&#39;)

plt.figure(1)
sns.distplot(mi1, label=&#39;rtf&#39;);
sns.distplot(mi2, label=&#39;rtm&#39;);
plt.legend()
plt.title(&#39;mutual info | n=&#39; + str(len(mi1)))
plt.savefig(&#39;/home/jeonj1/proj/mi/mi_distplot.png&#39;)


# let&#39;s plot box-dist plot combined
f, (ax_box1, ax_box2, ax_dist) = plt.subplots(3, sharex=True, gridspec_kw= {&amp;quot;height_ratios&amp;quot;: (0.3, 0.3, 1)})

cc1_mean = np.mean(cc1)
cc2_mean = np.mean(cc2)

sns.boxplot(cc1, ax=ax_box1, color=&#39;b&#39;)
sns.boxplot(cc2, ax=ax_box2, color=&#39;r&#39;)
ax_box1.axvline(cc1_mean, color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_box2.axvline(cc2_mean, color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.subplots_adjust(top=0.87)
plt.suptitle(&#39;correlation coefficient n=&#39; + str(len(cc1)), fontsize = 16)

sns.distplot(cc1, ax=ax_dist, label=&#39;rtf&#39;, color=&#39;b&#39;, norm_hist=True)
sns.distplot(cc2, ax=ax_dist, label=&#39;rtm&#39;, color=&#39;r&#39;, norm_hist=True)
ax_dist.axvline(cc1_mean, color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_dist.axvline(cc2_mean, color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.legend()
ax_box1.set(xlabel=&#39;&#39;)
ax_box2.set(xlabel=&#39;&#39;)
plt.savefig(&#39;/home/jeonj1/proj/mi/cc_box_distplot.png&#39;)


# let&#39;s plot box-dist plot combined
f, (ax_box1, ax_box2, ax_dist) = plt.subplots(3, sharex=True, gridspec_kw= {&amp;quot;height_ratios&amp;quot;: (0.3, 0.3, 1)})

mi1_mean = np.mean(mi1)
mi2_mean = np.mean(mi2)

sns.boxplot(mi1, ax=ax_box1, color=&#39;b&#39;)
sns.boxplot(mi2, ax=ax_box2, color=&#39;r&#39;)
ax_box1.axvline(mi1_mean, color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_box2.axvline(mi2_mean, color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.subplots_adjust(top=0.87)
plt.suptitle(&#39;mutual information n=&#39; + str(len(cc1)), fontsize = 16)

sns.distplot(mi1, ax=ax_dist, label=&#39;rtf&#39;, color=&#39;b&#39;, norm_hist=True)
sns.distplot(mi2, ax=ax_dist, label=&#39;rtm&#39;, color=&#39;r&#39;, norm_hist=True)
ax_dist.axvline(mi1_mean, color=&#39;g&#39;, linestyle=&#39;--&#39;)
ax_dist.axvline(mi2_mean, color=&#39;m&#39;, linestyle=&#39;--&#39;)
plt.legend()
ax_box1.set(xlabel=&#39;&#39;)
ax_box2.set(xlabel=&#39;&#39;)
plt.savefig(&#39;/home/jeonj1/proj/mi/mi_box_distplot.png&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following code above will generate two joint plots that combine the box plot and distribution plot for correlation coefficient and mutual information. The first image below shows the correlation coefficient distribution (note that the max is at 1.00). The N here represents the total number of scan images that were compared.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cc_box_distplot.png&#34; alt=&#34;CC distribution plot for rtf vs rtm&#34; title=&#34;CC distribution plot for rtf vs rtm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The image below is what we&amp;rsquo;re really interested in (!The title should be changed to MI!). You can see the values range from 0.6~1.4. Also, it shows evidence that RTF version has outliers depicted in the box plot, but the outlier is moderated with the RTM method.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mi_box_distplot.png&#34; alt=&#34;MI distribution plot for rtf vs rtm&#34; title=&#34;MI distribution plot for rtf vs rtm&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;references-useful-links&#34;&gt;References &amp;amp; Useful Links&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://people.csail.mit.edu/fisher/publications/papers/tsai99.pdf&#34; target=&#34;_blank&#34;&gt;https://people.csail.mit.edu/fisher/publications/papers/tsai99.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2130&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2130&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://matthew-brett.github.io/teaching/mutual_information.html#t1-t2-scatter&#34; target=&#34;_blank&#34;&gt;https://matthew-brett.github.io/teaching/mutual_information.html#t1-t2-scatter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>fMRI Neuroimaging &amp; Classifier Coming Soon...</title>
      <link>https://jinjeon.me/project-archives/fmri/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project-archives/fmri/</guid>
      <description>&lt;p&gt;In prep for fMRI neuroimaging processing and building classifier&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Universal Sentence Encoder and GloVe on Narrative Semantic Representation</title>
      <link>https://jinjeon.me/post/vectorspace/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/vectorspace/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;See full repo at &lt;a href=&#34;https://github.com/jeon11/use-glove-narrative.git&#34; target=&#34;_blank&#34;&gt;https://github.com/jeon11/use-glove-narrative.git&lt;/a&gt;&lt;/strong&gt;
&lt;br&gt;
&lt;strong&gt;Note:&lt;/strong&gt; The results are shown in the &lt;a href=&#34;https://jinjeon.me/#posters&#34;&gt;poster&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Google&amp;rsquo;s Universal Sentence Encoder (USE)&lt;/strong&gt; provides 512-dimension vectors for each input that are pre-trained on large corpus, and can be plugged into a variety of different task models, such as sentiment analysis, classification, and etc. It is speed-efficient without losing task accuracy, and also provides embeddings not just for word level, but also for phrases, sentences, and even paragraphs. However, the more the words are given as input, the more likely each word meaning gets diluted.&lt;/p&gt;

&lt;p&gt;This notebook is based on the Semantic Similarity with TF-Hub Universal Encoder tutorial, but uses a separate input from one of the projects. We will also use &lt;strong&gt;GloVe&lt;/strong&gt; vectors to compare how the vectors and cosine similarity differ between the two models.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, the notebook goes over setting up locally and use one sample data to create embeddings saved out as a separate csv file using Pandas.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then assuming you have cloned the repository, we call in custom functions to quickly extract vectors of given word, phrase, sentences in USE and GloVe.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;table-of-contents-short-cuts&#34;&gt;Table of Contents/Short-cuts:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#About-USE-Models-and-Deep-Average-Network&#34;&gt;About USE Models and Deep Average Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Installation-&amp;amp;-Setup&#34;&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Path-Setup&#34;&gt;Path Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Raw-Data-Format&#34;&gt;Raw Data Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Get-USE-Embeddings&#34;&gt;Get USE Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Cosine-Similarity&#34;&gt;Cosine Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Cosine-Similarity-Example&#34;&gt;Cosine Similarity Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Plotting-Similarity-Matrix&#34;&gt;Plotting Similarity Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;about-use-models-and-deep-average-network&#34;&gt;About USE Models and Deep Average Network&lt;/h3&gt;

&lt;p&gt;There are two types of models in &lt;strong&gt;USE&lt;/strong&gt;: &lt;strong&gt;Transformer&lt;/strong&gt; and &lt;strong&gt;Deep Averaging Network (DAN)&lt;/strong&gt;. We will use DAN which is a lighter version for efficiency and speed in exchange for reduced accuracy (still accurate enough).&lt;/p&gt;

&lt;p&gt;DAN first averages the input word embeddings to create a sentence embedding. It uses PTB tokenizer, which divides a sentence into a sequence of tokens based on set of rules on  how to process punctuation, articles, etc, in order to create 512 dimension embeddings. This averaged 512 vector is passed to one or more feedforward layers. Then it is multi-task-trained on unsupervised data drawn from various internet sources,  Wikipedia, Stanford Natural Language Inference corpus, web news, and forums.
- Training  goals:
    - Uses skip-thought-like model that predicts the surrounding sentences of a given text (see below)
    - Conversational response suggestion
    - Classification task on supervised data&lt;/p&gt;

&lt;p&gt;The intuition behind deep feedforward neural network is that each layer learns a more abstract representation of the input than the previous one. So its depth allows to capture subtle variations of the input with more depths. Also, each layer only involves a single matrix multiplication, allowing minimal computing time.&lt;/p&gt;

&lt;p&gt;See full USE paper: &lt;a href=&#34;https://arxiv.org/pdf/1803.11175.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1803.11175.pdf&lt;/a&gt;
See full DAN paper: &lt;a href=&#34;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&#34; target=&#34;_blank&#34;&gt;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;installation-setup&#34;&gt;Installation &amp;amp; Setup&lt;/h3&gt;

&lt;p&gt;I used Anaconda to create a TensorFlow-specific environment to customize the package versions. After installing Anaconda&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Creating a new environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create -n py3 python=3.6.8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Activate the created environment by &lt;code&gt;conda activate py3&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Using pip, install packages for pandas, numpy, seaborn, tensorflow, tensorflow_hub. ie. &lt;code&gt;pip install pckge-name&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, let&amp;rsquo;s make sure to set the packages to exact version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install --upgrade tensorflow=1.15.0
pip install --upgrade tensorflow-hub=0.7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the steps are done, we should be able to run the codes locally.&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from absl import logging
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from glob import glob
import re
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;
due to some depecrated methods and changes made with the tf version upgrade from tf1.X to tf2.0, here we use a specific set of Python and tf versions. You can check via &lt;code&gt;pip freeze&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tested on python == 3.6.8 | tensorflow == 1.15.0 | tensorflow_hub == 0.7.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Or you can check the version in Python via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
print(sys.version_info)  # sys.version_info(major=3, minor=6, micro=8, releaselevel=&#39;final&#39;)
print(tf.__version__)    # &#39;1.15.0&#39;
print(hub.__version__)   # &#39;0.7.0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# script variables

# for lite/DAN version:
module_url = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder/2&amp;quot;

# for heavy/Transformer version:
# module_url = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder-large/3&amp;quot;

baseDir = &#39;use-glove-narrative&#39;  # repository/base folder name
embedding_size = 512  # base 512-dimension embedding
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;path-setup&#34;&gt;Path Setup&lt;/h3&gt;

&lt;p&gt;Assuming that you git cloned the project (which is for demo purposes) to your local directory, we set the path so the code knows where to look for certain data files using the &lt;code&gt;baseDir&lt;/code&gt; specified above. We will mainly just work within the cloned folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pwd = os.getcwd()
# recursively find absolute path
while os.path.basename(pwd) != baseDir:
    os.chdir(&#39;..&#39;)
    pwd = os.getcwd()
baseDir = pwd
dataDir = baseDir + &#39;/data&#39;

# recursively find all csv files. We will work with one file here
all_csvs = [y for x in os.walk(dataDir) for y in glob(os.path.join(x[0], &#39;*.csv&#39;))]
all_csvs.sort()
all_csvs = all_csvs[0]  # we will just use one sample data
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;raw-data-format&#34;&gt;Raw Data Format&lt;/h3&gt;

&lt;p&gt;To briefly show the data, the data is comprised of numerous idea units, or phrases of words with unique meanings. Here, we are only interested in the &amp;lsquo;text&amp;rsquo; column and &amp;lsquo;index&amp;rsquo; column. We will call in the text of the entire story to create embeddings for each idea unit. Below the example print out, we will loop over each story to create embeddings. Since we will use one story this time, it shouldn&amp;rsquo;t take that long.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# an example print of data format
datafile = pd.read_csv(all_csvs)
datafile.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;paragraph&lt;/th&gt;
      &lt;th&gt;index&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;scoring&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;One fine day an old Maine man was fishing&lt;/td&gt;
      &lt;td&gt;mentions at least 3 of the following: ‚Äúold‚Äù, ‚Äú...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;on his favorite lake&lt;/td&gt;
      &lt;td&gt;mentions ‚Äúfavorite lake‚Äù or ‚Äúfavorite river‚Äù&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;and catching very little.&lt;/td&gt;
      &lt;td&gt;mentions that he/the fisherman was not having ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Finally, he gave up&lt;/td&gt;
      &lt;td&gt;mentions that he gave up/stopped fishing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;and walked back along the shore to his fishing...&lt;/td&gt;
      &lt;td&gt;mentions that he walked home/to his fishing sh...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;get-use-embeddings&#34;&gt;Get USE Embeddings&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let&#39;s read in the data file
textfile = pd.read_csv(all_csvs)
# get the title of the narrative story, cutting out the .csv extension
title = os.path.basename(all_csvs)[:-4]


# create df to save out at the end
vector_df_columns = [&#39;paragraph&#39;, &#39;index&#39;, &#39;text&#39;, &#39;size&#39;]
# create column for each dimension (out of 512)
for i in range(1, embedding_size + 1):
    vector_df_columns.append(&#39;dim&#39; + str(i))
vector_df = pd.DataFrame(columns=vector_df_columns)


# import the Universal Sentence Encoder&#39;s TF Hub module
embed = hub.Module(module_url)  # hub.load(module_url) for tf==2.0.0

# we call in the text column from data file
messages = []
for t in range(0, len(textfile)):
    messages.append(textfile.iloc[t][&#39;text&#39;])

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reduce logging output.
logging.set_verbosity(logging.ERROR)

with tf.compat.v1.Session() as session:
    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])
    message_embeddings = session.run(embed(messages))

# make sure all units are there/sanity check
assert len(message_embeddings) == len(textfile) == len(messages)

# loop over each vector value to corresponding text
for e in range(0, len(message_embeddings)):
    vector_df.at[e, &#39;paragraph&#39;] = textfile.iloc[e][&#39;paragraph&#39;]
    vector_df.at[e, &#39;index&#39;] = textfile.iloc[e][&#39;index&#39;]
    vector_df.at[e, &#39;text&#39;] = messages[e]
    vector_df.at[e, &#39;size&#39;] = len(message_embeddings[e])
    for dim in range(0, len(message_embeddings[e])):
        vector_df.at[e, &#39;dim&#39;+str(dim+1)] = message_embeddings[e][dim]

# display sample format
vector_df.head()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;paragraph&lt;/th&gt;
      &lt;th&gt;index&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
      &lt;th&gt;dim1&lt;/th&gt;
      &lt;th&gt;dim2&lt;/th&gt;
      &lt;th&gt;dim3&lt;/th&gt;
      &lt;th&gt;dim4&lt;/th&gt;
      &lt;th&gt;dim5&lt;/th&gt;
      &lt;th&gt;dim6&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;dim503&lt;/th&gt;
      &lt;th&gt;dim504&lt;/th&gt;
      &lt;th&gt;dim505&lt;/th&gt;
      &lt;th&gt;dim506&lt;/th&gt;
      &lt;th&gt;dim507&lt;/th&gt;
      &lt;th&gt;dim508&lt;/th&gt;
      &lt;th&gt;dim509&lt;/th&gt;
      &lt;th&gt;dim510&lt;/th&gt;
      &lt;th&gt;dim511&lt;/th&gt;
      &lt;th&gt;dim512&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;One fine day an old Maine man was fishing&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0169429&lt;/td&gt;
      &lt;td&gt;-0.0030699&lt;/td&gt;
      &lt;td&gt;-0.0156278&lt;/td&gt;
      &lt;td&gt;-0.00649163&lt;/td&gt;
      &lt;td&gt;0.0213989&lt;/td&gt;
      &lt;td&gt;-0.0541645&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0404136&lt;/td&gt;
      &lt;td&gt;-0.0577177&lt;/td&gt;
      &lt;td&gt;0.0108959&lt;/td&gt;
      &lt;td&gt;-0.0337963&lt;/td&gt;
      &lt;td&gt;0.0817816&lt;/td&gt;
      &lt;td&gt;-0.074783&lt;/td&gt;
      &lt;td&gt;0.0231454&lt;/td&gt;
      &lt;td&gt;0.0719041&lt;/td&gt;
      &lt;td&gt;-0.047105&lt;/td&gt;
      &lt;td&gt;0.0127639&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;on his favorite lake&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;-0.0172151&lt;/td&gt;
      &lt;td&gt;0.0418602&lt;/td&gt;
      &lt;td&gt;0.0105562&lt;/td&gt;
      &lt;td&gt;0.0290091&lt;/td&gt;
      &lt;td&gt;0.0351211&lt;/td&gt;
      &lt;td&gt;-0.0121579&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0319399&lt;/td&gt;
      &lt;td&gt;-0.0201722&lt;/td&gt;
      &lt;td&gt;-0.00480706&lt;/td&gt;
      &lt;td&gt;-0.0490393&lt;/td&gt;
      &lt;td&gt;0.0562807&lt;/td&gt;
      &lt;td&gt;-0.0840528&lt;/td&gt;
      &lt;td&gt;0.0359073&lt;/td&gt;
      &lt;td&gt;0.0519214&lt;/td&gt;
      &lt;td&gt;0.0635523&lt;/td&gt;
      &lt;td&gt;-0.0615548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;and catching very little.&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0515628&lt;/td&gt;
      &lt;td&gt;0.00556853&lt;/td&gt;
      &lt;td&gt;-0.0606079&lt;/td&gt;
      &lt;td&gt;-0.0281095&lt;/td&gt;
      &lt;td&gt;-0.0631535&lt;/td&gt;
      &lt;td&gt;-0.0586548&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.0266129&lt;/td&gt;
      &lt;td&gt;0.0111167&lt;/td&gt;
      &lt;td&gt;-0.0238963&lt;/td&gt;
      &lt;td&gt;0.00741908&lt;/td&gt;
      &lt;td&gt;-0.0685881&lt;/td&gt;
      &lt;td&gt;-0.0858848&lt;/td&gt;
      &lt;td&gt;0.066858&lt;/td&gt;
      &lt;td&gt;-0.0616563&lt;/td&gt;
      &lt;td&gt;-0.0844253&lt;/td&gt;
      &lt;td&gt;-0.026685&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Finally, he gave up&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.0818241&lt;/td&gt;
      &lt;td&gt;0.00549721&lt;/td&gt;
      &lt;td&gt;-0.0245033&lt;/td&gt;
      &lt;td&gt;0.0286504&lt;/td&gt;
      &lt;td&gt;-0.0284165&lt;/td&gt;
      &lt;td&gt;-0.0575481&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0474779&lt;/td&gt;
      &lt;td&gt;-0.00603216&lt;/td&gt;
      &lt;td&gt;-0.0116888&lt;/td&gt;
      &lt;td&gt;-0.06419&lt;/td&gt;
      &lt;td&gt;0.0268704&lt;/td&gt;
      &lt;td&gt;-0.0640136&lt;/td&gt;
      &lt;td&gt;0.103409&lt;/td&gt;
      &lt;td&gt;-0.0235997&lt;/td&gt;
      &lt;td&gt;-0.0781731&lt;/td&gt;
      &lt;td&gt;-0.0365196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;and walked back along the shore to his fishing...&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
      &lt;td&gt;0.00286058&lt;/td&gt;
      &lt;td&gt;0.0576001&lt;/td&gt;
      &lt;td&gt;0.0103945&lt;/td&gt;
      &lt;td&gt;-0.00301533&lt;/td&gt;
      &lt;td&gt;0.0199591&lt;/td&gt;
      &lt;td&gt;-0.0644398&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.0145959&lt;/td&gt;
      &lt;td&gt;0.0137776&lt;/td&gt;
      &lt;td&gt;0.0165417&lt;/td&gt;
      &lt;td&gt;-0.0406641&lt;/td&gt;
      &lt;td&gt;-0.0204453&lt;/td&gt;
      &lt;td&gt;-0.0713526&lt;/td&gt;
      &lt;td&gt;0.0121754&lt;/td&gt;
      &lt;td&gt;0.00591647&lt;/td&gt;
      &lt;td&gt;0.0262764&lt;/td&gt;
      &lt;td&gt;-0.0329477&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows √ó 516 columns&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.shape(vector_df))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(0, 516)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sample data shows each idea unit/text converted to 512 dimension vectors. &lt;code&gt;np.shape(vector_df)&lt;/code&gt; will return a 41 total idea units/phrases to 516 columns (512 dimensions + custom columns (paragraph info, index, text, and size)). We then use these vectors to explore semantic similarity between text and phrases.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run the code below to save out as csv file
vector_df.reindex(columns=vector_df_columns)
vector_df.to_csv(title + &#39;_vectors.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h2&gt;

&lt;p&gt;As a brief description, &lt;strong&gt;cosine similarity&lt;/strong&gt; is basically the measure of cosine angle between the two vectors. Since we have USE and GloVe vectors that represent words into multidimensional vectors, we can apply these vector values to calculate how similar the two words are.&lt;/p&gt;

&lt;p&gt;It can be easily calculated in Python with its useful packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cos_sim = numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1) * numpy.linalg.norm(vector2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming we established some basic understanding, let&amp;rsquo;s call in the functions I made so that we can easily get USE and GloVe vectors at multiple word level.&lt;/p&gt;

&lt;p&gt;I will highlight some of the functions below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from get_glove_use import *
help(glove_vec)
help(use_vec)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Help on function glove_vec in module get_glove_use:

glove_vec(item1, item2)
    get vectors for given two words and calculate cosine similarity

    Parameters
    ----------
    item1 : str
        string in glove word pool vector to compare
    item2 : str
        string in glove word pool vector to compare

    Returns
    -------
    item1_vector : array
        item1 GloVe vector
    item2_vector : array
        item2 GloVe vector
    cos_sim : float
        cosine similarity of item1 and item2 vectors

Help on function use_vec in module get_glove_use:

use_vec(item1, item2)
    get USE vectors and cosine similairty of the two items

    Parameters
    ----------
    item1 : str, list
        any word to compare, put in string for more than one word
    item2 : str, list
        any word to compare, put in string for more than one word

    Returns
    -------
    item1_vector : array
        item1 USE vector
    item2_vector : array
        item2 USE vector
    cos_sim : float
        cosine similarity of item1 and item2 vectors
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cosine-similarity-example&#34;&gt;Cosine Similarity Example&lt;/h3&gt;

&lt;p&gt;Using the two functions above, and another function compare_word_vec (which basically uses the two functions), we can easily obtain cosine similarity of two words.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# using the two functions above, we can get
# GloVe and USE vectors and cosine similarity of two input words
os.chdir(gloveDir)
_, _, glove_sim = glove_vec(&#39;fish&#39;,&#39;bear&#39;)
_, _, use_sim = use_vec(&#39;fish&#39;,&#39;bear&#39;)
print(&#39;use cos: &#39; + str(use_sim))
print(&#39;glove cos: &#39; + str(glove_sim))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


0.11964830574261577
0.5305143
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the two functions glove_vex and use_vec are use in compare_word_vec
compare_word_vec(&#39;man&#39;,&#39;fish&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


use cos: 0.49838725
glove cos: 0.18601566881803455
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; From the example above, USE and GloVe similarly identy &lt;em&gt;fish&lt;/em&gt; to be somewhat equally similar to &lt;em&gt;bear&lt;/em&gt; and &lt;em&gt;man&lt;/em&gt; (but just in different scale/degree).&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s try comparing at multiple words or phrase level. We will use new functions and give in new inputs as strings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentence1 = [&#39;old&#39;, &#39;man&#39;, &#39;caught&#39;, &#39;fish&#39;]
sentence2 = [&#39;bear&#39;, &#39;hunted&#39;, &#39;trout&#39;]
sentence3 = [&#39;bear&#39;,&#39;eat&#39;,&#39;six&#39;,&#39;fish&#39;]

print(&#39;old man caught fish &amp;amp; bear hunted trout:&#39;)
phrase_vec(sentence1, sentence2)

print(&#39;old man caught fish &amp;amp; bear eat six fish:&#39;)
phrase_vec(sentence1, sentence3)

print(&#39;bear hunted trout &amp;amp; bear eat six fish:&#39;)
phrase_vec(sentence2, sentence3)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;old man caught fish &amp;amp; bear hunted trout:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.36609688461789297
USE sim: 0.50494814
old man caught fish &amp;amp; bear eat six fish:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.6818474640845398
USE sim: 0.5896743
bear hunted trout &amp;amp; bear eat six fish:
INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore


glove sim: 0.6082457470353315
USE sim: 0.72352856
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; From the example above, we can see that USE and GloVe capture somewhat differently. We can see that &lt;em&gt;bear hunted trout&lt;/em&gt; and &lt;em&gt;bear eat six fish&lt;/em&gt; are the most similar to each other, whereas &lt;em&gt;old man caught fish&lt;/em&gt; is also similar to the context of bear eating six fish.&lt;/p&gt;

&lt;p&gt;More detailed analysis is required, but the example above shows great possibilities to exploring semantics.&lt;/p&gt;

&lt;h3 id=&#34;plotting-similarity-matrix&#34;&gt;Plotting Similarity Matrix&lt;/h3&gt;

&lt;p&gt;Now that we can compare similarity of words and sentences, we can plot a simple pairwise matrix, which basically compares how similar each word is to another in the given list. Fortunately, we already have a plot for doing it (using Seaborn).&lt;/p&gt;

&lt;p&gt;I will only use few words as demonstration, since it&amp;rsquo;s been slowing up my computer so much!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_sim_matrix([&#39;man&#39;, &#39;bear&#39;, &#39;fish&#39;, &#39;trout&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO:tensorflow:Saver not created because there are no variables in the graph to restore


INFO:tensorflow:Saver not created because there are no variables in the graph to restore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./use_glove_cosine_similarity_25_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;ending-note&#34;&gt;Ending Note&lt;/h2&gt;

&lt;p&gt;In the example above, we only used simple noun words. The stronger blue color, the more similar the two words are. Thus, the diagonal strip is deep blue (similarity of same two words is 1). You can see &lt;em&gt;fish&lt;/em&gt; and &lt;em&gt;trout&lt;/em&gt; are more similar to each other, than is &lt;em&gt;man&lt;/em&gt; to &lt;em&gt;trout&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Keep in mind that you can feed in more words and sentences to create and visualize a larger matrix.&lt;/p&gt;

&lt;p&gt;We looked at setting up USE locally, and creating embeddings from USE. The cloned project also has sample version of GloVe vectors. We use the vectors from the two models to extract vectors and compare similarity of two texts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EEG/Signal Processing--Advanced Part 2</title>
      <link>https://jinjeon.me/post/eeg-advanced/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/eeg-advanced/</guid>
      <description>

&lt;h3 id=&#34;note&#34;&gt;Note:&lt;/h3&gt;

&lt;p&gt;This post is a ported version of Jupyter Notebook from my mne-eeg project: &lt;a href=&#34;https://github.com/jeon11/mne-egi/blob/master/walkthrough_advanced.ipynb/walkthrough_basics.ipynb&#34; target=&#34;_blank&#34;&gt;https://github.com/jeon11/mne-egi/blob/master/walkthrough_advanced.ipynb/walkthrough_basics.ipynb&lt;/a&gt;
&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;advanced-processing&#34;&gt;Advanced Processing&lt;/h2&gt;

&lt;p&gt;In the previous walkthrough notebook, we got to manually inspect raw instance and do some cleaning based on annotations and creating evoked responses from time-locked events.&lt;/p&gt;

&lt;p&gt;In this section, we run independent component analysis (ICA) on the epochs we had from the last notebook. We look in ICs to identify potentially bad components with eye related artifcats. Then, we implement autoreject (&lt;a href=&#34;http://autoreject.github.io&#34; target=&#34;_blank&#34;&gt;http://autoreject.github.io&lt;/a&gt;) which automatically attempts to find bad channels and interpolate those based on nearby channels. At the end, we plot the ERPs by channels that we are interested in looking and make comparison.&lt;/p&gt;

&lt;p&gt;Note that the plots below will be using &lt;code&gt;print&lt;/code&gt; statements for demonstration purposes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mne
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import Tkinter
from autoreject import AutoReject
from autoreject import get_rejection_threshold
from mne.preprocessing import ICA
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;loading-epochs&#34;&gt;Loading epochs&lt;/h3&gt;

&lt;p&gt;We imported all the necessary dependencies. Now we load the saved epochs from last notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs_tlstS = mne.read_epochs(&#39;/data/epochs_tlsts-epo.fif&#39;, preload=True)
print(epochs_tlstS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;EpochsFIF  |   388 events (all good), -0.25 - 0.8 sec, baseline [-0.25, 0], ~72.8 MB, data loaded, with metadata,
 u&#39;lstS&#39;: 388&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;running-independent-component-analysis-ica&#34;&gt;Running Independent Component Analysis (ICA)&lt;/h2&gt;

&lt;p&gt;ICA is a signal processing method to decompose signals into independent sources from a mixed signal. A representative example is the cocktail party effect, which is a phenomenon in which you are able to concentrate on the voice of the speaker  you are conversing with regardless of the various background noise in a party. Using ICA helps seperate the different sources of mixed sound, under the assumption that the sound components are linear. This method works for EEG signal preprocessing because we assume that each electrode is independent from the others. To think of it easily, I consider ICA as decomposing the data into multiple layers, and by excluding bad ICs, we filter the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the function calculates optimal reject threshold for ICA
reject = get_rejection_threshold(epochs_tlstS)
print(reject)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimating rejection dictionary for eeg
Estimating rejection dictionary for eog
{&#39;eeg&#39;: 0.0007759871430524497, &#39;eog&#39;: 5.903189072009943e-05}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;low-frequency-slow-drifts&#34;&gt;Low-frequency slow drifts&lt;/h3&gt;

&lt;p&gt;Because ICA is sensitive to low-frequency slow drifts, it is recommended that 1Hz highpass filter is applied. Since this was already done to our raw instance in the previous notebook, it can be skipped. You can double check as below, or apply the highpass filter if you haven&amp;rsquo;t already.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# double check highpass filter
print(epochs_tlstS.info[&#39;highpass&#39;])

# epochs_tlstS.info[&#39;highpass&#39;] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fit-ica&#34;&gt;Fit ICA&lt;/h3&gt;

&lt;p&gt;Now we will run ICA on our epoch data. For simplicity and time sake, we will limit the number of components to 20 with fastICA method, which is the generally used one. The number of ICs can be created up to as many electrodes (in this case 128 - bad channels). In &lt;code&gt;ica1.fit&lt;/code&gt;, we use the recommended reject threshold from Autoreject.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ica = ICA(n_components=20, max_pca_components=None, n_pca_components=None, noise_cov=None,
           random_state=None, method=&#39;fastica&#39;, fit_params=None, max_iter=200, verbose=None)
print(&#39;fitting ica...&#39;)
ica.fit(epochs_tlstS, reject=reject)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;fitting ica...


/Users/Jin/Library/Python/2.7/lib/python/site-packages/scipy/linalg/basic.py:1321: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to &#39;gelss&#39; driver.
  x, resids, rank, s = lstsq(a, b, cond=cond, check_finite=False)





&amp;lt;ICA  |  epochs decomposition, fit (fastica): 81868 samples, 20 components, channels used: &amp;quot;eeg&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;artifact-detection-using-ica-correlation&#34;&gt;Artifact detection using ICA correlation&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;plot_sources&lt;/code&gt; can show the signals of each ICs. We can manually inspect for ICs with noise, or identify bad ICs that correlates with oscillations from eye-related channels. We use the builtin &lt;code&gt;find_bads_eog&lt;/code&gt; from ICA class.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;plot_scores&lt;/code&gt; will show the correlation values for each component, and mark the ones that are potentially bad with red. Note that because we only specified 20 components, the decomposition is rather compressed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eog_inds, scores = ica.find_bads_eog(epochs_tlstS)
print(&#39;suggested eog component: &#39; + str(eog_inds))
print(ica.plot_scores(scores, exclude=eog_inds, labels=&#39;eog&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;suggested eog component: [3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_11_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(460.8x194.4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;find_bads_eog&lt;/code&gt; suggested that component &amp;lsquo;3&amp;rsquo; is bad IC related to eye-related artifact. We can plot that specific component to inspect manually.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(ica.plot_properties(epochs_tlstS, picks=eog_inds, psd_args={&#39;fmax&#39;: 35.}, image_args={&#39;sigma&#39;: 1.}))
ica.exclude += eog_inds
print(ica.exclude)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/Users/Jin/Library/Python/2.7/lib/python/site-packages/mne/transforms.py:689: RuntimeWarning: invalid value encountered in divide
  out[:, 2] = np.arccos(cart[:, 2] / out[:, 0])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_13_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;lt;Figure size 504x432 with 5 Axes&amp;gt;]
[3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Usually, eye blinks are characterized as having significantly polar activities between the frontal and the posterior regions with high activity in the frontal region (ie. eyes). Also, activities shown in the frontal region, especially near the eye area, would not be helpful in our analysis. Eye movements are characterized as having significantly split activities between left and right. Component above does seem containing eye blinks, we mark that component bad by &lt;code&gt;ica.exclude&lt;/code&gt; and we can see that component has been added.&lt;/p&gt;

&lt;p&gt;We can also manually inspect for other components using &lt;code&gt;plot_components&lt;/code&gt; besides the ones that the builtin method suggested. You can see that the component speficied above being grayed out as a bad IC. The plot prioritizes showing ICs with large activations and polarity, which means that most of the bad ICs could be found in the early ICs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(ica.plot_components(inst=epochs_tlstS))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_15_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;lt;Figure size 540x504 with 20 Axes&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When running the code on &lt;code&gt;ipython&lt;/code&gt; as suggested in the previous notebook, the plot is actually interactive. By clicking on the component, it shows the component properties. Clicking on the name of the component will gray out the name and be marked as bad IC. Here, it seems components 2, 14, and 18 have high activation in the eye regions, which could be identified as components with eye blinks. Also, componnent 5 has activation in the frontal region, and has polar activities between left and right, which could potentially be eye movements. Because the plot above is not interactive, we will specify which ICs to exclude as a line of code.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;ve identified the bad ICs, we can apply it to our epochs_tlstS, and proceed to autoreject.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ica.exclude += [0, 2, 5, 14, 18]
print(ica.exclude)
ica.apply(epochs_tlstS)
print(&#39;number of ICs dropped: &#39; + str(len(ica.exclude)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[3, 0, 2, 5, 14, 18]
number of ICs dropped: 6
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autoreject&#34;&gt;Autoreject&lt;/h2&gt;

&lt;p&gt;Now that we have bad ICs identified, we try implementing autoreject for cleaning. Note that the step below may take some time as it tries find bad channels and fix them.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ar = AutoReject()
epochs_clean = ar.fit_transform(epochs_tlstS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Running autoreject on ch_type=eeg
[........................................] 100.00% Creating augmented epochs \ Computing thresholds ...
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.5s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    7.3s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.5s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   13.2s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   16.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   19.5s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   21.7s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   24.6s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   27.5s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   30.3s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   33.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   35.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   36.8s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   38.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   40.2s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   42.2s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   44.8s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   47.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   48.7s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   50.1s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   51.6s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   53.1s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   54.8s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   56.4s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   57.9s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   59.8s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  1.0min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.1min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  1.1min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  1.1min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  1.1min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  1.2min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  1.2min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  1.2min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:  1.2min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:  1.3min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  1.3min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  1.3min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:  1.4min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  1.4min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:  1.4min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:  1.4min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  1.5min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:  1.5min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:  1.5min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  1.6min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:  1.6min remaining:    0.0s
[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  1.6min remaining:    0.0s
[Parallel(n_jobs=1)]: Done 115 out of 115 | elapsed:  3.8min finished
[........................................] 100.00% n_interp \   chs |   



Estimated consensus=0.30 and n_interpolate=4
[........................................] 100.00% Repairing epochs |   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above created a new epochs called &lt;code&gt;epochs_clean&lt;/code&gt;. We can compare how the epochs are cleaned by comparing the two plots. For demonstration, we only plot the &lt;code&gt;epochs_clean&lt;/code&gt;. The plot shows individual epochs with green line being 0 (the onset of the word in the experiment). In the interactive plot mode, you can scroll vertically to see different channels and horizontally to search through epochs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(epochs_clean.plot())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_21_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(869.6x536.8)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-evoked-response-from-epochs-clean&#34;&gt;Creating evoked response from epochs_clean&lt;/h2&gt;

&lt;p&gt;Now that we have a new, ideally cleaner epochs, we create evoked response for each condition. Currently, &lt;code&gt;epochs_clean&lt;/code&gt; contains all four conditions with approximately 100 epochs for each (less than 400 now because epochs been rejected). Note that the y-axis microvolt scale has been refined compared to our previous notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# now let&#39;s create a new evoked responses (ie. the autoreject evoked)
arevoked_tlst_c1 = epochs_clean[&amp;quot;label==&#39;lstS&#39; and cond==&#39;1&#39;&amp;quot;].average()
arevoked_tlst_c2 = epochs_clean[&amp;quot;label==&#39;lstS&#39; and cond==&#39;2&#39;&amp;quot;].average()
arevoked_tlst_c3 = epochs_clean[&amp;quot;label==&#39;lstS&#39; and cond==&#39;3&#39;&amp;quot;].average()
arevoked_tlst_c4 = epochs_clean[&amp;quot;label==&#39;lstS&#39; and cond==&#39;4&#39;&amp;quot;].average()

# let&#39;s see a sample evoked response
print(arevoked_tlst_c1.plot_joint(times=&#39;peaks&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_23_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(576x302.4)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;plotting-erp-comparison&#34;&gt;Plotting ERP comparison&lt;/h2&gt;

&lt;p&gt;Now that we have evoked response for each condition, we can look into specific channels of interest to see how the signals differ by conditions. For the selection list, we will only specify channel E92 as it will create 4 graphs for each channel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we specify which channels to look at
selection = [&#39;E92&#39;]  # [&#39;EB&#39;,&#39;E11&#39;,&#39;E24&#39;,&#39;E124&#39;,&#39;E36&#39;,&#39;E104&#39;,&#39;E52&#39;,&#39;E62&#39;,&#39;E92&#39;]
picks_select = mne.pick_types(epochs_clean.info, meg=False, eeg=True, eog=True, stim=False,
                              exclude=&#39;bads&#39;, selection=selection)

# create dictionary for each condition
evoked_dict = {&#39;highcosval&#39;: arevoked_tlst_c1,
                &#39;lowcosval&#39;: arevoked_tlst_c2,
                &#39;highcosinval&#39;: arevoked_tlst_c3,
                &#39;lowcosinval&#39;: arevoked_tlst_c4}

picks_select = mne.pick_types(arevoked_tlst_c1.info, meg=False, eeg=True, eog=True, stim=False,
                              exclude=&#39;bads&#39;, selection=selection)


# this will plot each selected channel with comparison of two conditions
title = &#39;%s_vs_%s_E%s.png&#39;
for i in range(0, len(picks_select)):
    fig1 = mne.viz.plot_compare_evokeds({&#39;highcos/val&#39;:evoked_dict[&#39;highcosval&#39;],
                                         &#39;lowcos/val&#39;:evoked_dict[&#39;lowcosval&#39;]}, picks=picks_select[i])
    fig2 = mne.viz.plot_compare_evokeds({&#39;highcos/inval&#39;:evoked_dict[&#39;highcosinval&#39;],
                                         &#39;lowcos/inval&#39;:evoked_dict[&#39;lowcosinval&#39;]}, picks=picks_select[i])
    fig3 = mne.viz.plot_compare_evokeds({&#39;highcos/val&#39;:evoked_dict[&#39;highcosval&#39;],
                                         &#39;highcos/inval&#39;:evoked_dict[&#39;highcosinval&#39;]},picks=picks_select[i])
    fig4 = mne.viz.plot_compare_evokeds({&#39;lowcos/val&#39;:evoked_dict[&#39;lowcosval&#39;],
                                         &#39;lowcos/inval&#39;:evoked_dict[&#39;lowcosinval&#39;]}, picks=picks_select[i])

    # save figs
    # fig1.savefig(title % (evoked_dict.keys()[0], evoked_dict.keys()[1], i))
    # fig2.savefig(title % (evoked_dict.keys()[2], evoked_dict.keys()[3], i))
    # fig3.savefig(title % (evoked_dict.keys()[0], evoked_dict.keys()[2], i))
    # fig4.savefig(title % (evoked_dict.keys()[1], evoked_dict.keys()[3], i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_25_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_25_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_25_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_25_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this will plot just the evoked responses per conditions with all channels
fig5 = arevoked_tlst_c1.plot(titles=&#39;cond1: high cos/val&#39;)
fig6 = arevoked_tlst_c2.plot(titles=&#39;cond2: low cos/val&#39;)
fig7 = arevoked_tlst_c3.plot(titles=&#39;cond3: high cos/inval&#39;)
fig8 = arevoked_tlst_c4.plot(titles=&#39;cond4: low cos/inval&#39;)

# save figs
# fig5.savefig(&#39;c1all.png&#39;)
# fig6.savefig(&#39;c2all.png&#39;)
# fig7.savefig(&#39;c3all.png&#39;)
# fig8.savefig(&#39;c4all.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_26_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_26_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_26_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_advanced_26_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sources-and-useful-links&#34;&gt;Sources and useful links&lt;/h2&gt;

&lt;p&gt;EEGLab ICA guide: &lt;a href=&#34;https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA&#34; target=&#34;_blank&#34;&gt;https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MNE ICA class: &lt;a href=&#34;https://martinos.org/mne/stable/generated/mne.preprocessing.ICA.html&#34; target=&#34;_blank&#34;&gt;https://martinos.org/mne/stable/generated/mne.preprocessing.ICA.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;autoreject: &lt;a href=&#34;http://autoreject.github.io/auto_examples/plot_auto_repair.html#sphx-glr-auto-examples-plot-auto-repair-py&#34; target=&#34;_blank&#34;&gt;http://autoreject.github.io/auto_examples/plot_auto_repair.html#sphx-glr-auto-examples-plot-auto-repair-py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Clemens Brunner&amp;rsquo;s great guide on ICA: &lt;a href=&#34;https://cbrnr.github.io/2018/01/29/removing-eog-ica/&#34; target=&#34;_blank&#34;&gt;https://cbrnr.github.io/2018/01/29/removing-eog-ica/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Clemens Brunner&amp;rsquo;s great guide on EOG detection using linear regression: &lt;a href=&#34;https://cbrnr.github.io/2017/10/20/removing-eog-regression/&#34; target=&#34;_blank&#34;&gt;https://cbrnr.github.io/2017/10/20/removing-eog-regression/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MNE stats: &lt;a href=&#34;https://martinos.org/mne/stable/auto_tutorials/plot_stats_cluster_erp.html#sphx-glr-auto-tutorials-plot-stats-cluster-erp-py&#34; target=&#34;_blank&#34;&gt;https://martinos.org/mne/stable/auto_tutorials/plot_stats_cluster_erp.html#sphx-glr-auto-tutorials-plot-stats-cluster-erp-py&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EEG/Signal Processing--Basics Part 1</title>
      <link>https://jinjeon.me/post/eeg-basics/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/post/eeg-basics/</guid>
      <description>

&lt;h3 id=&#34;note&#34;&gt;Note:&lt;/h3&gt;

&lt;p&gt;This post is a ported version of Jupyter Notebook from my mne-eeg project: &lt;a href=&#34;https://github.com/jeon11/mne-egi/blob/master/walkthrough_basics.ipynb&#34; target=&#34;_blank&#34;&gt;https://github.com/jeon11/mne-egi/blob/master/walkthrough_basics.ipynb&lt;/a&gt;
&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This script runs through sample experiment data from manually reading in raw file to preprocessing through applying filters, eye blink detection using peak finding techniques. Then we create epochs and plot evoked responses.&lt;/p&gt;

&lt;p&gt;In the advanced walkthrough: &lt;a href=&#34;https://github.com/jeon11/mne-egi/blob/master/walkthrough_advanced.ipynb&#34; target=&#34;_blank&#34;&gt;walkthrough_advanced.ipynb&lt;/a&gt;, we implement independent component analysis (ICA) and autoreject, which is an automated tool for fixing data, to see how the epochs are improved and compare the evoked responses by conditions.&lt;/p&gt;

&lt;p&gt;The script requires at least two files:
  - the raw data &lt;a href=&#34;https://drive.google.com/file/d/1W2UFu_6H4HzFF2DALAxfmr0BNSj7pEok/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;(download from Google Drive ~500MB)&lt;/a&gt;
  - exported event text log from NetStation software&lt;/p&gt;

&lt;h3 id=&#34;running-the-script-in-command-line&#34;&gt;Running the script in command line&lt;/h3&gt;

&lt;p&gt;When running the Python script from command line, MNE recommends using ipython via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ipython ‚Äî-pylab osx -i mne-egi-walkthrough.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For Windows, instead of &lt;code&gt;osx&lt;/code&gt;, you would be specifying &lt;code&gt;qt&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;importing&#34;&gt;Importing&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s begin by importing all the necessary modules. Make sure you have all the required dependencies setup.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mne
import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
from mne.preprocessing import eog
from mne.preprocessing import create_eog_epochs
from mne.preprocessing.peak_finder import peak_finder
import Tkinter
import extract_nslog_event
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setting-basic-variables&#34;&gt;Setting basic variables&lt;/h3&gt;

&lt;p&gt;Before we begin any preprocessing, we create variables here to specify what we want to look for. The whole script basically requires two main files.
1. raw_fname: The raw instance of eeg data file in .raw format
2. ns_eventlog: Netstation&amp;rsquo;s event exports in text&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;selection&lt;/code&gt; variable is later used to specify which channels to plot and compare. Note, the first item in the &lt;code&gt;selection&lt;/code&gt; list, &lt;code&gt;EB&lt;/code&gt; channel is a virtual channel created from bipolar referene.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# specify sample subject data directory
raw_fname   = &#39;/Users/Jin/Documents/MATLAB/research/mne-egi/data/sfv_eeg_011ts.raw&#39;
ns_eventlog = &#39;/Users/Jin/Documents/MATLAB/research/mne-egi/data/sfv_eeg_011ts_nsevent&#39;

# specify sub-sample of channels to look in detail
selection = [&#39;EB&#39;,&#39;E11&#39;,&#39;E24&#39;,&#39;E124&#39;,&#39;E36&#39;,&#39;E104&#39;,&#39;E52&#39;,&#39;E62&#39;,&#39;E92&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reading-in-raw-file&#34;&gt;Reading in raw file&lt;/h3&gt;

&lt;p&gt;Raw eeg data can be read in with a simple line below. You can specify montage kind in strings. See &lt;a href=&#34;https://martinos.org/mne/dev/generated/mne.channels.read_montage.html&#34; target=&#34;_blank&#34;&gt;https://martinos.org/mne/dev/generated/mne.channels.read_montage.html&lt;/a&gt; for available montages. We set &lt;code&gt;preload=True&lt;/code&gt; because some of the preprocessing functions require raw file to be preloaded.&lt;/p&gt;

&lt;p&gt;Once the raw file is loaded, typing &lt;code&gt;raw&lt;/code&gt; and &lt;code&gt;raw.info&lt;/code&gt; will show details about the raw instance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;reading raw file...&#39;)
raw = mne.io.read_raw_egi(raw_fname, montage=&#39;GSN-HydroCel-128&#39;, preload=True)
print(&#39;Done!&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;reading raw file...


&amp;lt;ipython-input-3-8bc42ae4bead&amp;gt;:2: RuntimeWarning: The following EEG sensors did not have a position specified in the selected montage: [&#39;E129&#39;]. Their position has been left untouched.
  raw = mne.io.read_raw_egi(raw_fname, montage=&#39;GSN-HydroCel-128&#39;, preload=True)


Done!
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(raw)
# see the first ten list of channel names (note by default, prefix &#39;E&#39; is appended)
print(raw.info[&#39;ch_names&#39;][0:10])

# see highpass &amp;amp; lowpass filter
print(&#39;highpass filter: &#39; + str(raw.info[&#39;highpass&#39;]))
print(&#39;lowpass filter: &#39; + str(raw.info[&#39;lowpass&#39;]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;RawEGI  |  sfv_eeg_011ts.raw, n_channels x n_times : 136 x 989490 (4947.4 sec), ~1.00 GB, data loaded&amp;gt;
[&#39;E1&#39;, &#39;E2&#39;, &#39;E3&#39;, &#39;E4&#39;, &#39;E5&#39;, &#39;E6&#39;, &#39;E7&#39;, &#39;E8&#39;, &#39;E9&#39;, &#39;E10&#39;]
highpass filter: 0.0
lowpass filter: 100.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;applying-bandpass-filter&#34;&gt;Applying bandpass filter&lt;/h2&gt;

&lt;p&gt;Our first preprocessing step will be applying the bandpass filter of 1Hz and 30Hz. The numbers can be played around with, but this filter range will potentially remove general noise from environment and slow drifting signals. Other suggestions for highpass is 0.1; for 40 Hz lowpass.&lt;/p&gt;

&lt;p&gt;After bandpass filter is applied, type &lt;code&gt;raw.info&lt;/code&gt; to check how &lt;code&gt;raw.filter&lt;/code&gt; made changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# apply bandpass filter to raw file (highpass, lowpass)
raw.filter(1,30)

# see highpass &amp;amp; lowpass filter
print(&#39;highpass filter: &#39; + str(raw.info[&#39;highpass&#39;]))
print(&#39;lowpass filter: &#39; + str(raw.info[&#39;lowpass&#39;]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;highpass filter: 1.0
lowpass filter: 30.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-meta-dataframe&#34;&gt;Creating meta dataframe&lt;/h2&gt;

&lt;p&gt;We will deviate a little from processing raw file, and construct a dataframe that can be later used for effectively creating epochs or querying information we just need. This part uses the custom built module (also experiment specific as each experiment will have different paradigms and event tags). The &lt;code&gt;extract_nslog_event&lt;/code&gt; constructs necessary pandas dataframe from &lt;code&gt;ns_eventlog&lt;/code&gt; text file which we specified earlier in #Setting-basic-variables.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;create_df&lt;/code&gt; returns five dataframes, in which nsdata is a list from simply csv-read file that is used to create task-specific pandas dataframes. For example, &lt;code&gt;df_lst&lt;/code&gt; is the initial dataframe created that includes all practice, trials, and sentences tasks. The rest of dfs contain task specific data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create pandas data frames for different tasks
nsdata, df_lst, df_plst, df_tlst, df_slst = extract_nslog_event.create_df(ns_eventlog)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;creating data frame from ns event log...
dataframes created for subject 011
trials found: 800
sentences found: 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From now on, for simplicity sake, we will only examine the actual trials task part in this walkthrough. We can focus on looking at the data structure of trials task. Since the dataframe is already created specifically for trials, what we really want now is the onset (sample numbers) of when the event occured and the condition of the stimuli that was presented.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# show data frame structure of 2rd index
print(df_tlst.iloc[2])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;code      tlst
label     lstS
onset    99867
cond         4
indx         1
Name: 2, dtype: object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The label for trials task was either a last word start (lstS) or last word end (lstE). Since we are interested in the onset of the word, we will extract just the onsets using the custom code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create onset-only data frame (event tag specifications)
df_tlstS = extract_nslog_event.create_df_onset(df_tlst)
# show total events of interest
len(df_tlstS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;400
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;finding-impedance-check-periods-and-annotating&#34;&gt;Finding impedance check periods and annotating&lt;/h2&gt;

&lt;p&gt;Now that we have dataframes setup, we continue to clean up the raw data. Throughout the acquisition, we ran impedance checks to make sure that all electrodes were in good contact with the scalp and that good signal is being read in. During the impedance check, the waveforms peak in extreme amount and we want to make note of these periods, telling the mne functions to avoid and ignore such periods.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# find impedance onsets
imp_onset, imp_offset, imp_dur = extract_nslog_event.find_impedances(nsdata)

# annotate on raw with &#39;bad&#39; tags (params `reject_by_annotation` will search for &#39;bad&#39; tags later)
annot_imp = mne.Annotations(imp_onset, imp_dur, [&amp;quot;bad imp&amp;quot;] * len(imp_onset), orig_time=raw.info[&#39;meas_date&#39;])
raw.set_annotations(annot_imp)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;finding impedance periods...
found 4 impedance periods!





&amp;lt;RawEGI  |  sfv_eeg_011ts.raw, n_channels x n_times : 136 x 989490 (4947.4 sec), ~1.00 GB, data loaded&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;marking-bad-channels&#34;&gt;Marking bad channels&lt;/h2&gt;

&lt;p&gt;We also want to make note of bad channels. We can manually inspect for bad channels by seeing the actual raw data.  &lt;code&gt;raw.plot&lt;/code&gt; will show the actual raw file with annotations from above marked as red segments. You can inspect for good/bad channels and manually click on bad channels to mark them bad. Once you manually inspected the channels, type &lt;code&gt;raw.info[&#39;bads&#39;]&lt;/code&gt; to see how it is updated.&lt;/p&gt;

&lt;p&gt;Note that the plot below is a static figure for example sake. Running the code in ipython will allow us to horizontally and vertically scroll through data. Clicking on a channel will mark that channel red and be considered red. You can see that we&amp;rsquo;ve ran four impedance checks throughout the session (1 task switch period, every 100th trials out fo 400).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# block=True is useful because it will wait to whatever change you make in the raw file at the plot stage
print(raw.plot(bad_color=&#39;red&#39;, block=True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_basics_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(782.64x483.12)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you already had a list of bad channels noted during the acquisition period, you can skip the above manual inspection and simply specify the bad channels with a line of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;raw.info[&#39;bads&#39;] = [&#39;E127&#39;, &#39;E107&#39;, &#39;E49&#39;, &#39;E48&#39;, &#39;E115&#39;, &#39;E113&#39;, &#39;E122&#39;, &#39;E121&#39;, &#39;E123&#39;, &#39;E108&#39;, &#39;E63&#39;, &#39;E1&#39;]
print(raw.info[&#39;bads&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;E127&#39;, &#39;E107&#39;, &#39;E49&#39;, &#39;E48&#39;, &#39;E115&#39;, &#39;E113&#39;, &#39;E122&#39;, &#39;E121&#39;, &#39;E123&#39;, &#39;E108&#39;, &#39;E63&#39;, &#39;E1&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;detecting-and-rejecting-eye-blinks&#34;&gt;Detecting and rejecting eye blinks&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve so far applied generic/broad preprocessing steps, such as bandpass filters, marking chunks of bad segments, and marking bad channels. Now we will look at finding eye blinks in the raw and add more annotations to mark those samples bad.&lt;/p&gt;

&lt;h3 id=&#34;step-1-setting-bipolar-reference&#34;&gt;Step 1: Setting bipolar reference&lt;/h3&gt;

&lt;p&gt;Because the cap we use do not have EOG-specific channels, we use the channels that are nearest to the eyes and consider those as our virtual eye channels. Thus, such method has the risk of the eye channels actually not having just the eye-related oscillations. This is done by setting the bipolar reference, which is basically the subtraction of two opposing channels (ie. the top and bottom of each eye for eye blinks; the left and right of the eyes for eye movements).&lt;/p&gt;

&lt;p&gt;Here, we use just the right side of the eye only to detect eye blinks. From the subtraction of channel E8 and E126, a virtual channel EB is created.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let&#39;s begin eye artifact detections
print(&#39;Starting EOG artifact detection&#39;)
raw = mne.set_bipolar_reference(raw, [&#39;E8&#39;],[&#39;E126&#39;],[&#39;EB&#39;])

# specify this as the eye channel
raw.set_channel_types({&#39;EB&#39;: &#39;eog&#39;})

# double check the changes
# print(raw.info[&#39;chs&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Starting EOG artifact detection
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-detecting-eye-blinks&#34;&gt;Step 2: Detecting eye blinks&lt;/h3&gt;

&lt;p&gt;Now that we have a virtual eye channel to inspect, we can try to identify any eye blinks. Because the virtual eye channel that is created from subtraction of the channels, the waveform of EB channel will be generally flat. You can inspect this by &lt;code&gt;raw.plot&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Eye blinks are generally characterized as two eye channels having sudden opposing peaks. So the methodology is to find a sudden peak within the flat EB line. We have the options of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;finding eye blinks via mne built in function&lt;/li&gt;
&lt;li&gt;finding eye blinks via scipy peak finding method&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Both results in similar eye blink detections because the methodology of finding local peaks. We will only use the mne built in function and comment out the custom built function that uses scipy. &lt;code&gt;reject_by_annotation&lt;/code&gt; will ignore the bad segments marked as bad earlier. The threshold of 0.0001 can be played around with but it is a reasonable threshold set after having manually inspect the data. The &lt;code&gt;events_eog&lt;/code&gt; will be an array with [sample number, 0, eventlabel in number]&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;events_eog = eog.find_eog_events(raw, reject_by_annotation=True, thresh=0.0001, verbose=None)

# type `help(scipy_annotate_eyeblinks)` for detail
# raw = scipy_annotate_eyeblinks(raw, &#39;EB&#39;, 100)

print(&#39;number of eye blinks detected: &#39; + str(len(events_eog)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;number of eye blinks detected: 1720
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;events_eog&lt;/code&gt; above will give where the eye blinks occured in samples. We will convert the sample number to seconds so we can annotate on the raw file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# get just the sample numbers from the eog events
eog_sampleN = [i[0] for i in events_eog]
# convert to seconds for annotation-friendly purposes
for i in range(0, len(eog_sampleN)):
    eog_sampleN[i] = eog_sampleN[i] / float(200)

# set annotation
annot_eog = mne.Annotations(eog_sampleN, [0.1] * len(eog_sampleN),
                            [&amp;quot;bad eye&amp;quot;] * len(eog_sampleN), orig_time = raw.info[&#39;meas_date&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add this eye blink annotation to the previous annotation by simply adding
new_annot = annot_imp + annot_eog
raw.set_annotations(new_annot)
print(&#39;new annotation set!&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;new annotation set!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that new annotation is set, let&amp;rsquo;s see the changes made to the raw. Again we will just have a figure printed out here. You can see the bad channels marked red (like E1), and bunch of red bars that potentially mark spikes/eye blinks. Because the Times x-axis is so zoomed out, we see all parts being red, but as we see the plot above, that is actually not true. We see that &amp;lsquo;bad eye&amp;rsquo; is annotated for any potential peaks in the &amp;lsquo;EB&amp;rsquo; channel that is newly created from bipolar reference.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# you can check that more red segments are marked on the raw file
print(raw.plot(bad_color=&#39;red&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_basics_30_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(782.64x483.12)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;setting-rereference&#34;&gt;Setting rereference&lt;/h2&gt;

&lt;p&gt;Now that bad channels are marked and we know which bad segments to avoid, we will set eeg reference (We want to avoid doing reference before the bad data are marked and rejected).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;setting eeg reference...&#39;)
raw.set_eeg_reference(&#39;average&#39;, projection=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;setting eeg reference...





&amp;lt;RawEGI  |  sfv_eeg_011ts.raw, n_channels x n_times : 135 x 989490 (4947.4 sec), ~1019.5 MB, data loaded&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-epochs&#34;&gt;Creating epochs&lt;/h2&gt;

&lt;p&gt;Now that we have done some primary artifact detections, we can create a first look on how our epochs look. Epochs are time-locked events of interest. Here, we look at the few hundred milliseconds before and after the onset of the last word of a sentence presentation. Before creating the epochs, we will run some custom codes to update the event arrays accordingly so the event labels are properly labeled ie. 1 for onsets, 2 for offsets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# first find events related to tlst stim channel in the cleaned raw
events_tlst = mne.find_events(raw, stim_channel=&#39;tlst&#39;)

# events_tlst is a array structure ie.  (1, 0, 1) and so far, the all the event tags are 1 which is not true
# We will update the event tags with 1s and 2s with custom built function

# update event ids in mne events array and double check sampling onset timing as sanity check
events_tlstS = extract_nslog_event.assign_event_id(df_tlst, events_tlst)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;updating mne event array and double checking sampling onset time...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# epoching initially with metadata applied
event_id_tlst = dict(lstS=1)
tmin = -0.25  # start of each epoch
tmax = 0.8  # end of each epoch
# set baseline to 0
baseline = (tmin, 0)

# picks specify which channels we are interested
picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=True, stim=False, exclude=&#39;bads&#39;)

# `metadata` field is used to put in our comprehensive pandas dataframe
# it is useful for later creating evoked responses by conditions
epochs_tlstS = mne.Epochs(raw, events_tlstS, event_id_tlst, tmin, tmax, proj=False, picks=picks,
                          baseline=baseline, preload=True, reject=None, reject_by_annotation=True, metadata=df_tlstS)
print(&#39;epochs_tlstS:&#39;)
print(epochs_tlstS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;epochs_tlstS:
&amp;lt;Epochs  |   388 events (all good), -0.25 - 0.8 sec, baseline [-0.25, 0], ~72.8 MB, data loaded, with metadata,
 &#39;lstS&#39;: 388&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We created epochs named &lt;code&gt;epochs_tlstS&lt;/code&gt; which is mne&amp;rsquo;s epochs instance. Note that the epochs are 388 instead of original 400. It is likely that the some epochs are dropped from annotations. Let&amp;rsquo;s see if it&amp;rsquo;s true.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# show drop percentage from mne.Epochs
drop_count = 0
for j in range(0, len(epochs_tlstS.drop_log)):
    if &#39;bad eye&#39; in epochs_tlstS.drop_log[j]:
        drop_count += 1
print(str(drop_count) + &#39; epochs dropped by eog annotation&#39;)
print(&#39;perecentage dropped: &#39; + str(epochs_tlstS.drop_log_stats()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;12 epochs dropped by eog annotation
perecentage dropped: 3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-evoked-response-erp&#34;&gt;Creating evoked response (ERP)&lt;/h2&gt;

&lt;p&gt;Everything looks good. We can create an evoked response by condition. Currently, the epochs_tlst contains all four conditions of the task. By creating an evoked response by condition, we can examine the data for each condition.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create evoked respone using pandas query based on metadata created from previous epochs
evoked_tlst_c1 = epochs_tlstS[&amp;quot;label==&#39;lstS&#39; and cond==&#39;1&#39;&amp;quot;].average()
evoked_tlst_c2 = epochs_tlstS[&amp;quot;label==&#39;lstS&#39; and cond==&#39;2&#39;&amp;quot;].average()
evoked_tlst_c3 = epochs_tlstS[&amp;quot;label==&#39;lstS&#39; and cond==&#39;3&#39;&amp;quot;].average()
evoked_tlst_c4 = epochs_tlstS[&amp;quot;label==&#39;lstS&#39; and cond==&#39;4&#39;&amp;quot;].average()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Evoked responses are created by condition. Let&amp;rsquo;s just inspect the first one. The figure below will show the waveforms of all channels (except the ones marked bad and bipolar referenced channels) with total N epochs in that condition. Originally, N=100 for each condition.&lt;/p&gt;

&lt;p&gt;We can see something happening at 100ms to 300ms range after the onset of the word, which is time point 0s.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(evoked_tlst_c1.plot())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_basics_41_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(460.8x216)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure above is in black and could be hard to inspect. A more interesting plot could be using &lt;code&gt;plot_joint&lt;/code&gt; method. You can see that most of the channels in the frontal region are showing flat, insignificant patterns. On the other hand, the right occipital (marked in red, purplish colors) is revealing potentially interesting results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(evoked_tlst_c1.plot_joint(times=&#39;peaks&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/Users/Jin/Library/Python/2.7/lib/python/site-packages/mne/transforms.py:689: RuntimeWarning: invalid value encountered in divide
  out[:, 2] = np.arccos(cart[:, 2] / out[:, 0])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./walkthrough_basics_43_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure(576x302.4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To avoid going through the same process everytime you load in a subject, we can save the progress by saving the resulted epochs (ie. &lt;code&gt;epochs_tlstS&lt;/code&gt; or raw instance). In the other notebook, we will continue with more advanced artifact detection using the saved epochs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs_tlstS.save(&#39;epochs_tlsts-epo.fif&#39;, split_size=&#39;2GB&#39;, fmt=&#39;single&#39;, verbose=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sources-and-useful-links&#34;&gt;Sources and useful links&lt;/h2&gt;

&lt;p&gt;MNE querying metadata: &lt;a href=&#34;https://martinos.org/mne/stable/auto_examples/preprocessing/plot_metadata_query.html&#34; target=&#34;_blank&#34;&gt;https://martinos.org/mne/stable/auto_examples/preprocessing/plot_metadata_query.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MNE annotations: &lt;a href=&#34;https://martinos.org/mne/stable/generated/mne.Annotations.html&#34; target=&#34;_blank&#34;&gt;https://martinos.org/mne/stable/generated/mne.Annotations.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tangible UI: Augmented Shadow Play</title>
      <link>https://jinjeon.me/project-archives/shadowplay/</link>
      <pubDate>Sun, 10 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project-archives/shadowplay/</guid>
      <description>&lt;p&gt;Shadow casting possess different physical affordances that follow the laws of physics. Different shadow shapes and patterns can be created by manipulating the shadow‚Äôs light source direction and intensity, the physical object‚Äôs distance and angle, and the texture of the surface in which the shadow is casted. These physical properties can be used by the operator to create creative patterns with the projected shadows. Some of these physical affordances include playing with the shadow‚Äôs movement, superposing different object‚Äôs shadows and scaling the shadow‚Äôs size by moving the physical object closer or further from the light source. As aesthetically pleasing with its unique black and white contrast, shadows are a fascinating form of media that can create different textures and details based on the skill of the user.&lt;br&gt;&lt;br&gt;&lt;img src=&#34;https://jinjeon.me/img/shadowplay/test.gif&#34; alt=&#34;&#34; /&gt;&lt;br&gt; Shadow Play aims to enhance such a playful shadow experience to more advanced creative form of art by allowing users digitally manipulate their shadows. By adding new digital affordances Shadow Play, users can print and add layers of multiple shadows onto the screen, and invigorate them with animated effects and motions. With such various mix of augmented shadow effects, users can utilize their body as a tool for creating their own unique shadow artwork and animations.&lt;br&gt;&lt;a href=&#34;https://jinjeon.me/pdf/ShadowPlay.pdf&#34;&gt;See full paper&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;strong&gt;Summary of interaction:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Input/Output:&lt;/strong&gt; Shadow Play requires ample activity space for interaction. The system recognizes the user within the activity space, and then displays the digitally augmented shadow onto the projected screen. Optimizing the interaction was one of our key concerns. With camera as an input device for users to communicate with Shadow Play, we wanted to depict the metaphor of users &lt;em&gt;taking pictures&lt;/em&gt; and &lt;em&gt;filming&lt;/em&gt; the creation. By selecting different features, such as snapshot, mirror, effects, and loop, users can create unique patterns and artwork. &lt;img src=&#34;https://jinjeon.me/img/shadowplay/setup.jpg&#34; alt=&#34;&#34; /&gt;&lt;br&gt;&lt;strong&gt;Interaction:&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;1. Setting the environment:&lt;/strong&gt; With the shoot feature, users can add layers of current shadows to the previous ones. Users can create static shadows, in which we refer to as background/environment of the artwork. &lt;img src=&#34;https://jinjeon.me/img/shadowplay/env.gif&#34; alt=&#34;&#34; /&gt;&lt;br&gt;&lt;strong&gt;2. Adding in motion and effects:&lt;/strong&gt; After the environment has been set, users can enliven the shadow by adding in animated motions and effects. The loop feature, mainly inspired by Instagram, records the motion of shadows and then is automatically looped. By adding in the fireball effect, Shadow Play recognizes the hand motion of reaching out, which then shoots out a fireball from the tip of the hand. &lt;img src=&#34;https://jinjeon.me/img/shadowplay/motion.gif&#34; alt=&#34;&#34; /&gt;&lt;br&gt;&lt;strong&gt;3. Advanced: Mirror and Patterns:&lt;/strong&gt; The mirror feature allows users to create symmetrical images and animation. The mirror feature is especially useful for creating patterns and symmetrical image, such as butterfly. &lt;img src=&#34;https://jinjeon.me/img/shadowplay/mirror.gif&#34; alt=&#34;&#34; /&gt;&lt;br&gt;With the mix of these features, users can create shadow artwork in playful environment. Shadow Play resembles the thought process and planning of an actual painting. Just as an artist would plan ahead where certain objects would be placed within the space before making the strokes, Shadow Play users have to deliberately organize the angle and distance of their body in order to perfect the layers of shadow into a single object shape. Such deliberation allows for more playful and interactive activity. By having the augmented shadows displayed on the screen, there is a loop of feedback and manipulation as users constantly refer to the captured shadow to make adjustments accordingly.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Prioritization in Classification of Novel Objects</title>
      <link>https://jinjeon.me/project-archives/featureprioritization/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project-archives/featureprioritization/</guid>
      <description>&lt;p&gt;Object classification is essential to human learning as it helps us cope with various stimulus around the world. Regardless of multiple features within a single object, object classification seems to occur seamlessly within our cognitive process. In this experiment, we test how we prioritize each feature within an object and how these features are weighted when we categorize a certain object. Test subjects were given novel shapes that each featured either size, color, or orientation, and had to determine whether the shape belongs to a category of a given prototypical shape. &lt;br&gt; &lt;br&gt; The preliminary result showed that color was the single most determining feature when categorizing an object, showing 72.6% of incorporation in all trials, while orientation was the least with 60.7%, but the differences were not statistically significant. We further went on to use logistic regression to analyze the result, which showed thresholds for identifying a novel object to be in a certain category. However, these thresholds for each feature was not significantly different. The experiment suggests that categorization is more of an elaborate and holistic process that combines different features when categorizing a novel object.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yelp Dataset Challenge</title>
      <link>https://jinjeon.me/project-archives/yelp/</link>
      <pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project-archives/yelp/</guid>
      <description>&lt;p&gt;By analyzing Yelp‚Äôs dataset, specifically star ratings and text reviews, we created a classifier that predicts whether reviews are positive (star ratings of four or five) or negative (star ratings of one or two). We excluded star ratings of three because we weren‚Äôt sure whether they were positive or negative. While Yelp‚Äôs star ratings are helpful for concise overview of local businesses, they are also crucial metrics for businesses as the ratings reflect their reputations. However, we realized that star ratings are often misleading as they are subject to user bias and preference. Thus, we wanted to predict ratings solely based on textual features of the reviews and exclude any potential human errors and biases. Performing logistic regression with the combined five features, we were able to correctly predict the reviews with an overall accuracy of 79%.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Homy, private social network for families</title>
      <link>https://jinjeon.me/project-archives/homy/</link>
      <pubDate>Tue, 10 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://jinjeon.me/project-archives/homy/</guid>
      <description>

&lt;style&gt;
.introduction {
  column-count: 2;
}
&lt;/style&gt;

&lt;p&gt;&lt;body style=&#34;font-family:Arial; font-size: 12pt&#34;&gt;
&lt;div class=&#34;introduction&#34;&gt;
&lt;b&gt;Methods:&lt;/b&gt;
&lt;br&gt;&lt;small&gt;generative research, market research, prototyping, usability testing&lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Timeline: &lt;/b&gt;
&lt;br&gt;&lt;small&gt;June 2015 - Nov 2015 &lt;/small&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;div&#34;&gt;&lt;/div&gt;&lt;/h2&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I took a semester off to build my own mobile app, which is a niche-specific social media built just for families. Without any prior knowledge in UX, I self-taught myself along the entire journey, which involved conducting a &lt;a href=&#34;https://jinjeon.me/pdf/Homy_A1.pdf&#34; target=&#34;_blank&#34;_&gt;market research&lt;/a&gt;, sketching wireframes, designing prototypes, and conducting usability tests.&lt;/p&gt;

&lt;h2 id=&#34;impact&#34;&gt;Impact&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I received 5K funding from Rehoboth Business Idea Competition&lt;/li&gt;
&lt;li&gt;State government support with free office space in the tech hub valley&lt;/li&gt;
&lt;li&gt;Developed storyboard and high fidelity prototype through iterations of market research, surveys, and user testing&lt;/li&gt;
&lt;li&gt;Hired a software engineer to develop into an app
&amp;mdash;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;

&lt;p&gt;We are spending the least time with the most important people in our lives, family, while spending more and more time with strangers, coworkers and friends. The cause of the problem has been generally identified as increased work and commute time, and busier individual routines.&lt;/p&gt;

&lt;h2 id=&#34;design-question&#34;&gt;Design Question&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;p style=&#34;font-size: 16pt&#34;&gt;&lt;mark&gt;&lt;em&gt;&amp;ldquo;How might we recreate an online home where family members can seamlessly connect and provide emotional support?&amp;rdquo;&lt;/em&gt;&lt;/mark&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;mission&#34;&gt;Mission&lt;/h2&gt;

&lt;p&gt;Homy attempts to solve this growing social problem by recreating online home and allowing families to seamlessly connect. With unique post-it style fridge page, Homy wants to encourage families to communicate more often. The mission is &lt;em&gt;to provide emotional communication service to enrich family‚Äôs real, offline relationship.&lt;/em&gt;
&lt;br&gt;&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;homyui.png&#34; data-caption=&#34;Click to see in large view.  My initial iteration of the prototype using Proto.io The Fridge, the landing page, recreates the experience of having post-it notes on the kitchen fridge&#34;&gt;
&lt;img src=&#34;homyui.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view. &lt;br&gt; My initial iteration of the prototype using Proto.io&lt;/strong&gt; &lt;br&gt;The Fridge, the landing page, recreates the experience of having post-it notes on the kitchen fridge
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Homy_A2-1.png&#34; data-caption=&#34;Click to see in large view&#34;&gt;
&lt;img src=&#34;Homy_A2-1.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Homy_A2-2.png&#34; data-caption=&#34;Click to see in large view&#34;&gt;
&lt;img src=&#34;Homy_A2-2.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Homy_A2-3.png&#34; data-caption=&#34;Click to see in large view&#34;&gt;
&lt;img src=&#34;Homy_A2-3.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Homy_A2-4.png&#34; data-caption=&#34;Click to see in large view&#34;&gt;
&lt;img src=&#34;Homy_A2-4.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Homy_A2-5.png&#34; data-caption=&#34;Click to see in large view&#34;&gt;
&lt;img src=&#34;Homy_A2-5.png&#34; alt=&#34;&#34; width=&#34;full&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Click to see in large view&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Check more at:&lt;br&gt;
&lt;a href=&#34;https://jinjeon.me/pdf/Homy_A1.pdf&#34; target=&#34;_blank&#34;_&gt; &lt;strong&gt;Defining Market Opportunity.pdf&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
